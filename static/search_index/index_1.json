{"/docs/projects/ascend_matrix/index.html":{"title":"昇腾算子矩阵乘探究","content":"# 昇腾算子矩阵乘探究 ## 编写昇腾算子完成矩阵乘 [点击查看相关简介](../../tutorial/ascend/mindspore_develop/index.html) ## 使用add_custom算子验证流水线以及多缓冲 [原始数据.xlsx](./工作簿1.xlsx) >!数据的长度限制在int32范围，因此我们采用2048\\*64\\*64\\*64为上限； >!同时需要注意的是，cache的容量为65536，而输入数据的大小为int16，因此，一次add支持的长度为2048\\*16个int16 ### 首先验证计算段远快于搬运段，同时证明流水 我们仅仅需要在compute阶段增加一个add函数 >!这个add函数仅仅执行一次add操作，不会改变任何数据，除了compute执行时间 two add即： ![alt text](image 8.png) total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms)tip 2048\\*64\\*64\\*64164\\*64\\*812048\\*831.8391zero add 2048\\*64\\*64\\*64164\\*64\\*812048\\*841.581one add 2048\\*64\\*64\\*64164\\*64\\*812048\\*851.2499two add 2048\\*64\\*64\\*64164\\*64\\*812048\\*860.9961three add 老实说，上面的时间相当微妙，如果按照正常逻辑，计算段的时间远低于搬运段，且满足流水，那么此处的时间应该相差不大，但是存在差距，而且不小，且每次增加的时间基本相等。 增加一个实验，仅进行相同次数的add操作，如下： ![alt text](image.png) 得到执行时间为9.7928ms 所以存在以下两种可能性： + 非流水，整个core func是串行执行 + add执行的时间接近或高于2048*8个数据的搬运 再增加三个实验，即仅进行copyin、copyout、copyin+copyout ![alt text](image 4.png) copyin执行时间为19.158ms copyout执行时间为9.28448ms copyin+copyout执行时间为19.523ms 以上数据能够完美的证明搬入搬出单元是独立的,以及x，y的数据搬运是串行的，得到以下流水图示： ![alt text](image 1.png) 还有一点在于，在执行freeTensor之前，我们没办法执行新的一轮搬运，这是因为我们的buffernum为1，否则无法满足上述理论模型。 因此，对于buffernum为1的情况，约束条件为，tile> 2的copyin需要在add之后执行，add的执行必须在copyout之后。 接着实验 ### 单核下，成倍增加数据总长度 total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms) 2048\\*64\\*64\\*8 164\\*64\\*812048 14.9771 2048\\*64\\*64\\*16164\\*64\\*812048\\*218.1726 2048\\*64\\*64\\*32164\\*64\\*812048\\*425.9916 2048\\*64\\*64\\*64164\\*64\\*812048\\*841.5741 当成倍增长总数据长度时，如果是串行，那么执行时间也应该线性增长，但是执行时间非线性，这可以侧面说明执行流程当中的流水。 对于增加数据长度，数据片数量不变而带来的时间增长，这个很好理解，虽然tile数量没变，但是tile_length在增长： ![alt text](image 2.png) 对于以上流程，其copyin以及compute决定了执行的总时间，验证copyout是否真的不影响总的执行时间，即在copyout中再执行一轮datacopy，如下： ![alt text](image 3.png) 得到的数据与上面表格对应的执行时间分别是14.9949，18.7419，27.2262，44.5016，基本不产生区别，可证。 ### buffer num变化 total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch_2buffer(ms)mean_time_100epoch_1buffer(ms) 2048\\*64\\*64\\*64164\\*64\\*8 22048\\*821.707341.5925 2048\\*64\\*64\\*64164\\*64\\*1622048\\*435.014851.8856 2048\\*64\\*64\\*64164\\*64\\*3222048\\*269.96 72.5588 2048\\*64\\*64\\*64164\\*64\\*6422048 139.851119.389 随着数据片的数量增加，tile长度减小，时间损耗增大，如下图： ![alt text](image 6.png) ![alt text](image 7.png) 注意到同单缓冲数据相比，数据片长度分别为2048\\*8，2048\\*4，2048\\*2的样例表现更好，但是当数据片更小时,反而表现更差， 采用以下图例进行说明： ![alt text](image 5.png) 当数据片较大时，充分利用到了double buffer，即在进行add的同时也能进行copyin，add无需在copyout以后执行，因此效果较好。 但是当数据片较小时，为什么double buffer的表现效果却比单缓冲的要差？关于这一点不能够理解。 ### 多核 total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms) 2048\\*64\\*64\\*64164\\*64\\*412048\\*1636.8166 2048\\*64\\*64\\*64264\\*64\\*212048\\*1618.5086 2048\\*64\\*64\\*64464\\*64\\*112048\\*169.35409 2048\\*64\\*64\\*64864\\*3212048\\*164.97731 2048\\*64\\*64\\*641664\\*1612048\\*163.17449 实验符合预期 ## matrix 验证 ## 杂项 ![alt text](e06f146dd10f3b4bda45b86a43484302_720.jpg) 上面是分片，由上注意到，当B_Split_W为128，group_size为15时，根据代码，blockid 0，15，30的aicore会处理第0组数据，但是B矩阵仅被分了两组数据，因此blockid 30的aicore会轮空，也就是aicore 30实际上没有计算，其余aicore的计算负载增大，导致执行时间延长。"},"/docs/projects/tinytodo/index.html":{"title":"tinytodo","content":"# tinytodo 找不到一个好用的待办软件，一怒之下怒了一下，自己搞一个！ ## 需求 序号功能描述 fun_1显示登陆窗口 fun_2输入邮箱登录 fun_3显示待办窗口，置于底层，并固定在右上角 fun_4点击＋按钮添加待办 fun_5输入代办内容，点击添加，添加待办 fun_6点击统计按钮，弹出统计窗口 fun_7 fun_8 fun_9 ## 流程 登陆 ```mermaid flowchart LR 1[\"click exe\"] 2[\"show login window\"] 3[\"get email\"] 4[\"request url to get todolist\"] 5[\"generate the todolist window\"] 6[\"show todolist window\"] 1 > 2 2 > 3 3 > 4 4 > 5 5 > 6 ``` 添加待办 ```mermaid flowchart LR 1[\"click +\"] 2[\"show add window\"] 3[\"get input\"] 4[\"add todolist\"] 5[\"sync todolist to url\"] 6[\"regenerate todolist window\"] 1 > 2 2 > 3 3 > 4 4 > 5 5 > 6 ``` ## c/s protocol client:SYNC?email <email> server: success:\"READY?DBSIZE <DBSIZE>\" client:\"READY RESV\" server:FILE<DBSIZE> server:\"FILEOK\" fail:\"FAIL\" client:UPLOAD?email <email> server: success:response<\"READY\"> client:\"READY?DBSIZE <DBSIZE>\" server:\"READY RESV\" client:FILE<DBSIZE> server\"FILEOK\" fail:response<\"FAIL\"> client:<file> server: success:response<\"FILEOK\"> fail:response<\"FAIL\"> ## server process ### sync + 1.检查是否存在邮箱记录 + 2.存在返回 + 3.不存在则返回FAIL ### upload + 1.检查是否存在邮箱记录 + 2.存在邮箱则查询对应的数据库名称，返回READY，并接受文件保存为查询到的名称 + 3.不存在则新建邮箱记录 + 4.新建邮箱记录首先获取最大值id + 5.提交数据库 ## exec file versiondownloadsource_code <! tinytodo v1.0[download here](./tinytodov1.0.zip)[download source](./sourcev1.0.zip) tinytodo v1.1[download here](./tinytodov1.1.zip) tinytodo v1.2[download here](./tinytodov1.2.zip) tinytodo v1.3[download here](./tinytodov1.3.7z)[download source](./sourcev1.3.7z) tinytodo v1.3[download here](./tinytodov1.4.7z)[download source](https://gitee.com/helloyutao/tinytodo.git) >"},"/docs/projects/ftp-design/ftp-cjc.html":{"title":"openEuler应用软件开发赛+什么都不会+初赛+ftp服务器实现","content":"# openEuler应用软件开发赛+什么都不会+初赛+ftp服务器实现 > 支持被动模式避免客户端位于firewall或者NAT后面的情况 > > 支持主动模式 > > 整个项目完全使用仓颉语言开发 > > 未使用任何开源项目 > > x86_64/aarch_64 ## run + server `bash run ftp.sh` + client `dnf install ftp` `ftp 127.0.0.1` 内置用户 userpassword ftp user1123456 user2123456 可用协议指令 inst参数用途 \"USER\"username登陆用户 \"PASS\"password登陆密码 \"SYST\"查看系统信息 \"PORT\"port使用客户端端口设置主动模式 \"LIST\"列出目录文件 \"CWD\" path切换目录 \"PWD\" 查看当前路径 \"PASV\"设置被动模式 \"TYPE\"mode设置传输格式 \"RETR\"file下载文件 \"STOR\"file上传文件 \"MKD\" file创建文件夹 \"ABOR\"流产连接 \"QUIT\"退出 ## 效果 + filezilla ![filezilla](./filezilla.png) + ftp ![ftp](./ftp.png) + curl ![curl](./curl.png) ## 整体架构 ```mermaid flowchart LR frontend[\"frontend\"] requestDistri[\"request distribution\"] client[\"client\"] subgraph processUnit direction LR parser[\"parser\"] sender[\"sender\"] end subgraph processUnit1 end subgraph processUnit2 end subgraph ... end transCtl[\"transport control\"] userCtl[\"user control\"] utils[\"utils(String2Int...)\"] config[\"configuration(only one)\"] asClient[\"asClient\"] asServer[\"asServer\"] fs[\"file system\"] requestDistri > frontend parser > requestDistri processUnit1 > requestDistri processUnit2 > requestDistri ... > requestDistri frontend > client client > frontend config > userCtl transCtl > parser userCtl > parser fs > transCtl config > transCtl asClient > transCtl asServer > transCtl userCtl > transCtl linkStyle 0,1,2,3,4,5,6,8,9,10,12,13,14 stroke:green linkStyle 7,11 stroke:red ``` ## 并发架构 + 资源池方案 将操作系统资源抽象为池，当出现新的用户请求时，从池中取出资源进行分配 在本项目中的实现即为，使用request Distribution（reqdist）模块解耦业务逻辑与用户请求 新的请求进入时，reqdist模块将会分配新的线程用于处理当前的请求，当请求断开时，释放线程资源回到池中。因此所有占用的资源都是可限制的，我们可以预设线程数以限制访问的qps，防止某些不安全攻击操作。 [可扩展]解耦的好处是，我们能够轻易的扩展其余硬件，例如，当服务器不止一个cpu（或者其他协处理器）时，线程资源分配将会复杂化，我们仅需要在reqdist模块中将资源分配给process unit，而不必考虑具体的业务逻辑。 + 资源锁 多个线程对文件的变动将有可能产生资源冲突。 我们最小化公共资源，将所有对实际文件的操作都聚集在configer模块当中，因此我们使用configer改变文件状态时，将会使用锁解决访问资源冲突问题。 同时对资源进行精细化控制，以便平衡时间与资源同步的问题。 ## 权限管理 + 文件系统 为了实现足够精细化的权限管理，我们手工实现了一个简单的文件系统，仿照linux文件系统的inode设计，下面是一个文件夹的inode结构体，使用json文件存储inode列表。 ```json { \"name\": \"/\", \"type\": \"d\", \"user\": \"root\", \"user_power\": 7, \"group_power\": 7, \"others_power\": 0, \"subfiles\": [ 0,0,1,2 ] } ``` 文件的id取决于文件inode在json array中的index + 权限 我们的权限系统依然仿照linux，user_power代表用户（创建者）本身的权限，group_power代表组用户的权限，others_power代表其他用户的权限。 组用户取决于config.json文件中user group，如下 ```json \"user1\":{ \"password\":\"123456\", \"group\":[\"ftp\"], \"root\":1 } ``` 例如上面的user1信息中，ftp默认用户是其组用户成员 权限数字分别代表三位数字的bool值，当第一位为1，也就是100时值为4，代表有下载查看权限，当第二位为1，也就是010时值为2，代表有下载权限，剩余一位留作扩展。 + tip **既然每一样都跟linux类似，为什么不直接用linux的文件系统？** > 因为我们对项目的要求是可扩展性，如果直接使用linux的文件系统，如果我们以后需要扩展其余权限，例如更改文件的权限，删除文件的权限，我们的扩展就会非常困难。 ## 数据传输 ### 主动模式 主动模式下，需要客户端告知服务器端口 因此服务器需要作为客户端连接用户机，此时无需考虑端口的问题。 ### 被动模式 被动模式下，需要服务端告知客户端数据端口 此时服务器作为数据传输的服务器等待客户端连接。 服务器需要扫描服务器上的可用端口告知客户端以连接。 + 端口池 秉持所有资源皆可控的理念，我们将端口也作为配置放入配置文件，端口作为资源为客户端分配，达到可控的方式。 **假设我们采用临时扫描端口的方式，那么不得不考虑的问题是，在扫描到空闲端口后，但是该端口实际上属于service所使用的端口（出现暂时空闲），在扫描到确认连接这段时间内，该端口有可能被重新占用的问题。因此我们的方案既避免服务占用ftp端口的情况，也避免ftp占用服务端口的情况，同时减少代码开发代价** ## source [download](https://atomgit.com/openeuler123/nihaopeng)"},"/docs/projects/IntelligentAudioChat/index.html":{"title":"","content":""},"/docs/projects/index.html":{"title":"some tutorials about the configure","content":"# some tutorials about the configure some tutorials about the configure > **click the sidebar to open markdown file** >! [read from the first file](./ascend_matrix/index.html)"},"/docs/projects/riscv-telechat-openeuler/openeuler/openeuler.html":{"title":"openeuler 烧录到licheepi4a","content":"# openeuler 烧录到licheepi4a 跟着[官网教程](https://docs.openeuler.org/zh/docs/24.03_LTS/docs/Installation/RISC V LicheePi4A.html)走 tips： + 1，显示器可能会显示不出来，用串口直连调试 + 2，wifi无法正常list，网口直连，不要那种需要登陆的网络 + 3，登录账号：root，密码：openEuler12#$ + 4，如果无法下载东西不是因为源的问题，看看网络是否连接上了"},"/docs/projects/riscv-telechat-openeuler/telechat移植/telechat.html":{"title":"telechat-12B移植","content":"# telechat 12B移植 ## 从hf mirror下载模型 别问我为啥不在hf官网，梯子流量够用随便搞 + download hfd,a tool to download base on aria2 `wget https://hf mirror.com/hfd/hfd.sh` `chmod a+x hfd.sh` + 设置hugginface下载环境变量 `export HF_ENDPOINT https://hf mirror.com` + 下载模型 `./hfd.sh Tele AI/TeleChat 12B tool aria2c x 4` >出现报错看下面的教程 + 下载数据集（用于记录，不用执行） `./hfd.sh wikitext dataset tool aria2c x 4` ## aria编译 如果你跟着上面的步骤走，那一定会到这里，因为yum源没有aria2安装包，乐 + 克隆aria2源码 `git clone https://github.com/aria2/aria2.git` + build `autoreconf i` autoreconf装了运行不了，缺少autopoint + yum install gettext devel 装依赖 `yum install openssl devel zlib zlib devel` `./configure ARIA2_STATIC yes` error:A compiler with support for C++11 language features is required. + solution:yum install g++ `make install j4` ## git lfs编译 如果你跟着上面的步骤走，那一定会到这里，因为git lfs妹有riscv64版本的，蚌 + 安装go `yum install golang` + 编译 1,克隆git lfs 2,`go env w GOPROXY https://goproxy.cn` 3,添加bin目录到环境变量 4，执行`git lfs install` ## openeuler riscv上编译pytorch + 依赖 `dnf install python3 {hypothesis,psutil,pyyaml,requests,sympy,filelock,networkx,jinja2,fsspec,packaging,numpy,venv}` + 下载gitee预编译riscv whl `git clone recursive https://github.com/pytorch/pytorch`约4个GB，建议用梯子 `cd pytorch && python3 setup.py develop` + 测试 <div style \"text align:center;\"><img src \"QQ20241210 150124.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ## 安装transformers `git clone https://github.com/huggingface/transformers.git` >下面这句逆天操作据说是ninja和cmake互相依赖导致无限递归的问题，乐 `dnf install cmake python3 devel` 安装rust compiler(逆天) `export RUSTUP_DIST_SERVER https://mirrors.ustc.edu.cn/rust static` `export RUSTUP_UPDATE_ROOT https://mirrors.ustc.edu.cn/rust static/rustup` `curl proto ' https' tlsv1.2 sSf https://sh.rustup.rs sh` `pip install 'transformers[torch]'` ## 运行python inference + 问题1：需要安装flash attn，然而该库依赖cuda支持 修改12B模型中的modeling_telechat.py的357，增加config中的flash attn判断以取消初始化FlashSelfAttention类产生错误，如下图： <div style \"text align:center;\"><img src \"QQ20241210 223624.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 记得修改config中的参数 <div style \"text align:center;\"><img src \"QQ20241211 091833.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 修改虚拟环境中.venv/lib/python3.11/site packages/transformers/generation/utils.py用以显示进度 <div style \"text align:center;\"><img src \"QQ20241211 135143.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 推理脚本如下： ```python import os import torch from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig tokenizer AutoTokenizer.from_pretrained('../models/12B', trust_remote_code True) model AutoModelForCausalLM.from_pretrained('../models/12B', trust_remote_code True, torch_dtype torch.float16) device \"cpu\" model.to(device) generate_config GenerationConfig.from_pretrained('../models/12B') question \"你好！你是谁？\" answer, history model.chat(tokenizer tokenizer, question question, history [], generation_config generate_config, stream False) print(answer) ``` `python3 test.py` ## huggingface转为onnx `pip install optimum[exporters]` [下载12B模型](#从hf mirror下载模型) [修改推理模型](#运行python inference) 运行以下脚本转换为onnx格式 ```python import os import torch from transformers import AutoTokenizer, AutoModelForCausalLM import onnx import onnxruntime as ort import numpy as np os.environ['TRANSFORMERS_OFFLINE'] '1' class ModelWrapper(torch.nn.Module): def __init__(self, model): super().__init__() self.model model def forward(self, input_ids, attention_mask): # 显式传入 use_cache False，确保不使用 past_key_values outputs self.model(input_ids input_ids, attention_mask attention_mask, use_cache False) # 只返回 logits，以避免复杂的输出结构导致的问题 return outputs.logits def export_model_to_onnx(model_name, output_path, opset_version 13): tokenizer AutoTokenizer.from_pretrained(model_name, trust_remote_code True) model AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code True) model.eval() model.config.use_cache False text \"This is a sample input for ONNX export.\" inputs tokenizer(text, return_tensors \"pt\") # 用包装器替换原始模型 wrapped_model ModelWrapper(model) wrapped_model.eval() input_names [\"input_ids\", \"attention_mask\"] output_names [\"logits\"] dynamic_axes { \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"}, \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"}, \"logits\": {0: \"batch_size\", 1: \"sequence_length\"} } # 导出为 ONNX torch.onnx.export( wrapped_model, args (inputs[\"input_ids\"], inputs[\"attention_mask\"]), f output_path, input_names input_names, output_names output_names, dynamic_axes dynamic_axes, opset_version opset_version, export_params True, do_constant_folding True, ) print(f\"模型已成功导出到 {output_path}\") def validate_onnx_model(model_path, model_name, text): tokenizer AutoTokenizer.from_pretrained(model_name, trust_remote_code True) model AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code True) model.eval() model.c ``` ## onnx使用hhb转为二进制可执行文件 ## 利用npu加速执行 目前（2024 12 11）openeuler暂不支持npu。 2025 3 12, 当前使用openeuler24.03 sp1，已支持npu 手动挂载驱动，如下： ```bash insmod /lib/modules/6.6.0 72.0.0.76.oe2403sp1.riscv64/kernel/drivers/soc/xuantie/nna/img_mem/img_mem.ko.xz modprobe vha onchipmem_phys_start 0xffe0000000 onchipmem_size 0x100000 freq_khz 792000 insmod /lib/modules/6.6.0 72.0.0.76.oe2403sp1.riscv64/kernel/drivers/soc/xuantie/nna/vha/vha_info.ko.xz chmod a+rw /dev/vha0 lsmod ``` 此时/dev文件夹出现vha0"},"/docs/projects/sd1.5-fine-tuning/sd.html":{"title":"","content":"<style> pre { overflow y: auto; max height: 300px; } </style> # stable diffusion fine tuning ## ubuntu安装cuda toolkit `sudo apt install nvidia cuda toolkit` `nvcc V`验证安装并查看cuda版本 ## 安装pytorch 老生常谈了，这里不再给教程，需要注意的几个点是， + 下载的pytorch版本需要和cuda版本对应 + torch版本和GPU计算架构对应 ## 下载stable diffusion预训练模型 ### 从hf mirror下载模型 [点这里看更详细教程](../riscv telechat openeuler/telechat移植/telechat.html) 别问我为啥不在hf官网，梯子流量够用随便搞 + download hfd,a tool to download base on aria2 `wget https://hf mirror.com/hfd/hfd.sh` `chmod a+x hfd.sh` + 设置hugginface下载环境变量 `export HF_ENDPOINT https://hf mirror.com` + 下载模型 `./hfd.sh Tele AI/TeleChat 12B tool aria2c x 4` + 下载数据集（用于记录，不用执行） `./hfd.sh wikitext dataset tool aria2c x 4` ## 预处理数据 你的自定义数据集放在一个文件夹内，我们使用元数据的方式训练，放图片的同一个文件夹下创建一个`metadata.jsonl`文件，文件格式如下： ```json { \"file_name\": \"2.jpg\", \"text\": \"Off white flat garden layout,irregularity,water,mellow\" } { \"file_name\": \"3.jpg\", \"text\": \"Off white flat garden layout,irregularity,line,order\" } { \"file_name\": \"4.jpg\", \"text\": \"Off white flat garden layout,irregularity,bridge,water,line,mellow\" } ... ``` ## 微调 微调脚本，其中的MODEL_NAME赋值为你下载的模型文件路径， OUTPUT_DIR为微调中间文件 DATASET_NAME为放数据集的路径 如果你有多个gpu设备的话，可以将shell脚本中的python换为accelerate分布式训练（如果能到使用accelerate这一步，训练过程中产生的问题相信你也没有问题了） ```bash export MODEL_NAME \"../stable diffusion v1 5\" export OUTPUT_DIR \"./out_lora2\" export DATASET_NAME \"./data/layout\" python train.py \\ pretrained_model_name_or_path $MODEL_NAME \\ dataset_name $DATASET_NAME \\ dataloader_num_workers 0 \\ resolution 512 center_crop random_flip \\ train_batch_size 10 \\ gradient_accumulation_steps 4 \\ max_train_steps 15000 \\ learning_rate 1e 04 \\ max_grad_norm 1 \\ lr_scheduler \"cosine\" lr_warmup_steps 0 \\ output_dir ${OUTPUT_DIR} \\ checkpointing_steps 500 \\ seed 3407 \\ report_to wandb \\ ``` train.py文件是lora的训练脚本。 ```python #!/usr/bin/env python # coding utf 8 # Copyright 2024 The HuggingFace Inc. team. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE 2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"Fine tuning script for Stable Diffusion for text2image with support for LoRA.\"\"\" import argparse import logging import math import os import random import shutil from contextlib import nullcontext from pathlib import Path import datasets import numpy as np import torch import torch.nn.functional as F import torch.utils.checkpoint import transformers from accelerate import Accelerator from accelerate.logging import get_logger from accelerate.utils import ProjectConfiguration, set_seed from datasets import load_dataset from huggingface_hub import create_repo, upload_folder from packaging import version from peft import LoraConfig from peft.utils import get_peft_model_state_dict from torchvision import transforms from tqdm.auto import tqdm from transformers import CLIPTextModel, CLIPTokenizer import diffusers from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel from diffusers.optimization import get_scheduler from diffusers.training_utils import cast_training_params, compute_snr from diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card from diffusers.utils.import_utils import is_xformers_available from diffusers.utils.torch_utils import is_compiled_module if is_wandb_available(): import wandb # Will error if the minimal version of diffusers is not installed. Remove at your own risks. # check_min_version(\"0.31.0.dev0\") logger get_logger(__name__, log_level \"INFO\") def save_model_card( repo_id: str, images: list None, base_model: str None, dataset_name: str None, repo_folder: str None, ): img_str \"\" if images is not None: for i, image in enumerate(images): image.save(os.path.join(repo_folder, f\"image_{i}.png\")) img_str + f\"<div style \"text align:center;\"><img src \"./image_{i}.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div>\\n\" model_description f\"\"\" # LoRA text2image fine tuning {repo_id} These are LoRA adaption weights for {base_model}. The weights were fine tuned on the {dataset_name} dataset. You can find some example images in the following. \\n {img_str} \"\"\" model_card load_or_create_model_card( repo_id_or_path repo_id, from_training True, license \"creativeml openrail m\", base_model base_model, model_description model_description, inference True, ) tags [ \"stable diffusion\", \"stable diffusion diffusers\", \"text to image\", \"diffusers\", \"diffusers training\", \"lora\", ] model_card populate_model_card(model_card, tags tags) model_card.save(os.path.join(repo_folder, \"README.md\")) def log_validation( pipeline, args, accelerator, epoch, is_final_validation False, ): logger.info( f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\" f\" {args.validation_prompt}.\" ) pipeline pipeline.to(accelerator.device) pipeline.set_progress_bar_config(disable True) generator torch.Generator(device accelerator.device) if args.seed is not None: generator generator.manual_seed(args.seed) images [] if torch.backends.mps.is_available(): autocast_ctx nullcontext() else: autocast_ctx torch.autocast(accelerator.device.type) with autocast_ctx: for _ in range(args.num_validation_images): images.append(pipeline(args.validation_prompt, num_inference_steps 30, generator generator).images[0]) for tracker in accelerator.trackers: phase_name \"test\" if is_final_validation else \"validation\" if tracker.name \"tensorboard\": np_images np.stack([np.asarray(img) for img in images]) tracker.writer.add_images(phase_name, np_images, epoch, dataformats \"NHWC\") if tracker.name \"wandb\": tracker.log( { phase_name: [ wandb.Image(image, caption f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images) ] } ) return images def parse_args(): parser argparse.ArgumentParser(description \"Simple example of a training script.\") parser.add_argument( \" pretrained_model_name_or_path\", type str, default None, required True, help \"Path to pretrained model or model identifier from huggingface.co/models.\", ) parser.add_argument( \" revision\", type str, default None, required False, help \"Revision of pretrained model identifier from huggingface.co/models.\", ) parser.add_argument( \" variant\", type str, default None, help \"Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\", ) parser.add_argument( \" dataset_name\", type str, default None, help ( \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\" \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\" \" or to a folder containing files that 🤗 Datasets can understand.\" ), ) parser.add_argument( \" dataset_config_name\", type str, default None, help \"The config of the Dataset, leave as None if there's only one config.\", ) parser.add_argument( \" train_data_dir\", type str, default None, help ( \"A folder containing the training data. Folder contents must follow the structure described in\" \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\" \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\" ), ) parser.add_argument( \" image_column\", type str, default \"image\", help \"The column of the dataset containing an image.\" ) parser.add_argument( \" caption_column\", type str, default \"text\", help \"The column of the dataset containing a caption or a list of captions.\", ) parser.add_argument( \" validation_prompt\", type str, default None, help \"A prompt that is sampled during training for inference.\" ) parser.add_argument( \" num_validation_images\", type int, default 4, help \"Number of images that should be generated during validation with `validation_prompt`.\", ) parser.add_argument( \" validation_epochs\", type int, default 1, help ( \"Run fine tuning validation every X epochs. The validation process consists of running the prompt\" \" `args.validation_prompt` multiple times: `args.num_validation_images`.\" ), ) parser.add_argument( \" max_train_samples\", type int, default None, help ( \"For debugging purposes or quicker training, truncate the number of training examples to this \" \"value if set.\" ), ) parser.add_argument( \" output_dir\", type str, default \"sd model finetuned lora\", help \"The output directory where the model predictions and checkpoints will be written.\", ) parser.add_argument( \" cache_dir\", type str, default None, help \"The directory where the downloaded models and datasets will be stored.\", ) parser.add_argument(\" seed\", type int, default None, help \"A seed for reproducible training.\") parser.add_argument( \" resolution\", type int, default 512, help ( \"The resolution for input images, all the images in the train/validation dataset will be resized to this\" \" resolution\" ), ) parser.add_argument( \" center_crop\", default False, action \"store_true\", help ( \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\" \" cropped. The images will be resized to the resolution first before cropping.\" ), ) parser.add_argument( \" random_flip\", action \"store_true\", help \"whether to randomly flip images horizontally\", ) parser.add_argument( \" train_batch_size\", type int, default 16, help \"Batch size (per device) for the training dataloader.\" ) parser.add_argument(\" num_train_epochs\", type int, default 100) parser.add_argument( \" max_train_steps\", type int, default None, help \"Total number of training steps to perform. If provided, overrides num_train_epochs.\", ) parser.add_argument( \" gradient_accumulation_steps\", type int, default 1, help \"Number of updates steps to accumulate before performing a backward/update pass.\", ) parser.add_argument( \" gradient_checkpointing\", action \"store_true\", help \"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\", ) parser.add_argument( \" learning_rate\", type float, default 1e 4, help \"Initial learning rate (after the potential warmup period) to use.\", ) parser.add_argument( \" scale_lr\", action \"store_true\", default False, help \"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\", ) parser.add_argument( \" lr_scheduler\", type str, default \"constant\", help ( 'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",' ' \"constant\", \"constant_with_warmup\"]' ), ) parser.add_argument( \" lr_warmup_steps\", type int, default 500, help \"Number of steps for the warmup in the lr scheduler.\" ) parser.add_argument( \" snr_gamma\", type float, default None, help \"SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. \" \"More details here: https://arxiv.org/abs/2303.09556.\", ) parser.add_argument( \" use_8bit_adam\", action \"store_true\", help \"Whether or not to use 8 bit Adam from bitsandbytes.\" ) parser.add_argument( \" allow_tf32\", action \"store_true\", help ( \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\" \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat 32 tf32 on ampere devices\" ), ) parser.add_argument( \" dataloader_num_workers\", type int, default 0, help ( \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\" ), ) parser.add_argument(\" adam_beta1\", type float, default 0.9, help \"The beta1 parameter for the Adam optimizer.\") parser.add_argument(\" adam_beta2\", type float, default 0.999, help \"The beta2 parameter for the Adam optimizer.\") parser.add_argument(\" adam_weight_decay\", type float, default 1e 2, help \"Weight decay to use.\") parser.add_argument(\" adam_epsilon\", type float, default 1e 08, help \"Epsilon value for the Adam optimizer\") parser.add_argument(\" max_grad_norm\", default 1.0, type float, help \"Max gradient norm.\") parser.add_argument(\" push_to_hub\", action \"store_true\", help \"Whether or not to push the model to the Hub.\") parser.add_argument(\" hub_token\", type str, default None, help \"The token to use to push to the Model Hub.\") parser.add_argument( \" prediction_type\", type str, default None, help \"The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or leave `None`. If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.\", ) parser.add_argument( \" hub_model_id\", type str, default None, help \"The name of the repository to keep in sync with the local `output_dir`.\", ) parser.add_argument( \" logging_dir\", type str, default \"logs\", help ( \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\" \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\" ), ) parser.add_argument( \" mixed_precision\", type str, default None, choices [\"no\", \"fp16\", \"bf16\"], help ( \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch > \" \" 1.10.and an Nvidia Ampere GPU. Default to the value of accelerate config of the current system or the\" \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\" ), ) parser.add_argument( \" report_to\", type str, default \"tensorboard\", help ( 'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`' ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.' ), ) parser.add_argument(\" local_rank\", type int, default 1, help \"For distributed training: local_rank\") parser.add_argument( \" checkpointing_steps\", type int, default 500, help ( \"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\" \" training using ` resume_from_checkpoint`.\" ), ) parser.add_argument( \" checkpoints_total_limit\", type int, default None, help (\"Max number of checkpoints to store.\"), ) parser.add_argument( \" resume_from_checkpoint\", type str, default None, help ( \"Whether training should be resumed from a previous checkpoint. Use a path saved by\" ' ` checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.' ), ) parser.add_argument( \" enable_xformers_memory_efficient_attention\", action \"store_true\", help \"Whether or not to use xformers.\" ) parser.add_argument(\" noise_offset\", type float, default 0, help \"The scale of noise offset.\") parser.add_argument( \" rank\", type int, default 4, help (\"The dimension of the LoRA update matrices.\"), ) args parser.parse_args() env_local_rank int(os.environ.get(\"LOCAL_RANK\", 1)) if env_local_rank ! 1 and env_local_rank ! args.local_rank: args.local_rank env_local_rank # Sanity checks if args.dataset_name is None and args.train_data_dir is None: raise ValueError(\"Need either a dataset name or a training folder.\") return args DATASET_NAME_MAPPING { \"lambdalabs/naruto blip captions\": (\"image\", \"text\"), } def main(): args parse_args() if args.report_to \"wandb\" and args.hub_token is not None: raise ValueError( \"You cannot use both report_to wandb and hub_token due to a security risk of exposing your token.\" \" Please use `huggingface cli login` to authenticate with the Hub.\" ) logging_dir Path(args.output_dir, args.logging_dir) accelerator_project_config ProjectConfiguration(project_dir args.output_dir, logging_dir logging_dir) accelerator Accelerator( gradient_accumulation_steps args.gradient_accumulation_steps, mixed_precision args.mixed_precision, log_with args.report_to, project_config accelerator_project_config, ) # Disable AMP for MPS. if torch.backends.mps.is_available(): accelerator.native_amp False # Make one log on every process with the configuration for debugging. logging.basicConfig( format \"%(asctime)s %(levelname)s %(name)s %(message)s\", datefmt \"%m/%d/%Y %H:%M:%S\", level logging.INFO, ) logger.info(accelerator.state, main_process_only False) if accelerator.is_local_main_process: datasets.utils.logging.set_verbosity_warning() transformers.utils.logging.set_verbosity_warning() diffusers.utils.logging.set_verbosity_info() else: datasets.utils.logging.set_verbosity_error() transformers.utils.logging.set_verbosity_error() diffusers.utils.logging.set_verbosity_error() # If passed along, set the training seed now. if args.seed is not None: set_seed(args.seed) # Handle the repository creation if accelerator.is_main_process: if args.output_dir is not None: os.makedirs(args.output_dir, exist_ok True) if args.push_to_hub: repo_id create_repo( repo_id args.hub_model_id or Path(args.output_dir).name, exist_ok True, token args.hub_token ).repo_id # Load scheduler, tokenizer and models. noise_scheduler DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder \"scheduler\") tokenizer CLIPTokenizer.from_pretrained( args.pretrained_model_name_or_path, subfolder \"tokenizer\", revision args.revision ) text_encoder CLIPTextModel.from_pretrained( args.pretrained_model_name_or_path, subfolder \"text_encoder\", revision args.revision ) vae AutoencoderKL.from_pretrained( args.pretrained_model_name_or_path, subfolder \"vae\", revision args.revision, variant args.variant ) unet UNet2DConditionModel.from_pretrained( args.pretrained_model_name_or_path, subfolder \"unet\", revision args.revision, variant args.variant ) # freeze parameters of models to save more memory unet.requires_grad_(False) vae.requires_grad_(False) text_encoder.requires_grad_(False) # For mixed precision training we cast all non trainable weights (vae, non lora text_encoder and non lora unet) to half precision # as these weights are only used for inference, keeping weights in full precision is not required. weight_dtype torch.float32 if accelerator.mixed_precision \"fp16\": weight_dtype torch.float16 elif accelerator.mixed_precision \"bf16\": weight_dtype torch.bfloat16 # Freeze the unet parameters before adding adapters for param in unet.parameters(): param.requires_grad_(False) unet_lora_config LoraConfig( r args.rank, lora_alpha args.rank, init_lora_weights \"gaussian\", target_modules [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"], ) # Move unet, vae and text_encoder to device and cast to weight_dtype unet.to(accelerator.device, dtype weight_dtype) vae.to(accelerator.device, dtype weight_dtype) text_encoder.to(accelerator.device, dtype weight_dtype) # Add adapter and make sure the trainable params are in float32. unet.add_adapter(unet_lora_config) if args.mixed_precision \"fp16\": # only upcast trainable parameters (LoRA) into fp32 cast_training_params(unet, dtype torch.float32) if args.enable_xformers_memory_efficient_attention: if is_xformers_available(): import xformers xformers_version version.parse(xformers.__version__) if xformers_version version.parse(\"0.0.16\"): logger.warning( \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\" ) unet.enable_xformers_memory_efficient_attention() else: raise ValueError(\"xformers is not available. Make sure it is installed correctly\") lora_layers filter(lambda p: p.requires_grad, unet.parameters()) if args.gradient_checkpointing: unet.enable_gradient_checkpointing() # Enable TF32 for faster training on Ampere GPUs, # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat 32 tf32 on ampere devices if args.allow_tf32: torch.backends.cuda.matmul.allow_tf32 True if args.scale_lr: args.learning_rate ( args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes ) # Initialize the optimizer if args.use_8bit_adam: try: import bitsandbytes as bnb except ImportError: raise ImportError( \"Please install bitsandbytes to use 8 bit Adam. You can do so by running `pip install bitsandbytes`\" ) optimizer_cls bnb.optim.AdamW8bit else: optimizer_cls torch.optim.AdamW optimizer optimizer_cls( lora_layers, lr args.learning_rate, betas (args.adam_beta1, args.adam_beta2), weight_decay args.adam_weight_decay, eps args.adam_epsilon, ) # Get the datasets: you can either provide your own training and evaluation files (see below) # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub). # In distributed training, the load_dataset function guarantees that only one local process can concurrently # download the dataset. if args.dataset_name is not None: # Downloading and loading a dataset from the hub. dataset load_dataset( args.dataset_name, args.dataset_config_name, cache_dir args.cache_dir, data_dir args.train_data_dir, ) else: data_files {} if args.train_data_dir is not None: data_files[\"train\"] os.path.join(args.train_data_dir, \"**\") dataset load_dataset( \"imagefolder\", data_files data_files, cache_dir args.cache_dir, ) # See more about loading custom images at # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder # Preprocessing the datasets. # We need to tokenize inputs and targets. column_names dataset[\"train\"].column_names # 6. Get the column names for input/target. dataset_columns DATASET_NAME_MAPPING.get(args.dataset_name, None) if args.image_column is None: image_column dataset_columns[0] if dataset_columns is not None else column_names[0] else: image_column args.image_column if image_column not in column_names: raise ValueError( f\" image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\" ) if args.caption_column is None: caption_column dataset_columns[1] if dataset_columns is not None else column_names[1] else: caption_column args.caption_column if caption_column not in column_names: raise ValueError( f\" caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\" ) # Preprocessing the datasets. # We need to tokenize input captions and transform the images. def tokenize_captions(examples, is_train True): captions [] for caption in examples[caption_column]: if isinstance(caption, str): captions.append(caption) elif isinstance(caption, (list, np.ndarray)): # take a random caption if there are multiple captions.append(random.choice(caption) if is_train else caption[0]) else: raise ValueError( f\"Caption column `{caption_column}` should contain either strings or lists of strings.\" ) inputs tokenizer( captions, max_length tokenizer.model_max_length, padding \"max_length\", truncation True, return_tensors \"pt\" ) return inputs.input_ids # Preprocessing the datasets. train_transforms transforms.Compose( [ transforms.Resize(args.resolution, interpolation transforms.InterpolationMode.BILINEAR), transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution), transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x), transforms.ToTensor(), transforms.Normalize([0.5], [0.5]), ] ) def unwrap_model(model): model accelerator.unwrap_model(model) model model._orig_mod if is_compiled_module(model) else model return model def preprocess_train(examples): images [image.convert(\"RGB\") for image in examples[image_column]] examples[\"pixel_values\"] [train_transforms(image) for image in images] examples[\"input_ids\"] tokenize_captions(examples) return examples with accelerator.main_process_first(): if args.max_train_samples is not None: dataset[\"train\"] dataset[\"train\"].shuffle(seed args.seed).select(range(args.max_train_samples)) # Set the training transforms train_dataset dataset[\"train\"].with_transform(preprocess_train) def collate_fn(examples): pixel_values torch.stack([example[\"pixel_values\"] for example in examples]) pixel_values pixel_values.to(memory_format torch.contiguous_format).float() input_ids torch.stack([example[\"input_ids\"] for example in examples]) return {\"pixel_values\": pixel_values, \"input_ids\": input_ids} # DataLoaders creation: train_dataloader torch.utils.data.DataLoader( train_dataset, shuffle True, collate_fn collate_fn, batch_size args.train_batch_size, num_workers args.dataloader_num_workers, ) # Scheduler and math around the number of training steps. # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation. num_warmup_steps_for_scheduler args.lr_warmup_steps * accelerator.num_processes if args.max_train_steps is None: len_train_dataloader_after_sharding math.ceil(len(train_dataloader) / accelerator.num_processes) num_update_steps_per_epoch math.ceil(len_train_dataloader_after_sharding / args.gradient_accumulation_steps) num_training_steps_for_scheduler ( args.num_train_epochs * num_update_steps_per_epoch * accelerator.num_processes ) else: num_training_steps_for_scheduler args.max_train_steps * accelerator.num_processes lr_scheduler get_scheduler( args.lr_scheduler, optimizer optimizer, num_warmup_steps num_warmup_steps_for_scheduler, num_training_steps num_training_steps_for_scheduler, ) # Prepare everything with our `accelerator`. unet, optimizer, train_dataloader, lr_scheduler accelerator.prepare( unet, optimizer, train_dataloader, lr_scheduler ) # We need to recalculate our total training steps as the size of the training dataloader may have changed. num_update_steps_per_epoch math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) if args.max_train_steps is None: args.max_train_steps args.num_train_epochs * num_update_steps_per_epoch if num_training_steps_for_scheduler ! args.max_train_steps * accelerator.num_processes: logger.warning( f\"The length of the 'train_dataloader' after 'accelerator.prepare' ({len(train_dataloader)}) does not match \" f\"the expected length ({len_train_dataloader_after_sharding}) when the learning rate scheduler was created. \" f\"This inconsistency may result in the learning rate scheduler not functioning properly.\" ) # Afterwards we recalculate our number of training epochs args.num_train_epochs math.ceil(args.max_train_steps / num_update_steps_per_epoch) # We need to initialize the trackers we use, and also store our configuration. # The trackers initializes automatically on the main process. if accelerator.is_main_process: accelerator.init_trackers(\"text2image fine tune\", config vars(args)) # Train! total_batch_size args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps logger.info(\"***** Running training *****\") logger.info(f\" Num examples {len(train_dataset)}\") logger.info(f\" Num Epochs {args.num_train_epochs}\") logger.info(f\" Instantaneous batch size per device {args.train_batch_size}\") logger.info(f\" Total train batch size (w. parallel, distributed & accumulation) {total_batch_size}\") logger.info(f\" Gradient Accumulation steps {args.gradient_accumulation_steps}\") logger.info(f\" Total optimization steps {args.max_train_steps}\") global_step 0 first_epoch 0 # Potentially load in the weights and states from a previous save if args.resume_from_checkpoint: if args.resume_from_checkpoint ! \"latest\": path os.path.basename(args.resume_from_checkpoint) else: # Get the most recent checkpoint dirs os.listdir(args.output_dir) dirs [d for d in dirs if d.startswith(\"checkpoint\")] dirs sorted(dirs, key lambda x: int(x.split(\" \")[1])) path dirs[ 1] if len(dirs) > 0 else None if path is None: accelerator.print( f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\" ) args.resume_from_checkpoint None initial_global_step 0 else: accelerator.print(f\"Resuming from checkpoint {path}\") accelerator.load_state(os.path.join(args.output_dir, path)) global_step int(path.split(\" \")[1]) initial_global_step global_step first_epoch global_step // num_update_steps_per_epoch else: initial_global_step 0 progress_bar tqdm( range(0, args.max_train_steps), initial initial_global_step, desc \"Steps\", # Only show the progress bar once on each machine. disable not accelerator.is_local_main_process, ) for epoch in range(first_epoch, args.num_train_epochs): unet.train() train_loss 0.0 for step, batch in enumerate(train_dataloader): with accelerator.accumulate(unet): # Convert images to latent space latents vae.encode(batch[\"pixel_values\"].to(dtype weight_dtype)).latent_dist.sample() latents latents * vae.config.scaling_factor # Sample noise that we'll add to the latents noise torch.randn_like(latents) if args.noise_offset: # https://www.crosslabs.org//blog/diffusion with offset noise noise + args.noise_offset * torch.randn( (latents.shape[0], latents.shape[1], 1, 1), device latents.device ) bsz latents.shape[0] # Sample a random timestep for each image timesteps torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device latents.device) timesteps timesteps.long() # Add noise to the latents according to the noise magnitude at each timestep # (this is the forward diffusion process) noisy_latents noise_scheduler.add_noise(latents, noise, timesteps) # Get the text embedding for conditioning encoder_hidden_states text_encoder(batch[\"input_ids\"], return_dict False)[0] # Get the target for loss depending on the prediction type if args.prediction_type is not None: # set prediction_type of scheduler if defined noise_scheduler.register_to_config(prediction_type args.prediction_type) if noise_scheduler.config.prediction_type \"epsilon\": target noise elif noise_scheduler.config.prediction_type \"v_prediction\": target noise_scheduler.get_velocity(latents, noise, timesteps) else: raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\") # Predict the noise residual and compute loss model_pred unet(noisy_latents, timesteps, encoder_hidden_states, return_dict False)[0] if args.snr_gamma is None: loss F.mse_loss(model_pred.float(), target.float(), reduction \"mean\") else: # Compute loss weights as per Section 3.4 of https://arxiv.org/abs/2303.09556. # Since we predict the noise instead of x_0, the original formulation is slightly changed. # This is discussed in Section 4.2 of the same paper. snr compute_snr(noise_scheduler, timesteps) mse_loss_weights torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim 1).min( dim 1 )[0] if noise_scheduler.config.prediction_type \"epsilon\": mse_loss_weights mse_loss_weights / snr elif noise_scheduler.config.prediction_type \"v_prediction\": mse_loss_weights mse_loss_weights / (snr + 1) loss F.mse_loss(model_pred.float(), target.float(), reduction \"none\") loss loss.mean(dim list(range(1, len(loss.shape)))) * mse_loss_weights loss loss.mean() # Gather the losses across all processes for logging (if we use distributed training). avg_loss accelerator.gather(loss.repeat(args.train_batch_size)).mean() train_loss + avg_loss.item() / args.gradient_accumulation_steps # Backpropagate accelerator.backward(loss) if accelerator.sync_gradients: params_to_clip lora_layers accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm) optimizer.step() lr_scheduler.step() optimizer.zero_grad() # Checks if the accelerator has performed an optimization step behind the scenes if accelerator.sync_gradients: progress_bar.update(1) global_step + 1 accelerator.log({\"train_loss\": train_loss}, step global_step) train_loss 0.0 if global_step % args.checkpointing_steps 0: if accelerator.is_main_process: # _before_ saving state, check if this save would set us over the `checkpoints_total_limit` if args.checkpoints_total_limit is not None: checkpoints os.listdir(args.output_dir) checkpoints [d for d in checkpoints if d.startswith(\"checkpoint\")] checkpoints sorted(checkpoints, key lambda x: int(x.split(\" \")[1])) # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit 1` checkpoints if len(checkpoints) > args.checkpoints_total_limit: num_to_remove len(checkpoints) args.checkpoints_total_limit + 1 removing_checkpoints checkpoints[0:num_to_remove] logger.info( f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\" ) logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\") for removing_checkpoint in removing_checkpoints: removing_checkpoint os.path.join(args.output_dir, removing_checkpoint) shutil.rmtree(removing_checkpoint) save_path os.path.join(args.output_dir, f\"checkpoint {global_step}\") accelerator.save_state(save_path) unwrapped_unet unwrap_model(unet) unet_lora_state_dict convert_state_dict_to_diffusers( get_peft_model_state_dict(unwrapped_unet) ) StableDiffusionPipeline.save_lora_weights( save_directory save_path, unet_lora_layers unet_lora_state_dict, safe_serialization True, ) logger.info(f\"Saved state to {save_path}\") logs {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]} progress_bar.set_postfix(**logs) if global_step > args.max_train_steps: break if accelerator.is_main_process: if args.validation_prompt is not None and epoch % args.validation_epochs 0: # create pipeline pipeline DiffusionPipeline.from_pretrained( args.pretrained_model_name_or_path, unet unwrap_model(unet), revision args.revision, variant args.variant, torch_dtype weight_dtype, ) images log_validation(pipeline, args, accelerator, epoch) del pipeline torch.cuda.empty_cache() # Save the lora layers accelerator.wait_for_everyone() if accelerator.is_main_process: unet unet.to(torch.float32) unwrapped_unet unwrap_model(unet) unet_lora_state_dict convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet)) StableDiffusionPipeline.save_lora_weights( save_directory args.output_dir, unet_lora_layers unet_lora_state_dict, safe_serialization True, ) # Final inference # Load previous pipeline if args.validation_prompt is not None: pipeline DiffusionPipeline.from_pretrained( args.pretrained_model_name_or_path, revision args.revision, variant args.variant, torch_dtype weight_dtype, ) # load attention processors pipeline.load_lora_weights(args.output_dir) # run inference images log_validation(pipeline, args, accelerator, epoch, is_final_validation True) if args.push_to_hub: save_model_card( repo_id, images images, base_model args.pretrained_model_name_or_path, dataset_name args.dataset_name, repo_folder args.output_dir, ) upload_folder( repo_id repo_id, folder_path args.output_dir, commit_message \"End of training\", ignore_patterns [\"step_*\", \"epoch_*\"], ) accelerator.end_training() if __name__ \"__main__\": main() ```"}}