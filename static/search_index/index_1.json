{"/docs/projects/ascend_matrix/index.html":{"title":"æ˜‡è…¾ç®—å­çŸ©é˜µä¹˜æ¢ç©¶","content":"# æ˜‡è…¾ç®—å­çŸ©é˜µä¹˜æ¢ç©¶ ## ç¼–å†™æ˜‡è…¾ç®—å­å®ŒæˆçŸ©é˜µä¹˜ [ç‚¹å‡»æŸ¥çœ‹ç›¸å…³ç®€ä»‹](../../tutorial/ascend/mindspore_develop/index.html) ## ä½¿ç”¨add_customç®—å­éªŒè¯æµæ°´çº¿ä»¥åŠå¤šç¼“å†² [åŸå§‹æ•°æ®.xlsx](./å·¥ä½œç°¿1.xlsx) >!æ•°æ®çš„é•¿åº¦é™åˆ¶åœ¨int32èŒƒå›´ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨2048\\*64\\*64\\*64ä¸ºä¸Šé™ï¼› >!åŒæ—¶éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œcacheçš„å®¹é‡ä¸º65536ï¼Œè€Œè¾“å…¥æ•°æ®çš„å¤§å°ä¸ºint16ï¼Œå› æ­¤ï¼Œä¸€æ¬¡addæ”¯æŒçš„é•¿åº¦ä¸º2048\\*16ä¸ªint16 ### é¦–å…ˆéªŒè¯è®¡ç®—æ®µè¿œå¿«äºæ¬è¿æ®µï¼ŒåŒæ—¶è¯æ˜æµæ°´ æˆ‘ä»¬ä»…ä»…éœ€è¦åœ¨computeé˜¶æ®µå¢åŠ ä¸€ä¸ªaddå‡½æ•° >!è¿™ä¸ªaddå‡½æ•°ä»…ä»…æ‰§è¡Œä¸€æ¬¡addæ“ä½œï¼Œä¸ä¼šæ”¹å˜ä»»ä½•æ•°æ®ï¼Œé™¤äº†computeæ‰§è¡Œæ—¶é—´ two addå³ï¼š ![alt text](image 8.png) total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms)tip 2048\\*64\\*64\\*64164\\*64\\*812048\\*831.8391zero add 2048\\*64\\*64\\*64164\\*64\\*812048\\*841.581one add 2048\\*64\\*64\\*64164\\*64\\*812048\\*851.2499two add 2048\\*64\\*64\\*64164\\*64\\*812048\\*860.9961three add è€å®è¯´ï¼Œä¸Šé¢çš„æ—¶é—´ç›¸å½“å¾®å¦™ï¼Œå¦‚æœæŒ‰ç…§æ­£å¸¸é€»è¾‘ï¼Œè®¡ç®—æ®µçš„æ—¶é—´è¿œä½äºæ¬è¿æ®µï¼Œä¸”æ»¡è¶³æµæ°´ï¼Œé‚£ä¹ˆæ­¤å¤„çš„æ—¶é—´åº”è¯¥ç›¸å·®ä¸å¤§ï¼Œä½†æ˜¯å­˜åœ¨å·®è·ï¼Œè€Œä¸”ä¸å°ï¼Œä¸”æ¯æ¬¡å¢åŠ çš„æ—¶é—´åŸºæœ¬ç›¸ç­‰ã€‚ å¢åŠ ä¸€ä¸ªå®éªŒï¼Œä»…è¿›è¡Œç›¸åŒæ¬¡æ•°çš„addæ“ä½œï¼Œå¦‚ä¸‹ï¼š ![alt text](image.png) å¾—åˆ°æ‰§è¡Œæ—¶é—´ä¸º9.7928ms æ‰€ä»¥å­˜åœ¨ä»¥ä¸‹ä¸¤ç§å¯èƒ½æ€§ï¼š + éæµæ°´ï¼Œæ•´ä¸ªcore funcæ˜¯ä¸²è¡Œæ‰§è¡Œ + addæ‰§è¡Œçš„æ—¶é—´æ¥è¿‘æˆ–é«˜äº2048*8ä¸ªæ•°æ®çš„æ¬è¿ å†å¢åŠ ä¸‰ä¸ªå®éªŒï¼Œå³ä»…è¿›è¡Œcopyinã€copyoutã€copyin+copyout ![alt text](image 4.png) copyinæ‰§è¡Œæ—¶é—´ä¸º19.158ms copyoutæ‰§è¡Œæ—¶é—´ä¸º9.28448ms copyin+copyoutæ‰§è¡Œæ—¶é—´ä¸º19.523ms ä»¥ä¸Šæ•°æ®èƒ½å¤Ÿå®Œç¾çš„è¯æ˜æ¬å…¥æ¬å‡ºå•å…ƒæ˜¯ç‹¬ç«‹çš„,ä»¥åŠxï¼Œyçš„æ•°æ®æ¬è¿æ˜¯ä¸²è¡Œçš„ï¼Œå¾—åˆ°ä»¥ä¸‹æµæ°´å›¾ç¤ºï¼š ![alt text](image 1.png) è¿˜æœ‰ä¸€ç‚¹åœ¨äºï¼Œåœ¨æ‰§è¡ŒfreeTensorä¹‹å‰ï¼Œæˆ‘ä»¬æ²¡åŠæ³•æ‰§è¡Œæ–°çš„ä¸€è½®æ¬è¿ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬çš„buffernumä¸º1ï¼Œå¦åˆ™æ— æ³•æ»¡è¶³ä¸Šè¿°ç†è®ºæ¨¡å‹ã€‚ å› æ­¤ï¼Œå¯¹äºbuffernumä¸º1çš„æƒ…å†µï¼Œçº¦æŸæ¡ä»¶ä¸ºï¼Œtile> 2çš„copyinéœ€è¦åœ¨addä¹‹åæ‰§è¡Œï¼Œaddçš„æ‰§è¡Œå¿…é¡»åœ¨copyoutä¹‹åã€‚ æ¥ç€å®éªŒ ### å•æ ¸ä¸‹ï¼Œæˆå€å¢åŠ æ•°æ®æ€»é•¿åº¦ total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms) 2048\\*64\\*64\\*8 164\\*64\\*812048 14.9771 2048\\*64\\*64\\*16164\\*64\\*812048\\*218.1726 2048\\*64\\*64\\*32164\\*64\\*812048\\*425.9916 2048\\*64\\*64\\*64164\\*64\\*812048\\*841.5741 å½“æˆå€å¢é•¿æ€»æ•°æ®é•¿åº¦æ—¶ï¼Œå¦‚æœæ˜¯ä¸²è¡Œï¼Œé‚£ä¹ˆæ‰§è¡Œæ—¶é—´ä¹Ÿåº”è¯¥çº¿æ€§å¢é•¿ï¼Œä½†æ˜¯æ‰§è¡Œæ—¶é—´éçº¿æ€§ï¼Œè¿™å¯ä»¥ä¾§é¢è¯´æ˜æ‰§è¡Œæµç¨‹å½“ä¸­çš„æµæ°´ã€‚ å¯¹äºå¢åŠ æ•°æ®é•¿åº¦ï¼Œæ•°æ®ç‰‡æ•°é‡ä¸å˜è€Œå¸¦æ¥çš„æ—¶é—´å¢é•¿ï¼Œè¿™ä¸ªå¾ˆå¥½ç†è§£ï¼Œè™½ç„¶tileæ•°é‡æ²¡å˜ï¼Œä½†æ˜¯tile_lengthåœ¨å¢é•¿ï¼š ![alt text](image 2.png) å¯¹äºä»¥ä¸Šæµç¨‹ï¼Œå…¶copyinä»¥åŠcomputeå†³å®šäº†æ‰§è¡Œçš„æ€»æ—¶é—´ï¼ŒéªŒè¯copyoutæ˜¯å¦çœŸçš„ä¸å½±å“æ€»çš„æ‰§è¡Œæ—¶é—´ï¼Œå³åœ¨copyoutä¸­å†æ‰§è¡Œä¸€è½®datacopyï¼Œå¦‚ä¸‹ï¼š ![alt text](image 3.png) å¾—åˆ°çš„æ•°æ®ä¸ä¸Šé¢è¡¨æ ¼å¯¹åº”çš„æ‰§è¡Œæ—¶é—´åˆ†åˆ«æ˜¯14.9949ï¼Œ18.7419ï¼Œ27.2262ï¼Œ44.5016ï¼ŒåŸºæœ¬ä¸äº§ç”ŸåŒºåˆ«ï¼Œå¯è¯ã€‚ ### buffer numå˜åŒ– total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch_2buffer(ms)mean_time_100epoch_1buffer(ms) 2048\\*64\\*64\\*64164\\*64\\*8 22048\\*821.707341.5925 2048\\*64\\*64\\*64164\\*64\\*1622048\\*435.014851.8856 2048\\*64\\*64\\*64164\\*64\\*3222048\\*269.96 72.5588 2048\\*64\\*64\\*64164\\*64\\*6422048 139.851119.389 éšç€æ•°æ®ç‰‡çš„æ•°é‡å¢åŠ ï¼Œtileé•¿åº¦å‡å°ï¼Œæ—¶é—´æŸè€—å¢å¤§ï¼Œå¦‚ä¸‹å›¾ï¼š ![alt text](image 6.png) ![alt text](image 7.png) æ³¨æ„åˆ°åŒå•ç¼“å†²æ•°æ®ç›¸æ¯”ï¼Œæ•°æ®ç‰‡é•¿åº¦åˆ†åˆ«ä¸º2048\\*8ï¼Œ2048\\*4ï¼Œ2048\\*2çš„æ ·ä¾‹è¡¨ç°æ›´å¥½ï¼Œä½†æ˜¯å½“æ•°æ®ç‰‡æ›´å°æ—¶,åè€Œè¡¨ç°æ›´å·®ï¼Œ é‡‡ç”¨ä»¥ä¸‹å›¾ä¾‹è¿›è¡Œè¯´æ˜ï¼š ![alt text](image 5.png) å½“æ•°æ®ç‰‡è¾ƒå¤§æ—¶ï¼Œå……åˆ†åˆ©ç”¨åˆ°äº†double bufferï¼Œå³åœ¨è¿›è¡Œaddçš„åŒæ—¶ä¹Ÿèƒ½è¿›è¡Œcopyinï¼Œaddæ— éœ€åœ¨copyoutä»¥åæ‰§è¡Œï¼Œå› æ­¤æ•ˆæœè¾ƒå¥½ã€‚ ä½†æ˜¯å½“æ•°æ®ç‰‡è¾ƒå°æ—¶ï¼Œä¸ºä»€ä¹ˆdouble bufferçš„è¡¨ç°æ•ˆæœå´æ¯”å•ç¼“å†²çš„è¦å·®ï¼Ÿå…³äºè¿™ä¸€ç‚¹ä¸èƒ½å¤Ÿç†è§£ã€‚ ### å¤šæ ¸ total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms) 2048\\*64\\*64\\*64164\\*64\\*412048\\*1636.8166 2048\\*64\\*64\\*64264\\*64\\*212048\\*1618.5086 2048\\*64\\*64\\*64464\\*64\\*112048\\*169.35409 2048\\*64\\*64\\*64864\\*3212048\\*164.97731 2048\\*64\\*64\\*641664\\*1612048\\*163.17449 å®éªŒç¬¦åˆé¢„æœŸ ## matrix éªŒè¯ ## æ‚é¡¹ ![alt text](e06f146dd10f3b4bda45b86a43484302_720.jpg) ä¸Šé¢æ˜¯åˆ†ç‰‡ï¼Œç”±ä¸Šæ³¨æ„åˆ°ï¼Œå½“B_Split_Wä¸º128ï¼Œgroup_sizeä¸º15æ—¶ï¼Œæ ¹æ®ä»£ç ï¼Œblockid 0ï¼Œ15ï¼Œ30çš„aicoreä¼šå¤„ç†ç¬¬0ç»„æ•°æ®ï¼Œä½†æ˜¯BçŸ©é˜µä»…è¢«åˆ†äº†ä¸¤ç»„æ•°æ®ï¼Œå› æ­¤blockid 30çš„aicoreä¼šè½®ç©ºï¼Œä¹Ÿå°±æ˜¯aicore 30å®é™…ä¸Šæ²¡æœ‰è®¡ç®—ï¼Œå…¶ä½™aicoreçš„è®¡ç®—è´Ÿè½½å¢å¤§ï¼Œå¯¼è‡´æ‰§è¡Œæ—¶é—´å»¶é•¿ã€‚"},"/docs/projects/tinytodo/index.html":{"title":"tinytodo","content":"# tinytodo æ‰¾ä¸åˆ°ä¸€ä¸ªå¥½ç”¨çš„å¾…åŠè½¯ä»¶ï¼Œä¸€æ€’ä¹‹ä¸‹æ€’äº†ä¸€ä¸‹ï¼Œè‡ªå·±æä¸€ä¸ªï¼ ## éœ€æ±‚ åºå·åŠŸèƒ½æè¿° fun_1æ˜¾ç¤ºç™»é™†çª—å£ fun_2è¾“å…¥é‚®ç®±ç™»å½• fun_3æ˜¾ç¤ºå¾…åŠçª—å£ï¼Œç½®äºåº•å±‚ï¼Œå¹¶å›ºå®šåœ¨å³ä¸Šè§’ fun_4ç‚¹å‡»ï¼‹æŒ‰é’®æ·»åŠ å¾…åŠ fun_5è¾“å…¥ä»£åŠå†…å®¹ï¼Œç‚¹å‡»æ·»åŠ ï¼Œæ·»åŠ å¾…åŠ fun_6ç‚¹å‡»ç»Ÿè®¡æŒ‰é’®ï¼Œå¼¹å‡ºç»Ÿè®¡çª—å£ fun_7 fun_8 fun_9 ## æµç¨‹ ç™»é™† ```mermaid flowchart LR 1[\"click exe\"] 2[\"show login window\"] 3[\"get email\"] 4[\"request url to get todolist\"] 5[\"generate the todolist window\"] 6[\"show todolist window\"] 1 > 2 2 > 3 3 > 4 4 > 5 5 > 6 ``` æ·»åŠ å¾…åŠ ```mermaid flowchart LR 1[\"click +\"] 2[\"show add window\"] 3[\"get input\"] 4[\"add todolist\"] 5[\"sync todolist to url\"] 6[\"regenerate todolist window\"] 1 > 2 2 > 3 3 > 4 4 > 5 5 > 6 ``` ## c/s protocol client:SYNC?email <email> server: success:\"READY?DBSIZE <DBSIZE>\" client:\"READY RESV\" server:FILE<DBSIZE> server:\"FILEOK\" fail:\"FAIL\" client:UPLOAD?email <email> server: success:response<\"READY\"> client:\"READY?DBSIZE <DBSIZE>\" server:\"READY RESV\" client:FILE<DBSIZE> server\"FILEOK\" fail:response<\"FAIL\"> client:<file> server: success:response<\"FILEOK\"> fail:response<\"FAIL\"> ## server process ### sync + 1.æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‚®ç®±è®°å½• + 2.å­˜åœ¨è¿”å› + 3.ä¸å­˜åœ¨åˆ™è¿”å›FAIL ### upload + 1.æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‚®ç®±è®°å½• + 2.å­˜åœ¨é‚®ç®±åˆ™æŸ¥è¯¢å¯¹åº”çš„æ•°æ®åº“åç§°ï¼Œè¿”å›READYï¼Œå¹¶æ¥å—æ–‡ä»¶ä¿å­˜ä¸ºæŸ¥è¯¢åˆ°çš„åç§° + 3.ä¸å­˜åœ¨åˆ™æ–°å»ºé‚®ç®±è®°å½• + 4.æ–°å»ºé‚®ç®±è®°å½•é¦–å…ˆè·å–æœ€å¤§å€¼id + 5.æäº¤æ•°æ®åº“ ## exec file versiondownloadsource_code <! tinytodo v1.0[download here](./tinytodov1.0.zip)[download source](./sourcev1.0.zip) tinytodo v1.1[download here](./tinytodov1.1.zip) tinytodo v1.2[download here](./tinytodov1.2.zip) tinytodo v1.3[download here](./tinytodov1.3.7z)[download source](./sourcev1.3.7z) tinytodo v1.3[download here](./tinytodov1.4.7z)[download source](https://gitee.com/helloyutao/tinytodo.git) >"},"/docs/projects/ftp-design/ftp-cjc.html":{"title":"openEuleråº”ç”¨è½¯ä»¶å¼€å‘èµ›+ä»€ä¹ˆéƒ½ä¸ä¼š+åˆèµ›+ftpæœåŠ¡å™¨å®ç°","content":"# openEuleråº”ç”¨è½¯ä»¶å¼€å‘èµ›+ä»€ä¹ˆéƒ½ä¸ä¼š+åˆèµ›+ftpæœåŠ¡å™¨å®ç° > æ”¯æŒè¢«åŠ¨æ¨¡å¼é¿å…å®¢æˆ·ç«¯ä½äºfirewallæˆ–è€…NATåé¢çš„æƒ…å†µ > > æ”¯æŒä¸»åŠ¨æ¨¡å¼ > > æ•´ä¸ªé¡¹ç›®å®Œå…¨ä½¿ç”¨ä»“é¢‰è¯­è¨€å¼€å‘ > > æœªä½¿ç”¨ä»»ä½•å¼€æºé¡¹ç›® > > x86_64/aarch_64 ## run + server `bash run ftp.sh` + client `dnf install ftp` `ftp 127.0.0.1` å†…ç½®ç”¨æˆ· userpassword ftp user1123456 user2123456 å¯ç”¨åè®®æŒ‡ä»¤ instå‚æ•°ç”¨é€” \"USER\"usernameç™»é™†ç”¨æˆ· \"PASS\"passwordç™»é™†å¯†ç  \"SYST\"æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯ \"PORT\"portä½¿ç”¨å®¢æˆ·ç«¯ç«¯å£è®¾ç½®ä¸»åŠ¨æ¨¡å¼ \"LIST\"åˆ—å‡ºç›®å½•æ–‡ä»¶ \"CWD\" pathåˆ‡æ¢ç›®å½• \"PWD\" æŸ¥çœ‹å½“å‰è·¯å¾„ \"PASV\"è®¾ç½®è¢«åŠ¨æ¨¡å¼ \"TYPE\"modeè®¾ç½®ä¼ è¾“æ ¼å¼ \"RETR\"fileä¸‹è½½æ–‡ä»¶ \"STOR\"fileä¸Šä¼ æ–‡ä»¶ \"MKD\" fileåˆ›å»ºæ–‡ä»¶å¤¹ \"ABOR\"æµäº§è¿æ¥ \"QUIT\"é€€å‡º ## æ•ˆæœ + filezilla ![filezilla](./filezilla.png) + ftp ![ftp](./ftp.png) + curl ![curl](./curl.png) ## æ•´ä½“æ¶æ„ ```mermaid flowchart LR frontend[\"frontend\"] requestDistri[\"request distribution\"] client[\"client\"] subgraph processUnit direction LR parser[\"parser\"] sender[\"sender\"] end subgraph processUnit1 end subgraph processUnit2 end subgraph ... end transCtl[\"transport control\"] userCtl[\"user control\"] utils[\"utils(String2Int...)\"] config[\"configuration(only one)\"] asClient[\"asClient\"] asServer[\"asServer\"] fs[\"file system\"] requestDistri > frontend parser > requestDistri processUnit1 > requestDistri processUnit2 > requestDistri ... > requestDistri frontend > client client > frontend config > userCtl transCtl > parser userCtl > parser fs > transCtl config > transCtl asClient > transCtl asServer > transCtl userCtl > transCtl linkStyle 0,1,2,3,4,5,6,8,9,10,12,13,14 stroke:green linkStyle 7,11 stroke:red ``` ## å¹¶å‘æ¶æ„ + èµ„æºæ± æ–¹æ¡ˆ å°†æ“ä½œç³»ç»Ÿèµ„æºæŠ½è±¡ä¸ºæ± ï¼Œå½“å‡ºç°æ–°çš„ç”¨æˆ·è¯·æ±‚æ—¶ï¼Œä»æ± ä¸­å–å‡ºèµ„æºè¿›è¡Œåˆ†é… åœ¨æœ¬é¡¹ç›®ä¸­çš„å®ç°å³ä¸ºï¼Œä½¿ç”¨request Distributionï¼ˆreqdistï¼‰æ¨¡å—è§£è€¦ä¸šåŠ¡é€»è¾‘ä¸ç”¨æˆ·è¯·æ±‚ æ–°çš„è¯·æ±‚è¿›å…¥æ—¶ï¼Œreqdistæ¨¡å—å°†ä¼šåˆ†é…æ–°çš„çº¿ç¨‹ç”¨äºå¤„ç†å½“å‰çš„è¯·æ±‚ï¼Œå½“è¯·æ±‚æ–­å¼€æ—¶ï¼Œé‡Šæ”¾çº¿ç¨‹èµ„æºå›åˆ°æ± ä¸­ã€‚å› æ­¤æ‰€æœ‰å ç”¨çš„èµ„æºéƒ½æ˜¯å¯é™åˆ¶çš„ï¼Œæˆ‘ä»¬å¯ä»¥é¢„è®¾çº¿ç¨‹æ•°ä»¥é™åˆ¶è®¿é—®çš„qpsï¼Œé˜²æ­¢æŸäº›ä¸å®‰å…¨æ”»å‡»æ“ä½œã€‚ [å¯æ‰©å±•]è§£è€¦çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè½»æ˜“çš„æ‰©å±•å…¶ä½™ç¡¬ä»¶ï¼Œä¾‹å¦‚ï¼Œå½“æœåŠ¡å™¨ä¸æ­¢ä¸€ä¸ªcpuï¼ˆæˆ–è€…å…¶ä»–åå¤„ç†å™¨ï¼‰æ—¶ï¼Œçº¿ç¨‹èµ„æºåˆ†é…å°†ä¼šå¤æ‚åŒ–ï¼Œæˆ‘ä»¬ä»…éœ€è¦åœ¨reqdistæ¨¡å—ä¸­å°†èµ„æºåˆ†é…ç»™process unitï¼Œè€Œä¸å¿…è€ƒè™‘å…·ä½“çš„ä¸šåŠ¡é€»è¾‘ã€‚ + èµ„æºé” å¤šä¸ªçº¿ç¨‹å¯¹æ–‡ä»¶çš„å˜åŠ¨å°†æœ‰å¯èƒ½äº§ç”Ÿèµ„æºå†²çªã€‚ æˆ‘ä»¬æœ€å°åŒ–å…¬å…±èµ„æºï¼Œå°†æ‰€æœ‰å¯¹å®é™…æ–‡ä»¶çš„æ“ä½œéƒ½èšé›†åœ¨configeræ¨¡å—å½“ä¸­ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨configeræ”¹å˜æ–‡ä»¶çŠ¶æ€æ—¶ï¼Œå°†ä¼šä½¿ç”¨é”è§£å†³è®¿é—®èµ„æºå†²çªé—®é¢˜ã€‚ åŒæ—¶å¯¹èµ„æºè¿›è¡Œç²¾ç»†åŒ–æ§åˆ¶ï¼Œä»¥ä¾¿å¹³è¡¡æ—¶é—´ä¸èµ„æºåŒæ­¥çš„é—®é¢˜ã€‚ ## æƒé™ç®¡ç† + æ–‡ä»¶ç³»ç»Ÿ ä¸ºäº†å®ç°è¶³å¤Ÿç²¾ç»†åŒ–çš„æƒé™ç®¡ç†ï¼Œæˆ‘ä»¬æ‰‹å·¥å®ç°äº†ä¸€ä¸ªç®€å•çš„æ–‡ä»¶ç³»ç»Ÿï¼Œä»¿ç…§linuxæ–‡ä»¶ç³»ç»Ÿçš„inodeè®¾è®¡ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹çš„inodeç»“æ„ä½“ï¼Œä½¿ç”¨jsonæ–‡ä»¶å­˜å‚¨inodeåˆ—è¡¨ã€‚ ```json { \"name\": \"/\", \"type\": \"d\", \"user\": \"root\", \"user_power\": 7, \"group_power\": 7, \"others_power\": 0, \"subfiles\": [ 0,0,1,2 ] } ``` æ–‡ä»¶çš„idå–å†³äºæ–‡ä»¶inodeåœ¨json arrayä¸­çš„index + æƒé™ æˆ‘ä»¬çš„æƒé™ç³»ç»Ÿä¾ç„¶ä»¿ç…§linuxï¼Œuser_powerä»£è¡¨ç”¨æˆ·ï¼ˆåˆ›å»ºè€…ï¼‰æœ¬èº«çš„æƒé™ï¼Œgroup_powerä»£è¡¨ç»„ç”¨æˆ·çš„æƒé™ï¼Œothers_powerä»£è¡¨å…¶ä»–ç”¨æˆ·çš„æƒé™ã€‚ ç»„ç”¨æˆ·å–å†³äºconfig.jsonæ–‡ä»¶ä¸­user groupï¼Œå¦‚ä¸‹ ```json \"user1\":{ \"password\":\"123456\", \"group\":[\"ftp\"], \"root\":1 } ``` ä¾‹å¦‚ä¸Šé¢çš„user1ä¿¡æ¯ä¸­ï¼Œftpé»˜è®¤ç”¨æˆ·æ˜¯å…¶ç»„ç”¨æˆ·æˆå‘˜ æƒé™æ•°å­—åˆ†åˆ«ä»£è¡¨ä¸‰ä½æ•°å­—çš„boolå€¼ï¼Œå½“ç¬¬ä¸€ä½ä¸º1ï¼Œä¹Ÿå°±æ˜¯100æ—¶å€¼ä¸º4ï¼Œä»£è¡¨æœ‰ä¸‹è½½æŸ¥çœ‹æƒé™ï¼Œå½“ç¬¬äºŒä½ä¸º1ï¼Œä¹Ÿå°±æ˜¯010æ—¶å€¼ä¸º2ï¼Œä»£è¡¨æœ‰ä¸‹è½½æƒé™ï¼Œå‰©ä½™ä¸€ä½ç•™ä½œæ‰©å±•ã€‚ + tip **æ—¢ç„¶æ¯ä¸€æ ·éƒ½è·Ÿlinuxç±»ä¼¼ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨linuxçš„æ–‡ä»¶ç³»ç»Ÿï¼Ÿ** > å› ä¸ºæˆ‘ä»¬å¯¹é¡¹ç›®çš„è¦æ±‚æ˜¯å¯æ‰©å±•æ€§ï¼Œå¦‚æœç›´æ¥ä½¿ç”¨linuxçš„æ–‡ä»¶ç³»ç»Ÿï¼Œå¦‚æœæˆ‘ä»¬ä»¥åéœ€è¦æ‰©å±•å…¶ä½™æƒé™ï¼Œä¾‹å¦‚æ›´æ”¹æ–‡ä»¶çš„æƒé™ï¼Œåˆ é™¤æ–‡ä»¶çš„æƒé™ï¼Œæˆ‘ä»¬çš„æ‰©å±•å°±ä¼šéå¸¸å›°éš¾ã€‚ ## æ•°æ®ä¼ è¾“ ### ä¸»åŠ¨æ¨¡å¼ ä¸»åŠ¨æ¨¡å¼ä¸‹ï¼Œéœ€è¦å®¢æˆ·ç«¯å‘ŠçŸ¥æœåŠ¡å™¨ç«¯å£ å› æ­¤æœåŠ¡å™¨éœ€è¦ä½œä¸ºå®¢æˆ·ç«¯è¿æ¥ç”¨æˆ·æœºï¼Œæ­¤æ—¶æ— éœ€è€ƒè™‘ç«¯å£çš„é—®é¢˜ã€‚ ### è¢«åŠ¨æ¨¡å¼ è¢«åŠ¨æ¨¡å¼ä¸‹ï¼Œéœ€è¦æœåŠ¡ç«¯å‘ŠçŸ¥å®¢æˆ·ç«¯æ•°æ®ç«¯å£ æ­¤æ—¶æœåŠ¡å™¨ä½œä¸ºæ•°æ®ä¼ è¾“çš„æœåŠ¡å™¨ç­‰å¾…å®¢æˆ·ç«¯è¿æ¥ã€‚ æœåŠ¡å™¨éœ€è¦æ‰«ææœåŠ¡å™¨ä¸Šçš„å¯ç”¨ç«¯å£å‘ŠçŸ¥å®¢æˆ·ç«¯ä»¥è¿æ¥ã€‚ + ç«¯å£æ±  ç§‰æŒæ‰€æœ‰èµ„æºçš†å¯æ§çš„ç†å¿µï¼Œæˆ‘ä»¬å°†ç«¯å£ä¹Ÿä½œä¸ºé…ç½®æ”¾å…¥é…ç½®æ–‡ä»¶ï¼Œç«¯å£ä½œä¸ºèµ„æºä¸ºå®¢æˆ·ç«¯åˆ†é…ï¼Œè¾¾åˆ°å¯æ§çš„æ–¹å¼ã€‚ **å‡è®¾æˆ‘ä»¬é‡‡ç”¨ä¸´æ—¶æ‰«æç«¯å£çš„æ–¹å¼ï¼Œé‚£ä¹ˆä¸å¾—ä¸è€ƒè™‘çš„é—®é¢˜æ˜¯ï¼Œåœ¨æ‰«æåˆ°ç©ºé—²ç«¯å£åï¼Œä½†æ˜¯è¯¥ç«¯å£å®é™…ä¸Šå±äºserviceæ‰€ä½¿ç”¨çš„ç«¯å£ï¼ˆå‡ºç°æš‚æ—¶ç©ºé—²ï¼‰ï¼Œåœ¨æ‰«æåˆ°ç¡®è®¤è¿æ¥è¿™æ®µæ—¶é—´å†…ï¼Œè¯¥ç«¯å£æœ‰å¯èƒ½è¢«é‡æ–°å ç”¨çš„é—®é¢˜ã€‚å› æ­¤æˆ‘ä»¬çš„æ–¹æ¡ˆæ—¢é¿å…æœåŠ¡å ç”¨ftpç«¯å£çš„æƒ…å†µï¼Œä¹Ÿé¿å…ftpå ç”¨æœåŠ¡ç«¯å£çš„æƒ…å†µï¼ŒåŒæ—¶å‡å°‘ä»£ç å¼€å‘ä»£ä»·** ## source [download](https://atomgit.com/openeuler123/nihaopeng)"},"/docs/projects/IntelligentAudioChat/index.html":{"title":"","content":""},"/docs/projects/index.html":{"title":"some tutorials about the configure","content":"# some tutorials about the configure some tutorials about the configure > **click the sidebar to open markdown file** >! [read from the first file](./ascend_matrix/index.html)"},"/docs/projects/riscv-telechat-openeuler/openeuler/openeuler.html":{"title":"openeuler çƒ§å½•åˆ°licheepi4a","content":"# openeuler çƒ§å½•åˆ°licheepi4a è·Ÿç€[å®˜ç½‘æ•™ç¨‹](https://docs.openeuler.org/zh/docs/24.03_LTS/docs/Installation/RISC V LicheePi4A.html)èµ° tipsï¼š + 1ï¼Œæ˜¾ç¤ºå™¨å¯èƒ½ä¼šæ˜¾ç¤ºä¸å‡ºæ¥ï¼Œç”¨ä¸²å£ç›´è¿è°ƒè¯• + 2ï¼Œwifiæ— æ³•æ­£å¸¸listï¼Œç½‘å£ç›´è¿ï¼Œä¸è¦é‚£ç§éœ€è¦ç™»é™†çš„ç½‘ç»œ + 3ï¼Œç™»å½•è´¦å·ï¼šrootï¼Œå¯†ç ï¼šopenEuler12#$ + 4ï¼Œå¦‚æœæ— æ³•ä¸‹è½½ä¸œè¥¿ä¸æ˜¯å› ä¸ºæºçš„é—®é¢˜ï¼Œçœ‹çœ‹ç½‘ç»œæ˜¯å¦è¿æ¥ä¸Šäº†"},"/docs/projects/riscv-telechat-openeuler/telechatç§»æ¤/telechat.html":{"title":"telechat-12Bç§»æ¤","content":"# telechat 12Bç§»æ¤ ## ä»hf mirrorä¸‹è½½æ¨¡å‹ åˆ«é—®æˆ‘ä¸ºå•¥ä¸åœ¨hfå®˜ç½‘ï¼Œæ¢¯å­æµé‡å¤Ÿç”¨éšä¾¿æ + download hfd,a tool to download base on aria2 `wget https://hf mirror.com/hfd/hfd.sh` `chmod a+x hfd.sh` + è®¾ç½®hugginfaceä¸‹è½½ç¯å¢ƒå˜é‡ `export HF_ENDPOINT https://hf mirror.com` + ä¸‹è½½æ¨¡å‹ `./hfd.sh Tele AI/TeleChat 12B tool aria2c x 4` >å‡ºç°æŠ¥é”™çœ‹ä¸‹é¢çš„æ•™ç¨‹ + ä¸‹è½½æ•°æ®é›†ï¼ˆç”¨äºè®°å½•ï¼Œä¸ç”¨æ‰§è¡Œï¼‰ `./hfd.sh wikitext dataset tool aria2c x 4` ## ariaç¼–è¯‘ å¦‚æœä½ è·Ÿç€ä¸Šé¢çš„æ­¥éª¤èµ°ï¼Œé‚£ä¸€å®šä¼šåˆ°è¿™é‡Œï¼Œå› ä¸ºyumæºæ²¡æœ‰aria2å®‰è£…åŒ…ï¼Œä¹ + å…‹éš†aria2æºç  `git clone https://github.com/aria2/aria2.git` + build `autoreconf i` autoreconfè£…äº†è¿è¡Œä¸äº†ï¼Œç¼ºå°‘autopoint + yum install gettext devel è£…ä¾èµ– `yum install openssl devel zlib zlib devel` `./configure ARIA2_STATIC yes` error:A compiler with support for C++11 language features is required. + solution:yum install g++ `make install j4` ## git lfsç¼–è¯‘ å¦‚æœä½ è·Ÿç€ä¸Šé¢çš„æ­¥éª¤èµ°ï¼Œé‚£ä¸€å®šä¼šåˆ°è¿™é‡Œï¼Œå› ä¸ºgit lfså¦¹æœ‰riscv64ç‰ˆæœ¬çš„ï¼ŒèšŒ + å®‰è£…go `yum install golang` + ç¼–è¯‘ 1,å…‹éš†git lfs 2,`go env w GOPROXY https://goproxy.cn` 3,æ·»åŠ binç›®å½•åˆ°ç¯å¢ƒå˜é‡ 4ï¼Œæ‰§è¡Œ`git lfs install` ## openeuler riscvä¸Šç¼–è¯‘pytorch + ä¾èµ– `dnf install python3 {hypothesis,psutil,pyyaml,requests,sympy,filelock,networkx,jinja2,fsspec,packaging,numpy,venv}` + ä¸‹è½½giteeé¢„ç¼–è¯‘riscv whl `git clone recursive https://github.com/pytorch/pytorch`çº¦4ä¸ªGBï¼Œå»ºè®®ç”¨æ¢¯å­ `cd pytorch && python3 setup.py develop` + æµ‹è¯• <div style \"text align:center;\"><img src \"QQ20241210 150124.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ## å®‰è£…transformers `git clone https://github.com/huggingface/transformers.git` >ä¸‹é¢è¿™å¥é€†å¤©æ“ä½œæ®è¯´æ˜¯ninjaå’Œcmakeäº’ç›¸ä¾èµ–å¯¼è‡´æ— é™é€’å½’çš„é—®é¢˜ï¼Œä¹ `dnf install cmake python3 devel` å®‰è£…rust compiler(é€†å¤©) `export RUSTUP_DIST_SERVER https://mirrors.ustc.edu.cn/rust static` `export RUSTUP_UPDATE_ROOT https://mirrors.ustc.edu.cn/rust static/rustup` `curl proto ' https' tlsv1.2 sSf https://sh.rustup.rs sh` `pip install 'transformers[torch]'` ## è¿è¡Œpython inference + é—®é¢˜1ï¼šéœ€è¦å®‰è£…flash attnï¼Œç„¶è€Œè¯¥åº“ä¾èµ–cudaæ”¯æŒ ä¿®æ”¹12Bæ¨¡å‹ä¸­çš„modeling_telechat.pyçš„357ï¼Œå¢åŠ configä¸­çš„flash attnåˆ¤æ–­ä»¥å–æ¶ˆåˆå§‹åŒ–FlashSelfAttentionç±»äº§ç”Ÿé”™è¯¯ï¼Œå¦‚ä¸‹å›¾ï¼š <div style \"text align:center;\"><img src \"QQ20241210 223624.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> è®°å¾—ä¿®æ”¹configä¸­çš„å‚æ•° <div style \"text align:center;\"><img src \"QQ20241211 091833.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + ä¿®æ”¹è™šæ‹Ÿç¯å¢ƒä¸­.venv/lib/python3.11/site packages/transformers/generation/utils.pyç”¨ä»¥æ˜¾ç¤ºè¿›åº¦ <div style \"text align:center;\"><img src \"QQ20241211 135143.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> æ¨ç†è„šæœ¬å¦‚ä¸‹ï¼š ```python import os import torch from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig tokenizer AutoTokenizer.from_pretrained('../models/12B', trust_remote_code True) model AutoModelForCausalLM.from_pretrained('../models/12B', trust_remote_code True, torch_dtype torch.float16) device \"cpu\" model.to(device) generate_config GenerationConfig.from_pretrained('../models/12B') question \"ä½ å¥½ï¼ä½ æ˜¯è°ï¼Ÿ\" answer, history model.chat(tokenizer tokenizer, question question, history [], generation_config generate_config, stream False) print(answer) ``` `python3 test.py` ## huggingfaceè½¬ä¸ºonnx `pip install optimum[exporters]` [ä¸‹è½½12Bæ¨¡å‹](#ä»hf mirrorä¸‹è½½æ¨¡å‹) [ä¿®æ”¹æ¨ç†æ¨¡å‹](#è¿è¡Œpython inference) è¿è¡Œä»¥ä¸‹è„šæœ¬è½¬æ¢ä¸ºonnxæ ¼å¼ ```python import os import torch from transformers import AutoTokenizer, AutoModelForCausalLM import onnx import onnxruntime as ort import numpy as np os.environ['TRANSFORMERS_OFFLINE'] '1' class ModelWrapper(torch.nn.Module): def __init__(self, model): super().__init__() self.model model def forward(self, input_ids, attention_mask): # æ˜¾å¼ä¼ å…¥ use_cache Falseï¼Œç¡®ä¿ä¸ä½¿ç”¨ past_key_values outputs self.model(input_ids input_ids, attention_mask attention_mask, use_cache False) # åªè¿”å› logitsï¼Œä»¥é¿å…å¤æ‚çš„è¾“å‡ºç»“æ„å¯¼è‡´çš„é—®é¢˜ return outputs.logits def export_model_to_onnx(model_name, output_path, opset_version 13): tokenizer AutoTokenizer.from_pretrained(model_name, trust_remote_code True) model AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code True) model.eval() model.config.use_cache False text \"This is a sample input for ONNX export.\" inputs tokenizer(text, return_tensors \"pt\") # ç”¨åŒ…è£…å™¨æ›¿æ¢åŸå§‹æ¨¡å‹ wrapped_model ModelWrapper(model) wrapped_model.eval() input_names [\"input_ids\", \"attention_mask\"] output_names [\"logits\"] dynamic_axes { \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"}, \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"}, \"logits\": {0: \"batch_size\", 1: \"sequence_length\"} } # å¯¼å‡ºä¸º ONNX torch.onnx.export( wrapped_model, args (inputs[\"input_ids\"], inputs[\"attention_mask\"]), f output_path, input_names input_names, output_names output_names, dynamic_axes dynamic_axes, opset_version opset_version, export_params True, do_constant_folding True, ) print(f\"æ¨¡å‹å·²æˆåŠŸå¯¼å‡ºåˆ° {output_path}\") def validate_onnx_model(model_path, model_name, text): tokenizer AutoTokenizer.from_pretrained(model_name, trust_remote_code True) model AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code True) model.eval() model.c ``` ## onnxä½¿ç”¨hhbè½¬ä¸ºäºŒè¿›åˆ¶å¯æ‰§è¡Œæ–‡ä»¶ ## åˆ©ç”¨npuåŠ é€Ÿæ‰§è¡Œ ç›®å‰ï¼ˆ2024 12 11ï¼‰openeuleræš‚ä¸æ”¯æŒnpuã€‚ 2025 3 12, å½“å‰ä½¿ç”¨openeuler24.03 sp1ï¼Œå·²æ”¯æŒnpu æ‰‹åŠ¨æŒ‚è½½é©±åŠ¨ï¼Œå¦‚ä¸‹ï¼š ```bash insmod /lib/modules/6.6.0 72.0.0.76.oe2403sp1.riscv64/kernel/drivers/soc/xuantie/nna/img_mem/img_mem.ko.xz modprobe vha onchipmem_phys_start 0xffe0000000 onchipmem_size 0x100000 freq_khz 792000 insmod /lib/modules/6.6.0 72.0.0.76.oe2403sp1.riscv64/kernel/drivers/soc/xuantie/nna/vha/vha_info.ko.xz chmod a+rw /dev/vha0 lsmod ``` æ­¤æ—¶/devæ–‡ä»¶å¤¹å‡ºç°vha0"},"/docs/projects/sd1.5-fine-tuning/sd.html":{"title":"","content":"<style> pre { overflow y: auto; max height: 300px; } </style> # stable diffusion fine tuning ## ubuntuå®‰è£…cuda toolkit `sudo apt install nvidia cuda toolkit` `nvcc V`éªŒè¯å®‰è£…å¹¶æŸ¥çœ‹cudaç‰ˆæœ¬ ## å®‰è£…pytorch è€ç”Ÿå¸¸è°ˆäº†ï¼Œè¿™é‡Œä¸å†ç»™æ•™ç¨‹ï¼Œéœ€è¦æ³¨æ„çš„å‡ ä¸ªç‚¹æ˜¯ï¼Œ + ä¸‹è½½çš„pytorchç‰ˆæœ¬éœ€è¦å’Œcudaç‰ˆæœ¬å¯¹åº” + torchç‰ˆæœ¬å’ŒGPUè®¡ç®—æ¶æ„å¯¹åº” ## ä¸‹è½½stable diffusioné¢„è®­ç»ƒæ¨¡å‹ ### ä»hf mirrorä¸‹è½½æ¨¡å‹ [ç‚¹è¿™é‡Œçœ‹æ›´è¯¦ç»†æ•™ç¨‹](../riscv telechat openeuler/telechatç§»æ¤/telechat.html) åˆ«é—®æˆ‘ä¸ºå•¥ä¸åœ¨hfå®˜ç½‘ï¼Œæ¢¯å­æµé‡å¤Ÿç”¨éšä¾¿æ + download hfd,a tool to download base on aria2 `wget https://hf mirror.com/hfd/hfd.sh` `chmod a+x hfd.sh` + è®¾ç½®hugginfaceä¸‹è½½ç¯å¢ƒå˜é‡ `export HF_ENDPOINT https://hf mirror.com` + ä¸‹è½½æ¨¡å‹ `./hfd.sh Tele AI/TeleChat 12B tool aria2c x 4` + ä¸‹è½½æ•°æ®é›†ï¼ˆç”¨äºè®°å½•ï¼Œä¸ç”¨æ‰§è¡Œï¼‰ `./hfd.sh wikitext dataset tool aria2c x 4` ## é¢„å¤„ç†æ•°æ® ä½ çš„è‡ªå®šä¹‰æ•°æ®é›†æ”¾åœ¨ä¸€ä¸ªæ–‡ä»¶å¤¹å†…ï¼Œæˆ‘ä»¬ä½¿ç”¨å…ƒæ•°æ®çš„æ–¹å¼è®­ç»ƒï¼Œæ”¾å›¾ç‰‡çš„åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹åˆ›å»ºä¸€ä¸ª`metadata.jsonl`æ–‡ä»¶ï¼Œæ–‡ä»¶æ ¼å¼å¦‚ä¸‹ï¼š ```json { \"file_name\": \"2.jpg\", \"text\": \"Off white flat garden layout,irregularity,water,mellow\" } { \"file_name\": \"3.jpg\", \"text\": \"Off white flat garden layout,irregularity,line,order\" } { \"file_name\": \"4.jpg\", \"text\": \"Off white flat garden layout,irregularity,bridge,water,line,mellow\" } ... ``` ## å¾®è°ƒ å¾®è°ƒè„šæœ¬ï¼Œå…¶ä¸­çš„MODEL_NAMEèµ‹å€¼ä¸ºä½ ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œ OUTPUT_DIRä¸ºå¾®è°ƒä¸­é—´æ–‡ä»¶ DATASET_NAMEä¸ºæ”¾æ•°æ®é›†çš„è·¯å¾„ å¦‚æœä½ æœ‰å¤šä¸ªgpuè®¾å¤‡çš„è¯ï¼Œå¯ä»¥å°†shellè„šæœ¬ä¸­çš„pythonæ¢ä¸ºaccelerateåˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚æœèƒ½åˆ°ä½¿ç”¨accelerateè¿™ä¸€æ­¥ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„é—®é¢˜ç›¸ä¿¡ä½ ä¹Ÿæ²¡æœ‰é—®é¢˜äº†ï¼‰ ```bash export MODEL_NAME \"../stable diffusion v1 5\" export OUTPUT_DIR \"./out_lora2\" export DATASET_NAME \"./data/layout\" python train.py \\ pretrained_model_name_or_path $MODEL_NAME \\ dataset_name $DATASET_NAME \\ dataloader_num_workers 0 \\ resolution 512 center_crop random_flip \\ train_batch_size 10 \\ gradient_accumulation_steps 4 \\ max_train_steps 15000 \\ learning_rate 1e 04 \\ max_grad_norm 1 \\ lr_scheduler \"cosine\" lr_warmup_steps 0 \\ output_dir ${OUTPUT_DIR} \\ checkpointing_steps 500 \\ seed 3407 \\ report_to wandb \\ ``` train.pyæ–‡ä»¶æ˜¯loraçš„è®­ç»ƒè„šæœ¬ã€‚ ```python #!/usr/bin/env python # coding utf 8 # Copyright 2024 The HuggingFace Inc. team. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE 2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"Fine tuning script for Stable Diffusion for text2image with support for LoRA.\"\"\" import argparse import logging import math import os import random import shutil from contextlib import nullcontext from pathlib import Path import datasets import numpy as np import torch import torch.nn.functional as F import torch.utils.checkpoint import transformers from accelerate import Accelerator from accelerate.logging import get_logger from accelerate.utils import ProjectConfiguration, set_seed from datasets import load_dataset from huggingface_hub import create_repo, upload_folder from packaging import version from peft import LoraConfig from peft.utils import get_peft_model_state_dict from torchvision import transforms from tqdm.auto import tqdm from transformers import CLIPTextModel, CLIPTokenizer import diffusers from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel from diffusers.optimization import get_scheduler from diffusers.training_utils import cast_training_params, compute_snr from diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card from diffusers.utils.import_utils import is_xformers_available from diffusers.utils.torch_utils import is_compiled_module if is_wandb_available(): import wandb # Will error if the minimal version of diffusers is not installed. Remove at your own risks. # check_min_version(\"0.31.0.dev0\") logger get_logger(__name__, log_level \"INFO\") def save_model_card( repo_id: str, images: list None, base_model: str None, dataset_name: str None, repo_folder: str None, ): img_str \"\" if images is not None: for i, image in enumerate(images): image.save(os.path.join(repo_folder, f\"image_{i}.png\")) img_str + f\"<div style \"text align:center;\"><img src \"./image_{i}.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div>\\n\" model_description f\"\"\" # LoRA text2image fine tuning {repo_id} These are LoRA adaption weights for {base_model}. The weights were fine tuned on the {dataset_name} dataset. You can find some example images in the following. \\n {img_str} \"\"\" model_card load_or_create_model_card( repo_id_or_path repo_id, from_training True, license \"creativeml openrail m\", base_model base_model, model_description model_description, inference True, ) tags [ \"stable diffusion\", \"stable diffusion diffusers\", \"text to image\", \"diffusers\", \"diffusers training\", \"lora\", ] model_card populate_model_card(model_card, tags tags) model_card.save(os.path.join(repo_folder, \"README.md\")) def log_validation( pipeline, args, accelerator, epoch, is_final_validation False, ): logger.info( f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\" f\" {args.validation_prompt}.\" ) pipeline pipeline.to(accelerator.device) pipeline.set_progress_bar_config(disable True) generator torch.Generator(device accelerator.device) if args.seed is not None: generator generator.manual_seed(args.seed) images [] if torch.backends.mps.is_available(): autocast_ctx nullcontext() else: autocast_ctx torch.autocast(accelerator.device.type) with autocast_ctx: for _ in range(args.num_validation_images): images.append(pipeline(args.validation_prompt, num_inference_steps 30, generator generator).images[0]) for tracker in accelerator.trackers: phase_name \"test\" if is_final_validation else \"validation\" if tracker.name \"tensorboard\": np_images np.stack([np.asarray(img) for img in images]) tracker.writer.add_images(phase_name, np_images, epoch, dataformats \"NHWC\") if tracker.name \"wandb\": tracker.log( { phase_name: [ wandb.Image(image, caption f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images) ] } ) return images def parse_args(): parser argparse.ArgumentParser(description \"Simple example of a training script.\") parser.add_argument( \" pretrained_model_name_or_path\", type str, default None, required True, help \"Path to pretrained model or model identifier from huggingface.co/models.\", ) parser.add_argument( \" revision\", type str, default None, required False, help \"Revision of pretrained model identifier from huggingface.co/models.\", ) parser.add_argument( \" variant\", type str, default None, help \"Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\", ) parser.add_argument( \" dataset_name\", type str, default None, help ( \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\" \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\" \" or to a folder containing files that ğŸ¤— Datasets can understand.\" ), ) parser.add_argument( \" dataset_config_name\", type str, default None, help \"The config of the Dataset, leave as None if there's only one config.\", ) parser.add_argument( \" train_data_dir\", type str, default None, help ( \"A folder containing the training data. Folder contents must follow the structure described in\" \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\" \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\" ), ) parser.add_argument( \" image_column\", type str, default \"image\", help \"The column of the dataset containing an image.\" ) parser.add_argument( \" caption_column\", type str, default \"text\", help \"The column of the dataset containing a caption or a list of captions.\", ) parser.add_argument( \" validation_prompt\", type str, default None, help \"A prompt that is sampled during training for inference.\" ) parser.add_argument( \" num_validation_images\", type int, default 4, help \"Number of images that should be generated during validation with `validation_prompt`.\", ) parser.add_argument( \" validation_epochs\", type int, default 1, help ( \"Run fine tuning validation every X epochs. The validation process consists of running the prompt\" \" `args.validation_prompt` multiple times: `args.num_validation_images`.\" ), ) parser.add_argument( \" max_train_samples\", type int, default None, help ( \"For debugging purposes or quicker training, truncate the number of training examples to this \" \"value if set.\" ), ) parser.add_argument( \" output_dir\", type str, default \"sd model finetuned lora\", help \"The output directory where the model predictions and checkpoints will be written.\", ) parser.add_argument( \" cache_dir\", type str, default None, help \"The directory where the downloaded models and datasets will be stored.\", ) parser.add_argument(\" seed\", type int, default None, help \"A seed for reproducible training.\") parser.add_argument( \" resolution\", type int, default 512, help ( \"The resolution for input images, all the images in the train/validation dataset will be resized to this\" \" resolution\" ), ) parser.add_argument( \" center_crop\", default False, action \"store_true\", help ( \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\" \" cropped. The images will be resized to the resolution first before cropping.\" ), ) parser.add_argument( \" random_flip\", action \"store_true\", help \"whether to randomly flip images horizontally\", ) parser.add_argument( \" train_batch_size\", type int, default 16, help \"Batch size (per device) for the training dataloader.\" ) parser.add_argument(\" num_train_epochs\", type int, default 100) parser.add_argument( \" max_train_steps\", type int, default None, help \"Total number of training steps to perform. If provided, overrides num_train_epochs.\", ) parser.add_argument( \" gradient_accumulation_steps\", type int, default 1, help \"Number of updates steps to accumulate before performing a backward/update pass.\", ) parser.add_argument( \" gradient_checkpointing\", action \"store_true\", help \"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\", ) parser.add_argument( \" learning_rate\", type float, default 1e 4, help \"Initial learning rate (after the potential warmup period) to use.\", ) parser.add_argument( \" scale_lr\", action \"store_true\", default False, help \"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\", ) parser.add_argument( \" lr_scheduler\", type str, default \"constant\", help ( 'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",' ' \"constant\", \"constant_with_warmup\"]' ), ) parser.add_argument( \" lr_warmup_steps\", type int, default 500, help \"Number of steps for the warmup in the lr scheduler.\" ) parser.add_argument( \" snr_gamma\", type float, default None, help \"SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. \" \"More details here: https://arxiv.org/abs/2303.09556.\", ) parser.add_argument( \" use_8bit_adam\", action \"store_true\", help \"Whether or not to use 8 bit Adam from bitsandbytes.\" ) parser.add_argument( \" allow_tf32\", action \"store_true\", help ( \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\" \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat 32 tf32 on ampere devices\" ), ) parser.add_argument( \" dataloader_num_workers\", type int, default 0, help ( \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\" ), ) parser.add_argument(\" adam_beta1\", type float, default 0.9, help \"The beta1 parameter for the Adam optimizer.\") parser.add_argument(\" adam_beta2\", type float, default 0.999, help \"The beta2 parameter for the Adam optimizer.\") parser.add_argument(\" adam_weight_decay\", type float, default 1e 2, help \"Weight decay to use.\") parser.add_argument(\" adam_epsilon\", type float, default 1e 08, help \"Epsilon value for the Adam optimizer\") parser.add_argument(\" max_grad_norm\", default 1.0, type float, help \"Max gradient norm.\") parser.add_argument(\" push_to_hub\", action \"store_true\", help \"Whether or not to push the model to the Hub.\") parser.add_argument(\" hub_token\", type str, default None, help \"The token to use to push to the Model Hub.\") parser.add_argument( \" prediction_type\", type str, default None, help \"The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or leave `None`. If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.\", ) parser.add_argument( \" hub_model_id\", type str, default None, help \"The name of the repository to keep in sync with the local `output_dir`.\", ) parser.add_argument( \" logging_dir\", type str, default \"logs\", help ( \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\" \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\" ), ) parser.add_argument( \" mixed_precision\", type str, default None, choices [\"no\", \"fp16\", \"bf16\"], help ( \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch > \" \" 1.10.and an Nvidia Ampere GPU. Default to the value of accelerate config of the current system or the\" \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\" ), ) parser.add_argument( \" report_to\", type str, default \"tensorboard\", help ( 'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`' ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.' ), ) parser.add_argument(\" local_rank\", type int, default 1, help \"For distributed training: local_rank\") parser.add_argument( \" checkpointing_steps\", type int, default 500, help ( \"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\" \" training using ` resume_from_checkpoint`.\" ), ) parser.add_argument( \" checkpoints_total_limit\", type int, default None, help (\"Max number of checkpoints to store.\"), ) parser.add_argument( \" resume_from_checkpoint\", type str, default None, help ( \"Whether training should be resumed from a previous checkpoint. Use a path saved by\" ' ` checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.' ), ) parser.add_argument( \" enable_xformers_memory_efficient_attention\", action \"store_true\", help \"Whether or not to use xformers.\" ) parser.add_argument(\" noise_offset\", type float, default 0, help \"The scale of noise offset.\") parser.add_argument( \" rank\", type int, default 4, help (\"The dimension of the LoRA update matrices.\"), ) args parser.parse_args() env_local_rank int(os.environ.get(\"LOCAL_RANK\", 1)) if env_local_rank ! 1 and env_local_rank ! args.local_rank: args.local_rank env_local_rank # Sanity checks if args.dataset_name is None and args.train_data_dir is None: raise ValueError(\"Need either a dataset name or a training folder.\") return args DATASET_NAME_MAPPING { \"lambdalabs/naruto blip captions\": (\"image\", \"text\"), } def main(): args parse_args() if args.report_to \"wandb\" and args.hub_token is not None: raise ValueError( \"You cannot use both report_to wandb and hub_token due to a security risk of exposing your token.\" \" Please use `huggingface cli login` to authenticate with the Hub.\" ) logging_dir Path(args.output_dir, args.logging_dir) accelerator_project_config ProjectConfiguration(project_dir args.output_dir, logging_dir logging_dir) accelerator Accelerator( gradient_accumulation_steps args.gradient_accumulation_steps, mixed_precision args.mixed_precision, log_with args.report_to, project_config accelerator_project_config, ) # Disable AMP for MPS. if torch.backends.mps.is_available(): accelerator.native_amp False # Make one log on every process with the configuration for debugging. logging.basicConfig( format \"%(asctime)s %(levelname)s %(name)s %(message)s\", datefmt \"%m/%d/%Y %H:%M:%S\", level logging.INFO, ) logger.info(accelerator.state, main_process_only False) if accelerator.is_local_main_process: datasets.utils.logging.set_verbosity_warning() transformers.utils.logging.set_verbosity_warning() diffusers.utils.logging.set_verbosity_info() else: datasets.utils.logging.set_verbosity_error() transformers.utils.logging.set_verbosity_error() diffusers.utils.logging.set_verbosity_error() # If passed along, set the training seed now. if args.seed is not None: set_seed(args.seed) # Handle the repository creation if accelerator.is_main_process: if args.output_dir is not None: os.makedirs(args.output_dir, exist_ok True) if args.push_to_hub: repo_id create_repo( repo_id args.hub_model_id or Path(args.output_dir).name, exist_ok True, token args.hub_token ).repo_id # Load scheduler, tokenizer and models. noise_scheduler DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder \"scheduler\") tokenizer CLIPTokenizer.from_pretrained( args.pretrained_model_name_or_path, subfolder \"tokenizer\", revision args.revision ) text_encoder CLIPTextModel.from_pretrained( args.pretrained_model_name_or_path, subfolder \"text_encoder\", revision args.revision ) vae AutoencoderKL.from_pretrained( args.pretrained_model_name_or_path, subfolder \"vae\", revision args.revision, variant args.variant ) unet UNet2DConditionModel.from_pretrained( args.pretrained_model_name_or_path, subfolder \"unet\", revision args.revision, variant args.variant ) # freeze parameters of models to save more memory unet.requires_grad_(False) vae.requires_grad_(False) text_encoder.requires_grad_(False) # For mixed precision training we cast all non trainable weights (vae, non lora text_encoder and non lora unet) to half precision # as these weights are only used for inference, keeping weights in full precision is not required. weight_dtype torch.float32 if accelerator.mixed_precision \"fp16\": weight_dtype torch.float16 elif accelerator.mixed_precision \"bf16\": weight_dtype torch.bfloat16 # Freeze the unet parameters before adding adapters for param in unet.parameters(): param.requires_grad_(False) unet_lora_config LoraConfig( r args.rank, lora_alpha args.rank, init_lora_weights \"gaussian\", target_modules [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"], ) # Move unet, vae and text_encoder to device and cast to weight_dtype unet.to(accelerator.device, dtype weight_dtype) vae.to(accelerator.device, dtype weight_dtype) text_encoder.to(accelerator.device, dtype weight_dtype) # Add adapter and make sure the trainable params are in float32. unet.add_adapter(unet_lora_config) if args.mixed_precision \"fp16\": # only upcast trainable parameters (LoRA) into fp32 cast_training_params(unet, dtype torch.float32) if args.enable_xformers_memory_efficient_attention: if is_xformers_available(): import xformers xformers_version version.parse(xformers.__version__) if xformers_version version.parse(\"0.0.16\"): logger.warning( \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\" ) unet.enable_xformers_memory_efficient_attention() else: raise ValueError(\"xformers is not available. Make sure it is installed correctly\") lora_layers filter(lambda p: p.requires_grad, unet.parameters()) if args.gradient_checkpointing: unet.enable_gradient_checkpointing() # Enable TF32 for faster training on Ampere GPUs, # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat 32 tf32 on ampere devices if args.allow_tf32: torch.backends.cuda.matmul.allow_tf32 True if args.scale_lr: args.learning_rate ( args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes ) # Initialize the optimizer if args.use_8bit_adam: try: import bitsandbytes as bnb except ImportError: raise ImportError( \"Please install bitsandbytes to use 8 bit Adam. You can do so by running `pip install bitsandbytes`\" ) optimizer_cls bnb.optim.AdamW8bit else: optimizer_cls torch.optim.AdamW optimizer optimizer_cls( lora_layers, lr args.learning_rate, betas (args.adam_beta1, args.adam_beta2), weight_decay args.adam_weight_decay, eps args.adam_epsilon, ) # Get the datasets: you can either provide your own training and evaluation files (see below) # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub). # In distributed training, the load_dataset function guarantees that only one local process can concurrently # download the dataset. if args.dataset_name is not None: # Downloading and loading a dataset from the hub. dataset load_dataset( args.dataset_name, args.dataset_config_name, cache_dir args.cache_dir, data_dir args.train_data_dir, ) else: data_files {} if args.train_data_dir is not None: data_files[\"train\"] os.path.join(args.train_data_dir, \"**\") dataset load_dataset( \"imagefolder\", data_files data_files, cache_dir args.cache_dir, ) # See more about loading custom images at # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder # Preprocessing the datasets. # We need to tokenize inputs and targets. column_names dataset[\"train\"].column_names # 6. Get the column names for input/target. dataset_columns DATASET_NAME_MAPPING.get(args.dataset_name, None) if args.image_column is None: image_column dataset_columns[0] if dataset_columns is not None else column_names[0] else: image_column args.image_column if image_column not in column_names: raise ValueError( f\" image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\" ) if args.caption_column is None: caption_column dataset_columns[1] if dataset_columns is not None else column_names[1] else: caption_column args.caption_column if caption_column not in column_names: raise ValueError( f\" caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\" ) # Preprocessing the datasets. # We need to tokenize input captions and transform the images. def tokenize_captions(examples, is_train True): captions [] for caption in examples[caption_column]: if isinstance(caption, str): captions.append(caption) elif isinstance(caption, (list, np.ndarray)): # take a random caption if there are multiple captions.append(random.choice(caption) if is_train else caption[0]) else: raise ValueError( f\"Caption column `{caption_column}` should contain either strings or lists of strings.\" ) inputs tokenizer( captions, max_length tokenizer.model_max_length, padding \"max_length\", truncation True, return_tensors \"pt\" ) return inputs.input_ids # Preprocessing the datasets. train_transforms transforms.Compose( [ transforms.Resize(args.resolution, interpolation transforms.InterpolationMode.BILINEAR), transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution), transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x), transforms.ToTensor(), transforms.Normalize([0.5], [0.5]), ] ) def unwrap_model(model): model accelerator.unwrap_model(model) model model._orig_mod if is_compiled_module(model) else model return model def preprocess_train(examples): images [image.convert(\"RGB\") for image in examples[image_column]] examples[\"pixel_values\"] [train_transforms(image) for image in images] examples[\"input_ids\"] tokenize_captions(examples) return examples with accelerator.main_process_first(): if args.max_train_samples is not None: dataset[\"train\"] dataset[\"train\"].shuffle(seed args.seed).select(range(args.max_train_samples)) # Set the training transforms train_dataset dataset[\"train\"].with_transform(preprocess_train) def collate_fn(examples): pixel_values torch.stack([example[\"pixel_values\"] for example in examples]) pixel_values pixel_values.to(memory_format torch.contiguous_format).float() input_ids torch.stack([example[\"input_ids\"] for example in examples]) return {\"pixel_values\": pixel_values, \"input_ids\": input_ids} # DataLoaders creation: train_dataloader torch.utils.data.DataLoader( train_dataset, shuffle True, collate_fn collate_fn, batch_size args.train_batch_size, num_workers args.dataloader_num_workers, ) # Scheduler and math around the number of training steps. # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation. num_warmup_steps_for_scheduler args.lr_warmup_steps * accelerator.num_processes if args.max_train_steps is None: len_train_dataloader_after_sharding math.ceil(len(train_dataloader) / accelerator.num_processes) num_update_steps_per_epoch math.ceil(len_train_dataloader_after_sharding / args.gradient_accumulation_steps) num_training_steps_for_scheduler ( args.num_train_epochs * num_update_steps_per_epoch * accelerator.num_processes ) else: num_training_steps_for_scheduler args.max_train_steps * accelerator.num_processes lr_scheduler get_scheduler( args.lr_scheduler, optimizer optimizer, num_warmup_steps num_warmup_steps_for_scheduler, num_training_steps num_training_steps_for_scheduler, ) # Prepare everything with our `accelerator`. unet, optimizer, train_dataloader, lr_scheduler accelerator.prepare( unet, optimizer, train_dataloader, lr_scheduler ) # We need to recalculate our total training steps as the size of the training dataloader may have changed. num_update_steps_per_epoch math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) if args.max_train_steps is None: args.max_train_steps args.num_train_epochs * num_update_steps_per_epoch if num_training_steps_for_scheduler ! args.max_train_steps * accelerator.num_processes: logger.warning( f\"The length of the 'train_dataloader' after 'accelerator.prepare' ({len(train_dataloader)}) does not match \" f\"the expected length ({len_train_dataloader_after_sharding}) when the learning rate scheduler was created. \" f\"This inconsistency may result in the learning rate scheduler not functioning properly.\" ) # Afterwards we recalculate our number of training epochs args.num_train_epochs math.ceil(args.max_train_steps / num_update_steps_per_epoch) # We need to initialize the trackers we use, and also store our configuration. # The trackers initializes automatically on the main process. if accelerator.is_main_process: accelerator.init_trackers(\"text2image fine tune\", config vars(args)) # Train! total_batch_size args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps logger.info(\"***** Running training *****\") logger.info(f\" Num examples {len(train_dataset)}\") logger.info(f\" Num Epochs {args.num_train_epochs}\") logger.info(f\" Instantaneous batch size per device {args.train_batch_size}\") logger.info(f\" Total train batch size (w. parallel, distributed & accumulation) {total_batch_size}\") logger.info(f\" Gradient Accumulation steps {args.gradient_accumulation_steps}\") logger.info(f\" Total optimization steps {args.max_train_steps}\") global_step 0 first_epoch 0 # Potentially load in the weights and states from a previous save if args.resume_from_checkpoint: if args.resume_from_checkpoint ! \"latest\": path os.path.basename(args.resume_from_checkpoint) else: # Get the most recent checkpoint dirs os.listdir(args.output_dir) dirs [d for d in dirs if d.startswith(\"checkpoint\")] dirs sorted(dirs, key lambda x: int(x.split(\" \")[1])) path dirs[ 1] if len(dirs) > 0 else None if path is None: accelerator.print( f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\" ) args.resume_from_checkpoint None initial_global_step 0 else: accelerator.print(f\"Resuming from checkpoint {path}\") accelerator.load_state(os.path.join(args.output_dir, path)) global_step int(path.split(\" \")[1]) initial_global_step global_step first_epoch global_step // num_update_steps_per_epoch else: initial_global_step 0 progress_bar tqdm( range(0, args.max_train_steps), initial initial_global_step, desc \"Steps\", # Only show the progress bar once on each machine. disable not accelerator.is_local_main_process, ) for epoch in range(first_epoch, args.num_train_epochs): unet.train() train_loss 0.0 for step, batch in enumerate(train_dataloader): with accelerator.accumulate(unet): # Convert images to latent space latents vae.encode(batch[\"pixel_values\"].to(dtype weight_dtype)).latent_dist.sample() latents latents * vae.config.scaling_factor # Sample noise that we'll add to the latents noise torch.randn_like(latents) if args.noise_offset: # https://www.crosslabs.org//blog/diffusion with offset noise noise + args.noise_offset * torch.randn( (latents.shape[0], latents.shape[1], 1, 1), device latents.device ) bsz latents.shape[0] # Sample a random timestep for each image timesteps torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device latents.device) timesteps timesteps.long() # Add noise to the latents according to the noise magnitude at each timestep # (this is the forward diffusion process) noisy_latents noise_scheduler.add_noise(latents, noise, timesteps) # Get the text embedding for conditioning encoder_hidden_states text_encoder(batch[\"input_ids\"], return_dict False)[0] # Get the target for loss depending on the prediction type if args.prediction_type is not None: # set prediction_type of scheduler if defined noise_scheduler.register_to_config(prediction_type args.prediction_type) if noise_scheduler.config.prediction_type \"epsilon\": target noise elif noise_scheduler.config.prediction_type \"v_prediction\": target noise_scheduler.get_velocity(latents, noise, timesteps) else: raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\") # Predict the noise residual and compute loss model_pred unet(noisy_latents, timesteps, encoder_hidden_states, return_dict False)[0] if args.snr_gamma is None: loss F.mse_loss(model_pred.float(), target.float(), reduction \"mean\") else: # Compute loss weights as per Section 3.4 of https://arxiv.org/abs/2303.09556. # Since we predict the noise instead of x_0, the original formulation is slightly changed. # This is discussed in Section 4.2 of the same paper. snr compute_snr(noise_scheduler, timesteps) mse_loss_weights torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim 1).min( dim 1 )[0] if noise_scheduler.config.prediction_type \"epsilon\": mse_loss_weights mse_loss_weights / snr elif noise_scheduler.config.prediction_type \"v_prediction\": mse_loss_weights mse_loss_weights / (snr + 1) loss F.mse_loss(model_pred.float(), target.float(), reduction \"none\") loss loss.mean(dim list(range(1, len(loss.shape)))) * mse_loss_weights loss loss.mean() # Gather the losses across all processes for logging (if we use distributed training). avg_loss accelerator.gather(loss.repeat(args.train_batch_size)).mean() train_loss + avg_loss.item() / args.gradient_accumulation_steps # Backpropagate accelerator.backward(loss) if accelerator.sync_gradients: params_to_clip lora_layers accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm) optimizer.step() lr_scheduler.step() optimizer.zero_grad() # Checks if the accelerator has performed an optimization step behind the scenes if accelerator.sync_gradients: progress_bar.update(1) global_step + 1 accelerator.log({\"train_loss\": train_loss}, step global_step) train_loss 0.0 if global_step % args.checkpointing_steps 0: if accelerator.is_main_process: # _before_ saving state, check if this save would set us over the `checkpoints_total_limit` if args.checkpoints_total_limit is not None: checkpoints os.listdir(args.output_dir) checkpoints [d for d in checkpoints if d.startswith(\"checkpoint\")] checkpoints sorted(checkpoints, key lambda x: int(x.split(\" \")[1])) # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit 1` checkpoints if len(checkpoints) > args.checkpoints_total_limit: num_to_remove len(checkpoints) args.checkpoints_total_limit + 1 removing_checkpoints checkpoints[0:num_to_remove] logger.info( f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\" ) logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\") for removing_checkpoint in removing_checkpoints: removing_checkpoint os.path.join(args.output_dir, removing_checkpoint) shutil.rmtree(removing_checkpoint) save_path os.path.join(args.output_dir, f\"checkpoint {global_step}\") accelerator.save_state(save_path) unwrapped_unet unwrap_model(unet) unet_lora_state_dict convert_state_dict_to_diffusers( get_peft_model_state_dict(unwrapped_unet) ) StableDiffusionPipeline.save_lora_weights( save_directory save_path, unet_lora_layers unet_lora_state_dict, safe_serialization True, ) logger.info(f\"Saved state to {save_path}\") logs {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]} progress_bar.set_postfix(**logs) if global_step > args.max_train_steps: break if accelerator.is_main_process: if args.validation_prompt is not None and epoch % args.validation_epochs 0: # create pipeline pipeline DiffusionPipeline.from_pretrained( args.pretrained_model_name_or_path, unet unwrap_model(unet), revision args.revision, variant args.variant, torch_dtype weight_dtype, ) images log_validation(pipeline, args, accelerator, epoch) del pipeline torch.cuda.empty_cache() # Save the lora layers accelerator.wait_for_everyone() if accelerator.is_main_process: unet unet.to(torch.float32) unwrapped_unet unwrap_model(unet) unet_lora_state_dict convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet)) StableDiffusionPipeline.save_lora_weights( save_directory args.output_dir, unet_lora_layers unet_lora_state_dict, safe_serialization True, ) # Final inference # Load previous pipeline if args.validation_prompt is not None: pipeline DiffusionPipeline.from_pretrained( args.pretrained_model_name_or_path, revision args.revision, variant args.variant, torch_dtype weight_dtype, ) # load attention processors pipeline.load_lora_weights(args.output_dir) # run inference images log_validation(pipeline, args, accelerator, epoch, is_final_validation True) if args.push_to_hub: save_model_card( repo_id, images images, base_model args.pretrained_model_name_or_path, dataset_name args.dataset_name, repo_folder args.output_dir, ) upload_folder( repo_id repo_id, folder_path args.output_dir, commit_message \"End of training\", ignore_patterns [\"step_*\", \"epoch_*\"], ) accelerator.end_training() if __name__ \"__main__\": main() ```"}}