{"/docs/tutorial/ascend/hccl/index.html":{"title":"HCCL","content":"# HCCL ## 概念 ### 基本术语 ![alt text](image.png) ![alt text](image 1.png) SDMA是节点内的不同RANK通信，RDMA是节点间的不同RANK通信。 ### buffer ![alt text](image 2.png) 从一个实体往另一个实体传输数据时，数据首先存到output buffer，再复制到CCL buffer，CCL buffer传输到另一个实体，另一个实体的CCL buffer再copy到input buffer，从而实现完整的传输过程。 ### 流 任务队列 通过Post/Wait进行同步。Post是将对应流的notify硬件寄存器的值置为1，Wait则是置为0，notify为1时，流会被阻塞在Wait语句。 ![alt text](image 3.png) ### 通信链路 ![alt text](image 4.png) ![alt text](image 5.png) ![alt text](image 6.png) ### 通信域 ![alt text](image 7.png) ### 硬件架构 ![alt text](image 8.png) ## 常见集合通信算法 ### Mesh ![alt text](image 10.png) 上面主要利用全双工的特性，一个NPU可以同时发送它自身的数据到所有的其他NPU，同时能够接收其他NPU发送到他的数据，由于全双工特性，两者可以同时进行，因此发送的时间仅为要处理数据的串行时间。 ### ring 适用与环形结构 ![alt text](image 11.png) ### RHD 递归二分与倍增 先做聚集，再两两之间交换。 ### pairwise（all2all） 原本是一个npu向所有的npu发送，如果是mesh那样全双工，将会存在资源争抢，由于节点只有一个RDMA网口，所以性能问题常出现在节点间数据通信。 即，当一个节点向其他节点采用mesh执行all2all，所有节点将会同时进行收发所有其他节点的数据，造成资源争抢，导致性能下降。 pairwise算法使用迭代间隔发送， 第一个迭代，1向2发送，2向3发送，3向4 第二个迭代，1向3，2向4，3向5. ![alt text](image 12.png) ### star（有根节点，例如reduce存在汇聚） ### hccl支持的原语 ![alt text](image 13.png) ## 通信业务流 ![alt text](image 14.png) 每一个rank都需要初始化通信域，每张卡都有自己的ip， ![alt text](image 15.png) 如果没有ranktable json，可以通过初始化root节点，并使用mpi广播通知其他rank。 ### 集合通信 ![alt text](image 16.png) 每一个rank都需要执行该函数 ## 算子实现 ![alt text](image 17.png) ![alt text](image 18.png) ![alt text](image 19.png)"},"/docs/tutorial/ascend/mindspore_develop/index.html":{"title":"算子开发教程","content":"# 算子开发教程 [教程](https://www.hiascend.com/edu/courses?activeTab %E7%AE%97%E5%AD%90%E5%BC%80%E5%8F%91) ## cann架构 ![alt text](image.png) cann类似于cuda，因此底层都是基于bisheng/nvcc编译器的。 算子是通过bisheng编译成动态库的。 ## 硬件架构 ![alt text](image 1.png) > AI Core负责执行标量、向量和张量相关的计算密集型算子，包括三种基础计算单元：Cube（矩阵）计算单元、Vector（向量）计算单元和Scalar（标量）计算单元，同时还包含存储单元（包括硬件存储和用于数据搬运的搬运单元）和控制单元。 + 耦合架构 ![alt text](image 2.png) + 分离架构 ![alt text](image 3.png) > 在AI Core中，输入缓冲区之后设置了一个存储转换单元（Memory Transfer Unit，MTE）。这是达芬奇架构的特色之一，主要的目的是为了以极高的效率实现数据格式的转换。比如前面提到GPU要通过矩阵计算来实现卷积，首先要通过Im2Col的方法把输入的网络和特征数据重新以一定的格式排列起来。这一步在GPU当中是通过软件来实现的，效率比较低下。达芬奇架构采用了一个专用的存储转换单元来完成这一过程，将这一步完全固化在硬件电路中，可以在很短的时间之内完成整个转置过程。 这样看，专用硬件上的卷积加速算是走到头了，欸 ## bisheng Test 测试代码如下： ```c // 文件名QuickStartDemo.cce #include \"acl/acl.h\" #include <stdio.h> #include <stdlib.h> #ifdef ASCENDC_CPU_DEBUG #define __aicore__ #else #define __aicore__ [aicore] #endif #define BLOCKS 4 #define CACHELINE_SZ 64 // Define a kernel __global__ __aicore__ void foo(__gm__ uint8_t *Out, int Stride) { Out[block_idx * Stride] block_idx; } int main(int argc, char *argv[]) { aclInit(nullptr); aclrtSetDevice(0); aclrtStream stream; aclrtCreateStream(&stream); uint8_t ExpectedValue[] {0, 1, 2, 3}; uint8_t *OutputValue nullptr; aclrtMalloc((void **)&OutputValue, BLOCKS, ACL_MEM_MALLOC_HUGE_FIRST); uint8_t InitValue[BLOCKS] {0}; aclrtMemcpyAsync((void *)OutputValue, sizeof(InitValue), InitValue, sizeof(InitValue), ACL_MEMCPY_HOST_TO_DEVICE, stream); aclrtSynchronizeStream(stream); // Invoke a kernel foo<<<BLOCKS, nullptr, stream>>>(OutputValue, CACHELINE_SZ); uint8_t *OutHost nullptr; aclrtMallocHost((void **)&OutHost, BLOCKS * CACHELINE_SZ); aclrtMemcpyAsync(OutHost, BLOCKS * CACHELINE_SZ, OutputValue, BLOCKS * CACHELINE_SZ, ACL_MEMCPY_DEVICE_TO_HOST, stream); aclrtSynchronizeStream(stream); for (int I 0; I < sizeof(ExpectedValue) / sizeof(uint8_t); I++) { printf(\"i%d\\t Expect: 0x%04x\\t\\t\\t\\tResult: 0x%04x\\n\", I, ExpectedValue[I], OutHost[I * CACHELINE_SZ]); } aclrtFreeHost(OutHost); aclrtFree(OutputValue); aclrtDestroyStream(stream); aclrtResetDevice(0); aclFinalize(); return 0; } ``` 如果你对上面直接使用 `bisheng O2 cce soc version AscendXXXYY cce soc core type VecCore I$RT_INC L$RT_LIB lascendcl lruntime QuickStartDemo.cce o QuickStartDemo` 会告诉你架构不支持Ascend910B 但是，如果使用官方提供的脚本是可以编译通过的，那我们知道官方示例代码中存在异构代码，因此不可能使用gnu标准的gcc编译器，只有可能使用bisheng。 那我们通过编译官方示例，并查看对应的异构代码的编译命令 进入build目录执行 `make VERBOSE 1 grep add_custom.cpp` ![alt text](image 4.png) 找到编译器实际路径，我们再使用该路径编译器编译代码 还是架构不支持 测试以下编译指令 ```makefile cc1: /usr/local/Ascend/ascend toolkit/latest/compiler/ccec_compiler/bin/bisheng cc2: /usr/local/Ascend/ascend toolkit/latest/tools/ccec_compiler/bin/bisheng all: \t$(cc2) \\ \t I/usr/local/Ascend/ascend toolkit/latest/runtime/include \\ \t L/usr/local/Ascend/ascend toolkit/latest/runtime/lib64 g \\ \t O2 \\ \t cce soc version Ascend310P1 cce soc core type AICore \\ \t o QuickStartDemo.o \\ \tQuickStartDemo.cce ``` 使用310P1架构与AICore类型交叉编译，出现链接错误 ![alt text](image 5.png) 推测当前的编译器仅支持编译算子，并不支持编译为可执行文件（欸，懒得骂了） ## kernel算子开发(矩阵乘) ### 编译 clone算子example，然后执行sudo bash run.sh r npu v Ascend910B ### 编程范式 对于矩阵乘，其运算api为[Mmad](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha001/devguide/opdevg/ascendcopdevg/atlas_ascendc_10_0016.html?sub_id %2Fzh%2FCANNCommunityEdition%2F800alpha001%2Fapiref%2Fascendcopapi%2Fatlasascendc_api_07_0239.html#ZH CN_TOPIC_0000002082776405__section8213173433312)，但是要执行该操作，需要内存中的数据满足zz的格式。 ![范式](image 6.png) 上面是cube的运算流程，使用copyin，compute，copyout的运算流程，核函数中也遵循该流程。 更详细的流程是，先将输入矩阵传入A1，B1，如下： ![alt text](image 7.png) 接着将矩阵分块后存入A2，B2，如下: ![alt text](image 8.png) 然后进行矩阵运算，[Mmad](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha001/devguide/opdevg/ascendcopdevg/atlas_ascendc_10_0016.html?sub_id %2Fzh%2FCANNCommunityEdition%2F800alpha001%2Fapiref%2Fascendcopapi%2Fatlasascendc_api_07_0239.html#ZH CN_TOPIC_0000002082776405__section8213173433312)，再将结果输出到CO1，将分块后的结果融合成最终结果，然后存入CO1。 A2与B2内数据格式如下： ![alt text](image 12.png) 需要注意的是以上流程中的数据搬运都是通过队列排队实现，这样才能够实现流水线的效果。 ![alt text](image 13.png) 按照上图中的代码，即可实现流水，从逻辑上看，datacopy的工作必然由一个独立单元mte负责，mte与compute单元并行执行才能达到流水的目的。 假证也可推理，即，如果上述代码是串行执行，那么没必要使用queue，直接使用转置后的数据即可。 下面两图协同证明,datacopy任务指令序列在搬运单元中执行，搬运完后执行了setflag通知计算单元： ![alt text](image 15.png) ![alt text](image 14.png) ### 调用 + 创建上下文 ![alt text](image 9.png) + 分配设备空间 ![alt text](image 10.png) + 调用内核函数 ![alt text](image 11.png)"},"/docs/references/llm/SLO_PD_multiplex/index.html":{"title":"Optimizing SLO-oriented LLM Serving with PD-Multiplexing","content":"# Optimizing SLO oriented LLM Serving with PD Multiplexing ## limitation ### pd分配 in place compute 和out place compute方式，如图： ![alt text](image.png) in place计算通过将prefill与decode分离达到减低TBT的作用，out place计算则是在实例内进行以上工作，能达到低内存开销，但是TBT无法保证 > 论文说将两者结合会带来较大的KV cache传输开销,out place计算不也是跨实例传输嘛？ ### prefill分块控制 为了满足TBT的需求，以前的做法将prefill分为了多个chunk，但是chunk大小不好控制，如果太大了TBT受到影响，如果太小了GPU利用率降低 且由于多个请求以及多个阶段间的kv cache共享，分块会造成对kv cache那一块内存的重复访问 ## solution"},"/docs/references/moe/deepseekMoe/index.html":{"title":"deepseek Moe","content":"# deepseek Moe [论文原文链接](https://arxiv.org/pdf/2401.06066) ## 传统moe存在的问题 + 知识混合 就是虽然分了不同的expert，但是输入时，有些专家分配了多个token，蕴含不同的知识 + 知识冗余 某些输入tokens包含的相同（通用）的知识，将会被每个专家学习到，那么专家的参数就是冗余的。 ## 解决 ### 细粒度化expert + 增加expert的数量 + 降低expert ffn的参数量 相应的，将激活的expert的数量增大到m倍，以维持计算成本近似 moe部分计算公式如下： ![alt text](image.png) $g_{i,t}$代表选择expert的权重 ### 设置共享专家处理共享知识 新增了一定的shared expert，他们用来处理共享知识，结构如下图： ![alt text](image 1.png) 计算公式如下： ![alt text](image 2.png) 这些expert是固定激活的，不参与路由的选择，因此相应的，为了维持计算成本，将会将其余的expert的激活数减少，有多少个shared expert，就会减少相应数量的常规expert。 ## 负载均衡问题 这个在[moe problems](../../algorithms/moe/readme.html#problems)提到了，也就是路由网络过分选择某几个专家，造成路由崩溃，同时如果专家分布在多个设备上，会因为负载不均衡产生计算瓶颈。 解决方法： 专家级平衡损失，用来解决某些expert分配的任务量过多，而有些闲置的问题导致的负载不均衡 ![alt text](image 3.png) 没看懂，信息太少了 还引入了一个设备级平衡损失，用来解决设备负载不均衡，貌似是分组计算的： ![alt text](image 4.png) ### 有个视频讲了相关的损失 [视频链接](https://www.bilibili.com/video/BV15XFQebEBM/?spm_id_from 333.337.search card.all.click&vd_source e32e76663edb97323b10f324b6d846ec) 先看switch transformer中的做法，总的说，就是经过softmax每个expert会有个选择概率，然后将每个expert接收的token数占总token数的占比作为实际值， 将softmax输出值平均作为理论值，取差值为loss，用loss更新参数。如下： ![alt text](image 5.png) 然后发现，deepseek中提到的参数是在switch tranformer中的，所以deepseek直接略过了。。 其中的f就是实际值，P就是理论值。 视频提到了这个loss func的问题，就是target是minimize这个loss，但是这个loss func没有能力去最小化loss，计算公式的问题。也就是最终其实没有达到load balance的目的。 而deepseek中采用loss free，在router的softmax中增加了一个bias用来修正，也就是如果某个专家过多被选择，这个bias会被减小从而减小该专家被选择的可能性（如果训练数据中该专家确实需要被多次选择，该做法是否会错过最佳路径？如果采用随机偏置是否有可能得到更好的效果）。类似于正则惩罚？和RL中的随机偏置也有点像。"},"/docs/tutorial/somethingInteresting/元素觉醒/法师之路.html":{"title":"成为大魔法师！","content":"# 成为大魔法师！ ## 基础知识（必读） 进游戏后按E键打开该界面 ![alt text](image.png) 工作台： 根据上面流程放置工作台后，通过以下流程生成物品 ![alt text](image 1.png) ![alt text](image 2.png) tip1：`可以通过滑轮快速切换物品` minecraft中所有的物品可以简要分为几类：建筑方块、功能方块，材料、装饰、植物、工具 + 建筑方块：可以用于建造，仅有堆叠功能，提供建筑的基础材料，通过不同建筑方块搭建美观的建筑。可以通过材料进行合成，或者直接挖掘获得 + 功能方块：这些方块可以用来放置，当右键点击他们时，会触发不同的功能，例如箱子可以放置物品，床可以睡觉，工作台合成物品等。 + 材料：这类物品不可放置，但可以通过工作台合成不同的物品 + 装饰：这类物品可以放置，但唯一的作用仅用来装饰 + 植物：植物分为本体和种子，基本遵循自然规律，例如土豆、小麦的种子是他本身，而南瓜等作物的种子需要特殊方式获取。 + 工具：通过材料合成，具有攻击，功能性等作用，例如剑左键可以进行攻击，由木头剑，石剑，铁剑，钻石剑品阶依次增高。而其他工具例如锄头，右键点击草方块可以进行锄地以进行种植。挖掘石头等使用镐子效率更高，而木头等使用斧头效率更高。 元素觉醒整合包新增了部分其他物品，这里简要介绍重要的部分： + 法术类：包括法杖、法术书等，对应的法术物品可以增强相应的属性。 + 装备类：整合包提供了种类繁多的装备，同样可以增加对应的属性。 + 召唤，传送类：例如召唤珍珠可以召唤一个传送门，将会传送怪物到周围；地图方块，通过领域地图右键点击可以传送到对应的副本地图。 tip2：`完成第一章的伐木任务后会获得一个大背包，按B键可以打开背包空间，需要注意该空间无法外部直接访问，例如货币放置在该空间时，无法在商店购买物品，需要放置到外部背包` ![alt text](image 10.png) ## 关于法术 ### 获取一个法术 首先右键点击`法术绑定台`，将一本书放置到左边，右边选取对应职业的法术书进行生成获取法术书。 ![alt text](image 3.png) 再将法术书放到法术绑定台，再放入青金石绑定对应的法术。 ![alt text](image 4.png) 此时鼠标放在法术书上，按ALT键将会看到法术书绑定的法术 ![alt text](image 5.png) 打开个人界面，点击如下按钮打开装备界面， ![alt text](image 6.png) 将法术书放置到槽位 ![alt text](image 7.png) 切换到武器，此时可以施放已绑定的法术，通过法术书绑定的法术只要是攻击性武器都可施放，但是法杖类武器具有相应的加成。 ![alt text](image 8.png) 想要施放法术还需要对应的符文，符文制作可以使用工作台或者符文制作台，后者能产出更多的符文。将符文放置到背包格子即可。 ![alt text](image 9.png) ### 其他施放方式 + 切换到卷轴施放，例如火球术卷轴施放 + 法杖自带法术施放，例如闪电法杖自带`魂灵雷击`法术。 ### 主线 打开任务界面，鼠标移到左侧三角形，完成对应的主线任务即可推进，同时获取整合包绝大部分的知识。 ![alt text](image 11.png)"},"/docs/tutorial/writing/index.html":{"title":"论文写作注意","content":"# 论文写作注意 ## 重视写作 给别人讲明白自己的idea ## 英文写作 英文论文阅读 减少辅助工具的使用 领域内专有名字，深度绑定第一个问题。 ## 观点 和图配合，论述清楚自己的观点。 缩写等先定义再使用，不要默认审稿人知道相关领域知识。 ## 用词准确 用词不标准，cache ratio，cache rate 用词不形象，动词的选择等。 ## 画图不严谨 缺少信息 图中重点表达模糊 ## 详略不当 没有一个凝练的写作思路，没有大纲。论文中出现的每一段最好都有意义。 强调为什么要这么做，比啰啰嗦嗦讲怎么做更重要。 ## 矢量图 放大不失真 字体统一，格式相关问题。 文章时态位一般现在时 ![alt text](image.png)"},"/docs/references/ann/second-Tier-memory/second-Tier-memory.html":{"title":"Characterizing the Dilemma of Performance and Index Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory","content":"# Characterizing the Dilemma of Performance and Index Size in Billion Scale Vector Search and Breaking It with Second Tier Memory 本文较特殊，前半部分花费了大量篇幅论证了在当前的存储下，图索引以及聚类索引所面临的困境，这一大部分内容对阅读造成了困难，因此后续分析将进行简略处理。 提到的二层存储是使用RDMA和CXL连接的远程DRAM/NVM（非易失性存储）。 ## main problem > 对于一个64核的商品化的服务器，使用SIMD在100 384维的公共向量数据集上搜索，效率可以达到3624 5212 Mvectors/sec。 > > 但是对于5.3GB/s带宽的固态硬盘，即使带宽利用率拉满，也仅仅能支持14 53Mvectors/s的向量查询效率，这个差距也就是存储墙的问题。 因此这里武断地说，如果不改变查询向量的系统结构，类似于fusionANNS那样，那唯一的解决方案就是降低i/o请求频率，如何解决呢？cache！乐，实际上就是类似于内存到三级缓存那样处理，如果一个数据之前被取过，那我不必再去发起新的i/o请求，这一点在fusionANNS中有提到过类似的解决方案。 论文又提到在fusionANNS上提到的问题，SSD访存颗粒度不匹配问题。 > 我们认为，第二层存储器 易失性或非易失性（NVM）存储器通过快速互连（如RDMA和CXL （§4））连接到主机，为系统地解决这个问题提供了机会。具体而言，这些设备的行为类似于存储，但支持更精细的访问粒度（256 B vs. 4 KB），这与向量索引的工作负载模式相匹配。此外，它们对于具有甚至更小访问粒度的随机读取（例如，100 B） 其实就是如果直接访问SSD，那单次请求的颗粒度大小为4KB（或许与文件系统的BLOCK相关），但是使用RDMA或者CXL这种类总线的间接访问方式可以降低其粒度以减少i/o请求的开销（<font color red>存疑，虽然粒度减小，但是请求次数没变，二层内存是否能够带来更高的访存效率？</font>）。 主要问题： > 我们将这种困境归因于高性能SSD访问的I/O需求与具有小索引放大的矢量索引发出的I/O工作负载之间的不匹配。 > 图索引需要细粒度的存储读取来实现实际的索引大小。对于具有最小索引大小的实用图索引，我们必须用每个节点的少量边来构造它。从算法的角度来看，这意味着图遍历必须使用具有小有效负载的随机I/ o来读取这些边。但是，这与使用足够大的I/O有效负载(4 KB)以有效利用SSD等传统存储设备的要求存在根本冲突。 文章用了大量篇幅论证上面的观点，论证过程不再赘述。 ## solution 再看看图索引的实现： <div style \"text align:center;\"><img src \"./QQ20241205 152623.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 其中节点是向量，边连接距离近的向量。例如，如果a → B，这意味着向量B是a的前k向量。提到的辅助搜索的压缩图是什么（<font color red>?</font>，查：质心代表簇中的向量数据，每个节点的邻居数有参数配置的k相关，不是查询使用的那个top k。 下面是在图上寻找前k个最近向量的流程，思路很简单，不再赘述。 <div style \"text align:center;\"><img src \"./QQ20241205 152916.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 图索引的缺点： >由于其指针追逐访问模式，导致长延迟和低带宽利用率。此外，图索引对向量插入不友好，因为构建图需要重建图。 聚类索引（也可以叫簇索引，只是名字不同，其本质是一种量化方法）： 基于簇索引的方法中需要注意的是不平衡划分问题以边界问题，边界问题在fusionANNS中提到，在此处更加明了，解决方案是将边界向量复制到一组关闭的集群，其中复制的数量在索引构建之前静态配置。下图可以说明该问题： <div style \"text align:center;\"><img src \"./QQ20241205 163411.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 如果top c质心仅包含cluster0，那cluster1中的邻近向量就被丢掉了，导致准确率降低，解决方法就是将cluster1中的边界向量复制到cluster0，缺点显而易见，存储代价增加。另一种解决方案就是将两个cluster都读一遍，也可以获取到所有的邻近向量，但是缺点也很明显，搜索代价增加。 看看一层存储与二层存储的结构图： <div style \"text align:center;\"><img src \"./QQ20241205 160018.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 优点是（1）老一代内存更便宜，（2）间接允许通过池化未使用的内存来提高内存利用率。 然而，它们仍然比DRAM慢得多，并且表现得更像存储，也就是说单次i/o请求的延迟比直接dram要慢得多，特别是RDMA（究其原因是经过的流程太多了，cpu向DMA发送请求，DMA接受请求处理完transfer后触发中断，等待中断执行），那解决方案显然是将单次i/o请求内容增大（术语叫请求载荷）。 常用的十亿规模的数据集（仅做个记录） <div style \"text align:center;\"><img src \"./QQ20241205 160649.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> **文章的主要方法就是借助二层内存（二级内存？），在该内存上构建图索引和聚类索引，借助二层内存更细粒度的访存，我们可以加速向量查询，以上说法不是很精准，因为借助RDMA以及CXL等中间传输介质，不能将二层内存直接看做内存** ### 基于second Tier Memory改进的图索引 + 软件流水线 很简单，就是使用异步i/o，（虽然不算惊艳，但是是非常稳健的方法） > 我们提出了一种软件管道机制，用计算来异步处理I/O，从而最大限度地利用计算能力。 RDMA中使用协程（就是一个用户级的多线程，可以理解为在多个函数之间来回调度，但是不通过os kernel，这么说可能有点抽象，你可简单理解成用户自己实现的多线程），NVM和CXL中使用预加载（mm_prefecth），能理解的，类似于操作总线，我们通过store/load指令访存，但是该操作无法实现异步，由于是cpu指令，需要强制同步等待，所以论文也提出了重新组织指令顺序（术语：乱序执行）。 + 压缩图布局 呃呃，因为是图，邻接稀疏矩阵，所以转为csr存储，就酱。。 ### 基于second Tier Memory改进的聚类索引 <div style \"text align:center;\"><img src \"./QQ20241206 100136.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 解耦索引布局 如上图，a图为原始的布局，对于边界向量，是直接将向量复制到各个簇当中，显然浪费空间。b为解耦布局，将向量地址和向量数据分开存储，这样边界向量的占用就被减小了。（注意，此处不要将二层内存看作内存，实际存储设备仍然是SSD，二层内存是一个抽象，借助它的细粒度以及异步i/o我们可以做到加速）。 那么新问题是，如何在新布局上执行搜索，先查询向量地址，再用向量地址读取数据（小负载），（这里有一个大前提，还记得前面各设备延迟对比么，那就是SSD的访问延迟是75微秒，而二级内存的延迟低得多，平均在个位数以下）。 + 集群感知分组 基于以上方式产生的新的问题是，该方式将原本的单个数据访问解耦后，产生了起码两倍的i/o数量，怎么解决？ 又是非常精彩的cache优化（万物皆可cache，悲）。 仍然见上图， > 通过将属于同一集群的向量分组在一起，并将它们存储在相邻的存储中，我们可以使用一个大I/O来读取所有组中的向量（地址） 注意到，虽然我们可以一次读取一个聚类所有的数据地址，但是别忘了还要读取数据，如果这个聚类中存在边界向量是由其他聚类复制过来的，那么我们就需要一个单独的i/o请求来访问该向量（读到这里，你肯定疑惑，都有地址了，为啥要单独访问该向量？别忘了，我们用的二层内存是RDMA这一类“介质”，每次的请求是一个连续的地址块，精彩） 所以引出了后续的工作，怎么分组？假设有一个向量数据库，有一组向量V，和一组聚类C，使用$P_{i,j}$表示向量i是否被分配到一个组（应该是不与同聚类其他向量不在一个组的意思，否则后边儿的公式说不通），$h_{j}$表示聚类的接入频率（访问频率，后台监测得到），所以问题用公式表述如下： $$minimize \\sum_{j}^{C}h_{j} \\cdot (1 + \\sum_{i}^{V}P_{i,j})$$ 以上问题使用整数线性规划（ILP）解决，（简单认为是动态规划，毕竟背包问题就是用ILP解决），论文没讲具体实现，针对不同场景，该算法可以千差万别。 聚类约束： <div style \"text align:center;\"><img src \"./QQ20241206 105125.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ok，$A_{i,j}$哪来的？似乎论文里面没有提到过。 组约束：就是每一个向量必须分配到最少一个组。 ok，看后文，似乎他们并没有给出实际的ILP解决方法，而是用贪心策略 > 观察问题的简单结构，我们可以使用简单的贪心算法来寻找最优解。具体来说，对于一个已经被复制到多个聚类的向量，将其分配给访问频率最高的聚类是最优选择。这是因为，非正式地说，将其分配给访问频率较低的集群会增加I/ o的数量。 ## interesting phenomenon 在常规的开发中，图索引有更小的存储占用，不需要太多边就能达到高精度，但是高频率的i/o访问带来性能瓶颈。 而聚类索引的访问模式更适合SSD的粗粒度访问，性能更高，但精度不足。 但在本文的实验中，恰好反过来了，图索引有更好的i/o效率。聚类索引有更小的索引占用。（？前面的好理解，但是聚类索引更小占用是因为将数据转为地址了？）"},"/docs/references/ascend/ascend_llm/index.html":{"title":"","content":""},"/docs/references/llm/APC/index.html":{"title":"自动前缀缓存","content":"# 自动前缀缓存 跨请求缓存KV矩阵 ## vLLM 对于每一个token采用hash编码，同时通过前缀构建前缀树，采用LRU逐出策略。"},"/docs/references/moe/parallel_method/index.html":{"title":"moe parallelism","content":"# moe parallelism ## GShard [论文链接](https://arxiv.org/pdf/2006.16668) 论文中提到超过千台设备上的A naive graph representation将会成为计算的瓶颈。 ![alt text](image.png) 对模型不同层进行分区，通过设备间通讯的方式同步参数将会涉及到底层通信机制的改变，对于开发者是一个严重的负担。 GShard是一个XLA的编译器扩展，简单来说就是按照其api规范添加注释，这个编译器会自动将模型进行分区，然后并行化。 ![alt text](image 1.png) ### 多FFN跨设备 ![alt text](image 2.png) 如图，feed forward被替换为多个expert，当采用多设备计算时，除开ffn部分，其余部分都是被复制到多台设备上。 3.2节再次提到，将注意力层复制到多个设备上，多个专家进行分割，分割到多台设备，因此，对于一台设备上的数据流，则类似原版的transformer。 每个router副本与每个expert进行all to all通信，容易理解。（糖丸了，all to all通信，这通信开销得多大啊） ### 自动分片 因为是利用了2048个TPU，那么如何利用tensor计算能力就很重要了，所以论文里面将tensor运算进行抽象，让用户将整个计算集群看作单个设备，只用加注释进行数据分片，至于哪些分片到哪个计算unit是系统负责的。我们主要探索分布式下的性能，所以不做过多探索。 ## switch transformer [论文链接](https://arxiv.org/pdf/2101.03961) 传统的moe将一个token路由到了多个专家，并将多个专家的输出乘以偏好进行输出，（也就是偏好决定了专家输出的重要性）。 ![alt text](image 3.png) 本文rethinking这个操作，使k 1，（乐） ### 辅助损失 ![alt text](image 4.png) 以上用于路由平衡，确保专家分配token的平衡，保证token dropped率低的情况下，减少expert容量。 ### 不同并行方式对比 ![alt text](image 5.png) 上面一排是权重参数在不同core上的分布，下面一排是数据在不同核上的分布。 直接看最后一个红框，按理说是最复杂的，对于上面的4×4方格，也就是16个core，不同颜色代表不同的权重参数，这里比较难理解，我看网上有的作者认为，对于第一个蓝色的块，里面是有四个expert的，他们作为一个整体在被处理，但是这样逻辑说不通。我更倾向于认为，一个块代表的一个expert，对于下面的数据分配，蓝色的块都是同一批数据，也就是比如[x1,x2,x3,x4]这么一组数据被复制到四个核上。 那么单个核的计算流程就变成了，一个批次的数据被input时，该核拥有一个专家的一部分参数，对应参数与数据中对应的部分计算后后，四个核的数据进行reduce，那么通信就被局限在了四个核中，从而减少了一定的通信量。 ## 其他 显然，直接对transformer进行分层然后分配设备进行pipeline计算一定有相关课题，不做过多赘述。 ## 梳理 那么从头看moe在多设备上的计算，从inference到back propagation。以每个设备分配一个专家为例。 + 1，tokens被输入到模型，并进入device 0 + 2，经过注意力模块计算，sync with other devices + 3，token经过router，得到输入每个expert的偏好值 + 4，根据偏好值得到top k个需要被激活的expert。 + 5，将token投入这k个expert，注意到expert在其他设备上，因此这里面临第一重通信，tokens的传输。 + 6，expert计算得到output后，所有的expert的output进行规约，那么这里面临第二重通信，output的传输。（还有个输入的残差结构的通信在上一重已经进行了） + 7，得到总的output后通过label得到loss，loss反向更新expert参数，expert反向的梯度将会更新router的参数，注意：router都是复制的副本，因此这里涉及第三重通信，gradient的传输/新parameters的传输 + 8，梯度传到注意力模块，再次进行参数更新，产生gradient的传输/新parameters的传输 从以上过程看，一旦设备数过多，那么就会产生大量的设备间通信。 需要注意的是，以上只是一个基础的过程，实际执行中不同设备同一时刻的输入tokens可能是不同batch的数据，此时多设备间进行梯度合并更新参数。不同的策略将会极大的影响moe的效率。 那么问题转移到了，怎么平衡计算与通信，如果看计算的极端，就是让每个设备都维护一份gradient，从而单独计算从而更新参数；那么通信的极端就是只让一个主机进行计算，其余从机继续向下一个状态转移，主机计算完成后，通知从机更新后的参数。 ## dualPipe 直接讲主要方法， 关于双路pipeline，如下： ![alt text](image 8.png) 上面是常规的流水（8卡），将整个网络中的layer平等的分为8份，每一份放在一张卡上，例如GPU0持有layer0 9，GPU1持有layer10 19，以此类推。 那么对于上面一部分，训练流程如下： + 1，batch0输入GPU0，layer进行inference，将output传输到GPU1，以此类推，直到layer7得到输出。 + 2，计算loss + 3，从layer7开始反向传播，将gradients传输到GPU6，以此类推，传输的同时进行参数更新。 + 4，循环以上步骤 通过调整forward与backward的顺序，能够减少流水线中的气泡，例如1F1B，ZB PP等。 因此引出下面一部分，我们需要结合论文中提到的Overlapping strategy来看： ![alt text](image 6.png) 上面的图是dualpipe中的pp方式，可以注意到的是，GPU0，GPU7同时持有layer0，那么我们在一开始可以同时输入两个batch分别到GPU0与GPU1 需要注意的是，这种操作单独来看并没有什么太大的作用，因为每个GPU同一时刻的计算能力仅能够负载一个layer的forward/backward。好就好在dualpipe中设计了下面流水块。 我们需要知道在单个GPU内，通信与计算是独立的，意味着这两者也可以形成流水，所以dualpipe设计了下面的流水方式。 ![alt text](image 7.png) 上面是一个紧密的流水块，怎么来的？看下面的流水线，可以注意到，两头batch输入，同时采用FB交替的方式进行流水，在流水线的中间部分就会“产生”这样的块。 （节奏大师） ![alt text](image 9.png) ## 整体分布 那么综合以上的并行结构，粗略的总结一个整体的分布式架构（或者pp在节点间进行？）： ![alt text](image 10.png) expert位于多个节点上，由于每一个batch的数据不同，router产生的结果也不同，导致每一次激活的expert也不同，因此，不可避免的，采用all 2 all通信是最佳选择。 > 一个问题是，为什么不在节点间流水？因为每个expert并不是每一次都会被激活，直觉上router产生的节点间的通信带宽需求远远小于pp中的数据传递。 > 同时也可以注意到，之前的主要瓶颈在于节点间的通信，因此负载均衡loss func的提出才能有效的提升整体的效率。 那么所谓的拓扑架构也就不再存在，因为不存在一个数据跨越多个节点传输的问题。"},"/docs/references/index.html":{"title":"reference read note","content":"# reference read note reference read note > **click the sidebar to open markdown file** >! [read from the first file](./ann/faastube/faastube.html)"},"/docs/tutorial/ascend/benchTest/index.html":{"title":"ascend bench test","content":"# ascend bench test ## work target 围绕mindspore graphlearning 使用官方提供的profile工具，做一些测试 1.运行GCN算法，使用reddit数据集（也可以考虑多个数据集，或者变换隐藏层维度），对比昇腾与GPU的性能差异（epoch运行时间）。 mindspore 即能跑GPU也能跑昇腾 2.profile一下mindspore graphlearning 在运行GCN算法时，不同算子在不同计算单元（CUBE/VECTOR/Scalar/AICPU）上的执行时间占比，profile工具好像还提供可视化算子流水的功能，能可视化一段时间内，是哪个算子在运行 ## 安装Graph leaning >! 一定要先进行这一步，如果你先安装mindspore，安装gl会进行环境检查，然后告诉你平台不对。（逆天bug） 记得先使用镜像安装requirements.txt中的依赖，不然后面自动下载依赖很慢 [官网教程](https://www.mindspore.cn/graphlearning/docs/zh CN/r0.2/mindspore_graphlearning_install.html) cuda gpu版本安装，需要注意环境问题,python只能是3.7 ![alt text](image 6.png) >! 血泪教训，记得添加CUDA_HOME环境变量，mindspore是根据cuda_home的变量来寻找cudatoolkit的位置的，官网没说，乐 ## 安装mindspore [官网教程](https://www.mindspore.cn/install/) ## gcn 不讨论其公式原理，和拉普拉斯矩阵相关，傅里叶那一套的数学转换方法，挺杂乱的，这里直接给出算法原理 ![alt text](image 1.png) 如上图，一个layer中的聚合方式，邻接矩阵矩阵乘节点特征矩阵达到聚合的目的。 对计算结果归一化，使用度矩阵（degree）乘上上面的计算结果。 但是如果仔细观察就会发现，该归一化的作用范围仅有行，例如图中的1/3，被1/3乘的只有横着的一行，也就是单向边，想要对列也起作用，那么在另一边也乘一个度矩阵逆即可。 但是乘了两次，所以使用$D^{ 1/2}$来减轻归一化的程度（不严谨说法） 上面的过程用公式表述为 $$D^{ 1/2}AD^{ 1/2}X$$ 在神经网络中还会乘上一个权重矩阵W,再激活一下，加个偏置，标准做法了 $$ReLU(D^{ 1/2}AD^{ 1/2}XW+B)$$ 以上就是一层的计算过程了，实际上一直到计算归一化的过程都是可以进行简化的。也就是进行融合。 ## 数据集 ### reddit [下载地址](https://data.dgl.ai/dataset/reddit.zip) 读取数据集，该数据集不像corav2几乎给了所有的列名，所以需要转换一下。 很坑的一点是，官网说Reddit类返回的adj_coo是ndarray类型，结果一打印，发现是scipy的coo_matrix格式，他们官网的文档完全是在误导人。 ![alt text](image 2.png) 数据读取转换代码如下： ```python class GraphReddit: \"\"\"Full training numpy dataset \"\"\" def __init__(self, data_path: str) > None: graph Reddit(data_path) self.x ms.Tensor(graph.node_feat,dtype ms.float16) self.y ms.Tensor(graph.node_label, ms.int32) self.train_mask graph.train_mask self.test_mask graph.test_mask self.n_nodes graph.node_count self.n_edges graph.edge_count self.g GraphField(ms.Tensor(graph.adj_coo.row, dtype ms.int32), ms.Tensor(graph.adj_coo.col, dtype ms.int32), int(self.n_nodes), int(self.n_edges)) self.n_classes int(graph.num_classes) in_deg np.zeros(shape self.n_nodes, dtype np.int64) out_deg np.zeros(shape self.n_nodes, dtype np.int64) for r in graph.adj_coo.row: out_deg[r] + 1 for c in graph.adj_coo.col: in_deg[c] + 1 self.in_deg ms.Tensor(in_deg, ms.int32) self.out_deg ms.Tensor(out_deg, ms.int32) print(\"data prepared\") ``` ok,那我开骂了，那两个for语句给我整笑了，如果出入度形状对不上，后面就会报错（alloc error gather）的问题，报错牛头不对马嘴。 最离谱的是，python的for慢地吃屎，numpy的优势属于是荡然无存了，样例里面写出两个for我能感觉到写这段代码的人的无语了。 ## 模型 还得吐槽 为什么一定需要套一个Datanet的inference层？？不是很明白这么做的意义是什么 ![alt text](image 3.png) ## 性能监测 1，使用profiler的api 在训练代码前添加 `profiler Profiler(profiler_level ProfilerLevel.Level0,output_path \"./profiler\",profile_memory True,hbm_ddr True)` 训练结束后添加 `profiler.analyse()` but我的没有生效，生成的数据无法通过mindinsight查看 2，使用环境变量 在bashrc中添加 ![alt text](image 4.png) 再次运行训练代码, 通过`mindinsight start port 8003 summary base dir root_dir` 即可查看数据可视化如下： ![alt text](image 5.png) ## 性能分析 由于mindspore gpu下跑gcn整图训练reddit数据集会爆（gpu4090，24GB显存），所以我们使用cora_v2进行对比 Ascend ![alt text](image 7.png) CPU ![alt text](image 8.png) 基本都是计算各算子的时间占比，一方面不熟悉这些算子功能，另一方面做加速对功能不敏感，但是对实现敏感，所以这里暂时转向[算子开发验证](../mindspore_develop/index.html),还有一部分原因是，其文档过于杂乱，有部分算子甚至是在mindspore版本1.3中才有，有一些又是使用的tf的接口。"},"/docs/references/algorithms/transformer/index.html":{"title":"transformer","content":"# transformer ## 整体结构 整体 ![alt text](image 1.png) 细节 ![alt text](image 2.png) ## 流程 简述一下 + 1，输入一句话，如“how are you” + 2，将这句话进行词向量嵌入，每个单词转换成一个m为的向量，这句话中有3个词，所以最终输入的矩阵维度就是3*m。 + 3，将词向量与位置编码直接相加，然后输入encoder，一层层encoder向下传 + 4，将最后一层encoder的结果输入给每一层的decoder，再一层层计算decoder的值，并向下传递，最终输出一个词向量矩阵，然后查词典得到对应的自然语言 + 5，需要注意的，对于这个结构而言，我们是一次性将所有的单词进行输入，而非像传统时序网络那般，每一个编码/解码器负责一个向量输入，这样对于整个网络更能注意到整体的信息，而无需像lstm那般设置很多的门来避免信息丢失，这也更符合人理解自然语言的流程。 ## self attention 只需要知道是计算qkv相关矩阵，基本是矩阵运算，不用深入原理，说不清的。 通过多头注意力计算得到输出，然后输出进行拼接，然后输入linear，如下： ![alt text](image 3.png) ## 其他 ADD && NORM就是残差连接再归一化，都好理解，前者避免linear信息丢失，后者避免输出爆炸 需要注意的是decoder中的第一个多头用的masked： ![alt text](image 4.png) 第二个多头中的KV用的是encoder的输出矩阵计算得到的。 Feed forward层就是一个非线性层，现在这些概念都快绕昏了，mlp，linear，dense，ffn一大堆。"},"/docs/references/ann/a_realtime_adaptive_multi_stream_GPU_system/adaptive_multi_stream.html":{"title":"A Real-Time Adaptive Multi-Stream GPU System for Online Approximate Nearest Neighborhood Search","content":"# A Real Time Adaptive Multi Stream GPU System for Online Approximate Nearest Neighborhood Search > 解决在线场景中需要实时插入的问题。 ## main problem + 1，在线场景实时插入 + 现有系统对ANNS采用单流串行执行模式：如下图上部分： <div style \"text align:center;\"><img src \"QQ20241207 102525.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 当进行实时向量扩展时，批处理需要进行阻塞，如果复制的数据量大，等待期延长，导致搜索过程被阻碍。 + 同时，触发向量扩展时，需要GPU启动kernel来进行内存分配和复制，这个过程是耗时的。 ## solution ### CPU/GPU ANNS中的在线向量插入方法 <div style \"text align:center;\"><img src \"QQ20241207 105307.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 首先改变了数据结构改为链表（如上图），现有系统将所有的向量连续的排列在内存当中。header info的内容报头包含指示prev_header、next_header和block_info的地址的重要信息（向量容量、向量大小等）。ml中记录的是这个簇中属于这个block的vectorid。mv被以32维交错的放在内存块中（？）如下图，D是单个向量的维度。 <div style \"text align:center;\"><img src \"QQ20241207 154410.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 设计新的内存分配方式，就是加了一个内存池管理，内存池的大小是整个主存的大小。然后将内存按块分为最小单位， <div style \"text align:center;\"><img src \"QQ20241207 160804.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 直接看看它的算法吧，老实说，看不太懂，就对于上面的伪码而言，k是从哪里冒出来的？文中似乎并没有提到这一点，我们就假设代指全部。同时忽略需要同步的代码，仅看代码逻辑。 那么当需要插入一个新向量时： + 1，计算向量列表的末尾索引did + 2，通过did计算应该放在哪个block，blockid为mid，还需要该向量在block内的偏移moff才能准确定位（吐槽一下，$T_{m}$又是哪来的？） + 3，如果mid大于已分配的block list，那就分配新的block，注意到嵌套if，第一个if查看是否需要插入新的block，第二个if判断是否是第一个元素，才选择从内存池中新分配block，这里需要强调的是，为什么这么做呢，别忘记了整个函数是一个gpu核函数，此时其他线程也在操作。原子化的判断，不会使其余线程也同时加入新block造成冲突（精彩） + 4，后续就是更新$m_{mid}$，然后将新的vector写入block。 + 5，如果新加入的block list的长度超过预定义的$T^{'}_{m}$,那么执行重排操作。 重排操作（严重吐槽，这还是个递归函数，关于函数的解释就两句话？？，图也没有一个，不确定正式发表是否有改动，我要是reviewer包给他挂了）： <div style \"text align:center;\"><img src \"QQ20241207 163515.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 言归正传，其实主要思想就是，我们不是新加入了一个block，上图中是$m_{j}$,我们这个新block可能和旧列表中的向量是连续排列的，因此需要将$m_{j}$放到$m_{i}$后面，那么就交换$m_{j}$和$m_{i+1}$，交换需要中间空闲临时块。 那么产生的新问题是，交换过去的$m_{i+1}$也可能和旧列表中的块邻近，因此采用递归操作知道收敛。这里的递归终止实在是看不懂了，merge是在干嘛是真不知道。 后面才注意到还有个图c，甚至论文里面没有引用过，如下： <div style \"text align:center;\"><img src \"QQ20241207 184738.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ### 流缓存多流执行 <div style \"text align:center;\"><img src \"QQ20241207 184852.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 如上图，将多个执行核放到多个流上。 这样做有两个问题要考虑 > 1）系统必须避免联机内存分配和解除分配，以防止全局阻塞和降级为串行化机制。2)系统中的每个内核都不应消耗大量资源，因为过多的资源消耗会导致阻塞。 总结就是由于存在共享资源，所以会有锁的问题，一旦多个流同时访问同一个资源，那就会由并行转为串行执行。 对于问题1： > 设计了一个双层的基于流的资源池。最初，每个批量搜索请求被分配给一个专用流，拥有单独的小内存分配（足以使用ivfflat和ivfpq算法）。 之前提到内存池大小是整个内存，这里就是为每一个搜索流单独分配一个足够使用的内存空间。这样就不会和实时插入流产生冲突。 如果不够用了，再从中央内存池分配空间。 ## deployment detail 实际使用的系统，（虽然文章让人很不爽，但是能用起来的都是牛逼方法） 直接看原文描述： > 我们的系统已经在T4/A10 GPU机器上运行了六个多月，无缝集成到一个广泛使用的信息应用程序的搜索和推荐系统中，每天有超过1亿用户。包括两个基本部分，即离线和在线组件，我们的系统有效地管理准备好的数据和实时矢量插入任务。离线段专用于处理准备好的数据，而在线段编排实时向量插入操作 > 在线段由多个CPU线程管理，并与Kafka接口进行消息摄取，它实现了一个动态的队列策略，在将聚合的批次分派给GPU之前，每秒或在达到128次插入的倍数的阈值时聚合向量，上限为1024。在内核执行之前，我们仔细地从资源池中分配了32个独立的资源，每个资源都配备了50 MB的缓存内存，如前所述。在临时内存需求超过此阈值的情况下，额外的内存将从中央池中获得，每个分配设置为200 MB。为了确保公平的资源分配，特别是在高QPS条件下，我们设计了无锁队列机制。当所有32个资源用尽时，请求被拒绝。此外，我们为载体插入任务建立了专用流，以简化处理。 实际执行算法时有一些细节还是需要注意，不算是trick，只能说作者没在method里面提到过，但是很重要。 **高地址分配策略进行搜索，低地址插入。**（看不懂，原文就这一句，乐）"},"/docs/references/ann/faastube/faastube.html":{"title":"FaaSTube: Optimizing GPU-oriented Data Transfer for Serverless Computing","content":"# FaaSTube: Optimizing GPU oriented Data Transfer for Serverless Computing 推理应用程序通常将多个模型和操作缝合到一个工作流中。这个是本文的前提，因为存在这样的工作流，因此在任务进行的过程中存在大量的数据transfer，这会造成大量的时间开销。 <div style \"text align:center;\"><img src \"./workflows.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 无服务器推理工作流涉及各种类型的数据传递。除了典型的cFunc（cpu函数）到cFunc数据传递之外，还有主机到gFunc（gpu函数）（其中主机表示主机内存中的cFunc或I/O数据）和gFunc到gFunc数据传递。不幸的是，当前的无服务器系统依赖于面向主机的数据传递方法 <div style \"text align:center;\"><img src \"./QQ20241202 162700.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 如上图，对于gpu到gpu的数据传输，首先将数据复制到host，然后再复制到gpu，忽略了nvlink的存在；而对于host到gpu，则采用单个PCIe链路，而忽略了gpu间存在的PCIe链路。如下图，将需要传输的data拆分，分别传输（并行）。 <div style \"text align:center;\"><img src \"./QQ20241202 163326.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> faastube作为数据传输的透明管道，用户不必操心传输的问题，就是开了一个抽象层，用户直接用其抽象层提供的api进行数据传输。 总结存在的第一个问题就是，现有的工作都是单点传输，即传输数据的时候使用一条PCIe总线通路，而没有考虑复杂系统中多条PCIe通路的利用，以及提出相应算法来利用gpu间PCIe的拓扑关系最大化传输效率。 第二个问题是，现在长时间运行的如ML任务，现有的内存管理系统会占用大量的内存，且临时分配内存会带来数据传输延迟，因此提出一个内存池方案，用以缓解内存压力。 <div style \"text align:center;\"><img src \"./QQ20241202 165639.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ## 解决问题1 各种拓扑链接如上图，私以为，除开右上角的链接方式，其余部分不太能够利用到gpu间的传输链路。 >! 论文中提到，G1 G4可以通过G4 G6 G7加倍带宽（<font color red>?</font>），这个数据通路不是很符合逻辑（看不懂）。以及后面提到的，G3 G7通过G3 G2 G1 G7以及G3 G4 G6 G7提高带宽，这个可以理解。 FaaSTube提供统一的数据ID，以简化管理主机端存储（例如，共享主机内存，Redis服务器）和GPU端存储（例如，CUDA IPC句柄） 也就是说该系统将数据进行封装，类似于“页表”机制<font color red>?</font>为每个页维护一条信息，可以传递这个页的id以获取地址信息。在文中，也就是一个tube，如下图。与页表机制相同，采用多级页表映射机制，这是由于内存空间很大，如果要保证内存划分的细粒度，那就必须要建立多级映射以维护页表。 <div style \"text align:center;\"><img src \"./QQ20241202 192113.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 论文中还提到，数据的复制采用流水线的方式进行，但我并没有看到任何关于流水线的说明。 关于数据传输的带宽控制，论文中提到将全局带宽抽象成整体来分配，如下图，每一个函数的data transfer会被划分为块来进行传输。 <div style \"text align:center;\"><img src \"./QQ20241202 193111.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> PCIe bandwidth schedular中计算$$Rate_{least} data_{size}/(L_{slo} L_{infer})$$(最小传输需求)$$L_{infer}$$代表推理计算延迟。SLO代表服务等级目标（serverless的一些专业知识，不是很了解），然后注意到还有一个循环固定缓冲区，类似于双缓冲机制，提前分配两块固定大小的缓冲区，两块缓冲同时使用，一块用来缓存需要传输的数据，另一块用来传输数据，也类似流水线的方式，提高传输效率。 FaaSTube利用并行NVLink路径来增强非统一拓扑中的点对点数据传递。FaaSTube引入了一种竞争感知路径选择算法，可优化NVLink使用，同时最大限度地减少来自其他功能的带宽竞争（避免路径重复）。给定无服务器工作流，FaaSTube首先应用MAPA 中的放置策略将函数分配给GPU，并最大化函数之间的NVLink连接。在确定功能布局后，FaaSTube首先在无服务器工作流中包含的GPU之间分配直接连接路径。如果这些直接NVLinks已经被其他函数占用，FaaSTube将强制其他函数释放路径并重新规划其他路径。 <div style \"text align:center;\"><img src \"./QQ20241202 194506.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 上面那个挺难理解的，直接看他抽象到最上面的算法，1 7行就是搜索空闲路径，然后将空闲path放入paths列表。如果找不到空闲路径，且输入输出带宽没有耗尽，那就搜索忙碌的路径，将占用这条忙碌路径的函数和当前需要路径的函数比较，如果不平衡，且占用当前路径的函数能够切换到另一条路径，那就将该函数挪到另一条路径，当前函数占用该path，这个过程看上去似乎比较复杂，因此作者强调了该过程在实验中仅花费10微秒。 ## 解决问题2 1）在每个GPU上提供了一个自动伸缩的内存池，可以根据功能的实际需求弹性伸缩，2）基于请求队列智能地在主机和GPU内存之间迁移数据。 当函数工作负载和中间数据大小动态变化时，这种方法会导致内存占用高达我们实验中实际需求的4倍。虽然PyTorch允许手动回收内存池，但它会回收所有内存块，从而在未来的分配中引入开销。最近的工作，GMlake ，通过使用CUDA虚拟内存和统一的2MB内存块来减少内存池中的碎片，但它仍然缺乏弹性内存回收。此外，GMlake中每个块上昂贵的IPC操作在数据传递中引入了大量开销（在我们的实验中高达45 ms）。 原本keep alive策略根据每个函数的请求间隔定义函数停留时间，也就是该函数所分配的内存需要维持的时间，但是对于本文所解决的工作流的场景，每个函数的中间数据大小也在波动，因此该波动仍会引起以上提到的问题，因此引入了三个变量用来计算保留的内存大小，如下图： <div style \"text align:center;\"> <img src \"./QQ20241202 200556.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/> </div> f分别是请求间隔（$R_{window}$），中间数据大小($R_{size}$)，以及数据积累度($R_{con}$)，内存保留计算为 $Data_{size} R_{size} \\cdot R_{con}$ ,内存池的最大大小为 $MemPool_{size} \\sum_{func}Data_{size} \\cdot 1_{ {R_{window} \\bigcap t \\neq 0} }$。 <div style \"text align:center;\"> <img src \"./QQ20241202 201622.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/> </div> 如上图，对于函数a，其请求数据a1由于先入队，所以先出（LRU策略），但是b1需要用到a1，但此时a1已经被复制到主机，b1还需要重新加载数据而带来延迟，因此faastube优先将a2这种不再使用的数据复制到主机（如何知道是否使用<font color red>?</font>），并且清除不再需要的中间数据，在有足够内存时，将先前迁移出去的数据再迁移回来，以避开transfer的高峰。"},"/docs/references/ann/fusionANNS/fusionANNs.html":{"title":"FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for Billion-scale Approximate Nearest Neighbor Search","content":"# FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for Billion scale Approximate Nearest Neighbor Search ## RAG <div style \"text align:center;\"><img src \"./QQ20241203 152351.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 上图为RAG框架，首先将领域知识作为高维向量嵌入到向量数据库中。当聊天机器人接收到查询时，它使用ANNS引擎从向量数据库中检索最相关的知识，从而允许LLM将该知识用作额外的上下文以进行更准确的推理。*事实上，我认为该框架更符合AGI的发展方向，如今人工智能的发展过于依赖神经网络的万能拟合，而忽略了严谨知识，我们应该理解的是，先验知识被训练作为NN中的参数具有一定的不可控性，比较明显的证据就是大模型的幻觉。神经网络应该作为逻辑推理模块而非一个知识库（<font color red>?</font>）*。 > 提到的IVF（inverted file），是直接对所有的向量进行聚类，然后将查询向量同聚类中心计算距离，取最近的前topk个簇，然后遍历计算簇下面的向量。 文章认为IVF占用了大量的内存资源，同时PQ是一种有损压缩，准确率不高。而如果直接使用gpu加速基于IVF的SPANN，性能甚至比原来下降10%，根本原因是数据的大量迁移。 ## ANN 关于ANN的知识点详见[ANN](../../algorithms/ann/ann.html) ANN中有三种常见的方法，kd树，在论文中称为分层索引；乘积量化，结合矢量量化使用；局部敏感hash；论文中还提到了使用GPU加速技术，但将这些方法集和起来使用发现比微软的SPANN性能要差。 <div style \"text align:center;\"><img src \"./QQ20241204 200225.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> > *直观地说，可以将分层索引、乘积量化和GPU加速技术结合起来，以实现最佳的ANNS解决方案。然而，我们发现，这些技术的组合导致甚至比SPANN更差的性能，它只利用层次索引（第2.3节）。总的来说，在GPU加速的ANNS系统中，将分层索引与产品量化合作仍然存在一些挑战。* <div style \"text align:center;\"><img src \"./QQ20241204 194332.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 对于spann，其qps不随线程增加而增大，在4个线程就达到了最大值，且不再增大，也就说硬件资源的增加不影响算法的效率，从软硬协同的角度这显然阻碍了效率的提升。 文章探究了其原因，测量了每个ANNS查询平均所需的I/O数量以及跨SSD、主内存和GPU HBM传输的数据量。如图c所示，虽然PQ技术显著地将发布列表的I/O大小从12 ‐ 48 KB减少到了页面粒度（4 KB），但由于重新排序过程，它将I/O数量增加了70%。因此，I/O性能瓶颈从固态硬盘的带宽转移到每秒输入/输出操作数（IOPS）。此外，CPU和GPU之间会传输大量的发布列表，从而抵消了GPU加速带来的好处，如图d所示。 也就是数据传输的频率提高了，每个数据传输请求之间的转换是存在开销的。 <div style \"text align:center;\"><img src \"./QQ20241204 204808.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ## 存在的三个挑战 + 1，为了提高查询的准确性和效率，大多数ANNS系统采用复制策略来构建高质量的IVF索引，其中边界向量被复制到相邻的倒排列表中。这可以显着扩展索引的大小，比原始向量大8倍。即使这些索引使用PQ压缩，GPU的HBM仍然无法容纳所有压缩的索引，导致GPU和CPU之间的大量数据交换。 + 2，由于PQ会造成不小的准确性损失，因此通常将其与向量重排序过程相关联以提高查询准确性。然而，由于在不同的压缩向量之间准确度损失显著地变化，因此确定在给定的准确度约束下对于每个查询需要重新排序的向量的最小数目是具有挑战性的。 + 3，由于原始向量（128 〜 384字节）远小于现代NVMe固态硬盘的最小读取粒度（4 KB），因此每次请求原始向量都会导致读取放大，从而导致重新排序期间的I/O效率较低。(不是很能理解这个，按道理，即使是向量，在主存中也应是顺序存储，这与读取粒度的关系是什么<font color red>?</font>也就是说，虽然原始向量只有128字节，但是一次可以读取多个向量<font color red>?</font>受到影响的应该仅仅是最后向量的读取。) ## solution 先看fusionANNS的架构图(非常清晰的图，很值得学习)： <div style \"text align:center;\"><img src \"./QQ20241204 205436.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 离线部分略过，讲讲在线部分，当有一个查询向量时， + 1，gpu生成距离表用于PQ计算（<font color red>?</font>） + 2，cpu查询前m个最近的倒排列表，就是前m个距离最近的质心，因为涉及排序，所以放在cpu（<font color red>?</font>存疑），排序也可以放在gpu来着。 + 3，查询元数据，因为上面拿到的是质心，需要获取质心所属簇的向量用来计算距离。 + 4，cpu传输这些向量到gpu + 5，PU接收到向量ID时，它首先使用并行hash模块删除重复数据。（更像是相近的归为一组，仅保留一个数据） + 6，GPU从HBM中读取相应的压缩向量，并计算它与查询向量之间的PQ距离。就是求查询向量与PQ得到的码本中的距离之和。 + 7.取计算得到的前n个id。 + 8，cpu使用启发式重新排序机制（<font color red>?</font>） + 9，re ranking，返回前k个邻居，在这个阶段采用冗余感知i/o消除重复i/o。 看看详细设计 <div style \"text align:center;\"><img src \"./QQ20241204 230506.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 上图中可以看到，首先将原始的vector数据通过分层平衡聚类算法聚类成n个list，每个list中就是每个簇的向量数据，将这些簇构造为图，并将图和簇内向量复制到主存。（图索引的构建方法：该图是通过不断向空图添加新的向量来构建的。将向量添加为新顶点时，将创建新边以将此新添加的顶点与其top k(通常为)最近的相邻顶点连接起来。然后，其相邻顶点应更新其最近的邻居以限制最大边数。） 后文提到了一个提高聚类质量的方法，我们focus on efficiency，所以不做赘述。 到这里，丢弃掉中间发布列表，仅保留上图所示的主机内存中的内容，到这里该系统在通用服务器上能够支持十亿规模的ANN。 同时注意到图下方的PQ矢量被存储在HBM当中。将所有压缩向量固定在HBM，避免了cpu与gpu大量的数据交换。（到这里，存在的一个疑惑是，原本的系统是如何处理这些数据的<font color red>?</font>为何之前的系统没有这样做<font color red>?</font>） 由于内存中的索引仍然保留了边界向量复制机制的优点，FusionANNS可以有效地获得候选向量的所有ID，然后将这些向量ID（不包括向量的内容）发送到GPU进行距离计算。通过这种方式，FusionANNS还消除了CPU和GPU之间有限的PCIe带宽造成的性能瓶颈。 后文提到： > SSD上的原始矢量。与基于IVF的SPANN将所有发布列表存储在SSD上不同，FusionANNS只需要将原始向量存储在SSD上以进行重新排名。由于原始向量的体积几乎是倒排列表的8倍，FusionANNS可以显著降低存储消耗。对于每个查询，由于只有重新排序过程会产生少量I/O请求，因此FusionANNS还可以缓解并发查询的SSD I/O瓶颈。 我不是很能理解，我非常好奇原本的系统难道不是这么做的吗<font color red>?</font>或者是将所有的原始向量都加载到主存<font color red>?</font><font color red>?</font> ### 重排机制 该机制是用以提高PQ带来的准确率降低（<font color red>?</font>），将重排化为多个小批处理（batch）按顺序执行，这样可以达到一旦重排不再有利于准确率就立即停止重排，提升效率。 > 由于所有候选向量都以其距离升序排序，因此较早执行的小批量通常具有更高的可能性来识别属于最终前k个最近邻居的更多向量。一旦一个小批处理完成，我们利用一个轻量级的反馈控制模型来检查后续的小批处理是否有利于提高查询精度。 轻量级反馈控制模型依据以下公式：$$\\Delta \\frac{S_{n} (S_{n} \\bigcap S_{n 1})}{k}$$ $S_{n}$和$S_{n 1}$分别是第n批和第n 1批向量id集合。k表示需要得到的前k个向量，也就是维护的max heap的向量数。这个公式的意义也就是向量的变化率，如果变化率连续$\\beta$次小于阈值$\\varepsilon$，那就终止重排。（显然，公式计算时间可以忽略。 <div style \"text align:center;\"><img src \"./QQ20241204 233713.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 看看上述过程的伪码，仅做个参考，抽象层级太高了。总结一下就是，按批处理数据，在每个批次中更新大端堆，当一个批次更新完后，看看是否停止重排。 ### 冗余感知i/o接口 到这里，前面提到的SSD细粒度问题得到解决，那就是之所以会产生细粒度不匹配问题，也就是单个向量过小（> 128k），而SSD访存单位为4k的这个问题，是因为在上述过程中，每次访存只访问一个向量，见算法1. 因此对于上面的过程会面临大频率的i/o请求，带来效率降低。 > 虽然需要重新排序的向量是通过PQ距离获得的，但它们都与查询向量高度相似，这使得它们通常在空间上彼此接近。这种相似性提供了通过仔细组织SSD上的数据布局来减轻读取放大的机会。 > 具体来说，当离线创建内存中索引时,对于导航图中的每一个质心，我们使用桶来存储最接近质心的多个原始向量。我们注意到，在桶之间没有重复的向量。对于每个桶，如果它不与SSD页面对齐，我们使用最大 最小算法(<font color red>?</font>)基于未对齐部分的大小组合联合收割机桶，以最小化SSD页面上的可用空间。最后，我们将所有桶分组为单个文件并将其存储在SSD上，并使用内存中的表来维护向量和SSD页面之间的映射。 似乎以上方法仅仅是将每个簇分文件存储以达到并行访问的目的<font color red>?</font>但似乎没什么意义，因为每次查询仅访问单个向量，此处没办法采用并行策略。 以下是i/o重复数据删除机制： <div style \"text align:center;\"><img src \"./QQ20241204 235737.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 原文已经讲得十分清楚，这里贴一下原文： > 其中小批量0的任务是重新排序向量：V2、V4和V6。mini batch 1的任务是对向量V5、V8和V9重新排序。当执行mini batch 0时，它首先查询映射表以获得与所请求的向量相对应的SSD页面ID。由于V2和V6都存储在同一个SSD页面P0中，因此我们可以合并这两个I/O请求，只读取一个SSD页面以获得V2和V6。由于P0和P2不存在于DRAM缓冲区中，我们通过两个I/O请求直接将它们读取到DRAM缓冲区。在小批量1中，尽管V5、V8和V9被存储在不同的SSD页中，但是DRAM缓冲器已经包含P2，P2包括V5。因此，mini batch 1只需要通过两个I/O请求读取P1和P3。 其中提到的合并v2和v6作为一个i/o请求，但并未提到其实现方法，dram缓存和这篇论文貌似没啥关系<font color red>?</font>这是SOC中已经非常成熟的cache设计，不再赘述。"},"/docs/projects/ascend_matrix/index.html":{"title":"昇腾算子矩阵乘探究","content":"# 昇腾算子矩阵乘探究 ## 编写昇腾算子完成矩阵乘 [点击查看相关简介](../../tutorial/ascend/mindspore_develop/index.html) ## 使用add_custom算子验证流水线以及多缓冲 [原始数据.xlsx](./工作簿1.xlsx) >!数据的长度限制在int32范围，因此我们采用2048\\*64\\*64\\*64为上限； >!同时需要注意的是，cache的容量为65536，而输入数据的大小为int16，因此，一次add支持的长度为2048\\*16个int16 ### 首先验证计算段远快于搬运段，同时证明流水 我们仅仅需要在compute阶段增加一个add函数 >!这个add函数仅仅执行一次add操作，不会改变任何数据，除了compute执行时间 two add即： ![alt text](image 8.png) total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms)tip 2048\\*64\\*64\\*64164\\*64\\*812048\\*831.8391zero add 2048\\*64\\*64\\*64164\\*64\\*812048\\*841.581one add 2048\\*64\\*64\\*64164\\*64\\*812048\\*851.2499two add 2048\\*64\\*64\\*64164\\*64\\*812048\\*860.9961three add 老实说，上面的时间相当微妙，如果按照正常逻辑，计算段的时间远低于搬运段，且满足流水，那么此处的时间应该相差不大，但是存在差距，而且不小，且每次增加的时间基本相等。 增加一个实验，仅进行相同次数的add操作，如下： ![alt text](image.png) 得到执行时间为9.7928ms 所以存在以下两种可能性： + 非流水，整个core func是串行执行 + add执行的时间接近或高于2048*8个数据的搬运 再增加三个实验，即仅进行copyin、copyout、copyin+copyout ![alt text](image 4.png) copyin执行时间为19.158ms copyout执行时间为9.28448ms copyin+copyout执行时间为19.523ms 以上数据能够完美的证明搬入搬出单元是独立的,以及x，y的数据搬运是串行的，得到以下流水图示： ![alt text](image 1.png) 还有一点在于，在执行freeTensor之前，我们没办法执行新的一轮搬运，这是因为我们的buffernum为1，否则无法满足上述理论模型。 因此，对于buffernum为1的情况，约束条件为，tile> 2的copyin需要在add之后执行，add的执行必须在copyout之后。 接着实验 ### 单核下，成倍增加数据总长度 total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms) 2048\\*64\\*64\\*8 164\\*64\\*812048 14.9771 2048\\*64\\*64\\*16164\\*64\\*812048\\*218.1726 2048\\*64\\*64\\*32164\\*64\\*812048\\*425.9916 2048\\*64\\*64\\*64164\\*64\\*812048\\*841.5741 当成倍增长总数据长度时，如果是串行，那么执行时间也应该线性增长，但是执行时间非线性，这可以侧面说明执行流程当中的流水。 对于增加数据长度，数据片数量不变而带来的时间增长，这个很好理解，虽然tile数量没变，但是tile_length在增长： ![alt text](image 2.png) 对于以上流程，其copyin以及compute决定了执行的总时间，验证copyout是否真的不影响总的执行时间，即在copyout中再执行一轮datacopy，如下： ![alt text](image 3.png) 得到的数据与上面表格对应的执行时间分别是14.9949，18.7419，27.2262，44.5016，基本不产生区别，可证。 ### buffer num变化 total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch_2buffer(ms)mean_time_100epoch_1buffer(ms) 2048\\*64\\*64\\*64164\\*64\\*8 22048\\*821.707341.5925 2048\\*64\\*64\\*64164\\*64\\*1622048\\*435.014851.8856 2048\\*64\\*64\\*64164\\*64\\*3222048\\*269.96 72.5588 2048\\*64\\*64\\*64164\\*64\\*6422048 139.851119.389 随着数据片的数量增加，tile长度减小，时间损耗增大，如下图： ![alt text](image 6.png) ![alt text](image 7.png) 注意到同单缓冲数据相比，数据片长度分别为2048\\*8，2048\\*4，2048\\*2的样例表现更好，但是当数据片更小时,反而表现更差， 采用以下图例进行说明： ![alt text](image 5.png) 当数据片较大时，充分利用到了double buffer，即在进行add的同时也能进行copyin，add无需在copyout以后执行，因此效果较好。 但是当数据片较小时，为什么double buffer的表现效果却比单缓冲的要差？关于这一点不能够理解。 ### 多核 total_lenghtuse_core_numtile_numbuffer_numtile_lengthmean_time_100epoch(ms) 2048\\*64\\*64\\*64164\\*64\\*412048\\*1636.8166 2048\\*64\\*64\\*64264\\*64\\*212048\\*1618.5086 2048\\*64\\*64\\*64464\\*64\\*112048\\*169.35409 2048\\*64\\*64\\*64864\\*3212048\\*164.97731 2048\\*64\\*64\\*641664\\*1612048\\*163.17449 实验符合预期 ## matrix 验证 ## 杂项 ![alt text](e06f146dd10f3b4bda45b86a43484302_720.jpg) 上面是分片，由上注意到，当B_Split_W为128，group_size为15时，根据代码，blockid 0，15，30的aicore会处理第0组数据，但是B矩阵仅被分了两组数据，因此blockid 30的aicore会轮空，也就是aicore 30实际上没有计算，其余aicore的计算负载增大，导致执行时间延长。"},"/docs/projects/ftp-design/ftp-cjc.html":{"title":"openEuler应用软件开发赛+什么都不会+初赛+ftp服务器实现","content":"# openEuler应用软件开发赛+什么都不会+初赛+ftp服务器实现 > 支持被动模式避免客户端位于firewall或者NAT后面的情况 > > 支持主动模式 > > 整个项目完全使用仓颉语言开发 > > 未使用任何开源项目 > > x86_64/aarch_64 ## run + server `bash run ftp.sh` + client `dnf install ftp` `ftp 127.0.0.1` 内置用户 userpassword ftp user1123456 user2123456 可用协议指令 inst参数用途 \"USER\"username登陆用户 \"PASS\"password登陆密码 \"SYST\"查看系统信息 \"PORT\"port使用客户端端口设置主动模式 \"LIST\"列出目录文件 \"CWD\" path切换目录 \"PWD\" 查看当前路径 \"PASV\"设置被动模式 \"TYPE\"mode设置传输格式 \"RETR\"file下载文件 \"STOR\"file上传文件 \"MKD\" file创建文件夹 \"ABOR\"流产连接 \"QUIT\"退出 ## 效果 + filezilla ![filezilla](./filezilla.png) + ftp ![ftp](./ftp.png) + curl ![curl](./curl.png) ## 整体架构 ```mermaid flowchart LR frontend[\"frontend\"] requestDistri[\"request distribution\"] client[\"client\"] subgraph processUnit direction LR parser[\"parser\"] sender[\"sender\"] end subgraph processUnit1 end subgraph processUnit2 end subgraph ... end transCtl[\"transport control\"] userCtl[\"user control\"] utils[\"utils(String2Int...)\"] config[\"configuration(only one)\"] asClient[\"asClient\"] asServer[\"asServer\"] fs[\"file system\"] requestDistri > frontend parser > requestDistri processUnit1 > requestDistri processUnit2 > requestDistri ... > requestDistri frontend > client client > frontend config > userCtl transCtl > parser userCtl > parser fs > transCtl config > transCtl asClient > transCtl asServer > transCtl userCtl > transCtl linkStyle 0,1,2,3,4,5,6,8,9,10,12,13,14 stroke:green linkStyle 7,11 stroke:red ``` ## 并发架构 + 资源池方案 将操作系统资源抽象为池，当出现新的用户请求时，从池中取出资源进行分配 在本项目中的实现即为，使用request Distribution（reqdist）模块解耦业务逻辑与用户请求 新的请求进入时，reqdist模块将会分配新的线程用于处理当前的请求，当请求断开时，释放线程资源回到池中。因此所有占用的资源都是可限制的，我们可以预设线程数以限制访问的qps，防止某些不安全攻击操作。 [可扩展]解耦的好处是，我们能够轻易的扩展其余硬件，例如，当服务器不止一个cpu（或者其他协处理器）时，线程资源分配将会复杂化，我们仅需要在reqdist模块中将资源分配给process unit，而不必考虑具体的业务逻辑。 + 资源锁 多个线程对文件的变动将有可能产生资源冲突。 我们最小化公共资源，将所有对实际文件的操作都聚集在configer模块当中，因此我们使用configer改变文件状态时，将会使用锁解决访问资源冲突问题。 同时对资源进行精细化控制，以便平衡时间与资源同步的问题。 ## 权限管理 + 文件系统 为了实现足够精细化的权限管理，我们手工实现了一个简单的文件系统，仿照linux文件系统的inode设计，下面是一个文件夹的inode结构体，使用json文件存储inode列表。 ```json { \"name\": \"/\", \"type\": \"d\", \"user\": \"root\", \"user_power\": 7, \"group_power\": 7, \"others_power\": 0, \"subfiles\": [ 0,0,1,2 ] } ``` 文件的id取决于文件inode在json array中的index + 权限 我们的权限系统依然仿照linux，user_power代表用户（创建者）本身的权限，group_power代表组用户的权限，others_power代表其他用户的权限。 组用户取决于config.json文件中user group，如下 ```json \"user1\":{ \"password\":\"123456\", \"group\":[\"ftp\"], \"root\":1 } ``` 例如上面的user1信息中，ftp默认用户是其组用户成员 权限数字分别代表三位数字的bool值，当第一位为1，也就是100时值为4，代表有下载查看权限，当第二位为1，也就是010时值为2，代表有下载权限，剩余一位留作扩展。 + tip **既然每一样都跟linux类似，为什么不直接用linux的文件系统？** > 因为我们对项目的要求是可扩展性，如果直接使用linux的文件系统，如果我们以后需要扩展其余权限，例如更改文件的权限，删除文件的权限，我们的扩展就会非常困难。 ## 数据传输 ### 主动模式 主动模式下，需要客户端告知服务器端口 因此服务器需要作为客户端连接用户机，此时无需考虑端口的问题。 ### 被动模式 被动模式下，需要服务端告知客户端数据端口 此时服务器作为数据传输的服务器等待客户端连接。 服务器需要扫描服务器上的可用端口告知客户端以连接。 + 端口池 秉持所有资源皆可控的理念，我们将端口也作为配置放入配置文件，端口作为资源为客户端分配，达到可控的方式。 **假设我们采用临时扫描端口的方式，那么不得不考虑的问题是，在扫描到空闲端口后，但是该端口实际上属于service所使用的端口（出现暂时空闲），在扫描到确认连接这段时间内，该端口有可能被重新占用的问题。因此我们的方案既避免服务占用ftp端口的情况，也避免ftp占用服务端口的情况，同时减少代码开发代价** ## source [download](https://atomgit.com/openeuler123/nihaopeng)"},"/docs/projects/IntelligentAudioChat/index.html":{"title":"","content":""},"/docs/projects/index.html":{"title":"some tutorials about the configure","content":"# some tutorials about the configure some tutorials about the configure > **click the sidebar to open markdown file** >! [read from the first file](./ascend_matrix/index.html)"},"/docs/projects/riscv-telechat-openeuler/openeuler/openeuler.html":{"title":"openeuler 烧录到licheepi4a","content":"# openeuler 烧录到licheepi4a 跟着[官网教程](https://docs.openeuler.org/zh/docs/24.03_LTS/docs/Installation/RISC V LicheePi4A.html)走 tips： + 1，显示器可能会显示不出来，用串口直连调试 + 2，wifi无法正常list，网口直连，不要那种需要登陆的网络 + 3，登录账号：root，密码：openEuler12#$ + 4，如果无法下载东西不是因为源的问题，看看网络是否连接上了"},"/docs/projects/riscv-telechat-openeuler/telechat移植/telechat.html":{"title":"telechat-12B移植","content":"# telechat 12B移植 ## 从hf mirror下载模型 别问我为啥不在hf官网，梯子流量够用随便搞 + download hfd,a tool to download base on aria2 `wget https://hf mirror.com/hfd/hfd.sh` `chmod a+x hfd.sh` + 设置hugginface下载环境变量 `export HF_ENDPOINT https://hf mirror.com` + 下载模型 `./hfd.sh Tele AI/TeleChat 12B tool aria2c x 4` >出现报错看下面的教程 + 下载数据集（用于记录，不用执行） `./hfd.sh wikitext dataset tool aria2c x 4` ## aria编译 如果你跟着上面的步骤走，那一定会到这里，因为yum源没有aria2安装包，乐 + 克隆aria2源码 `git clone https://github.com/aria2/aria2.git` + build `autoreconf i` autoreconf装了运行不了，缺少autopoint + yum install gettext devel 装依赖 `yum install openssl devel zlib zlib devel` `./configure ARIA2_STATIC yes` error:A compiler with support for C++11 language features is required. + solution:yum install g++ `make install j4` ## git lfs编译 如果你跟着上面的步骤走，那一定会到这里，因为git lfs妹有riscv64版本的，蚌 + 安装go `yum install golang` + 编译 1,克隆git lfs 2,`go env w GOPROXY https://goproxy.cn` 3,添加bin目录到环境变量 4，执行`git lfs install` ## openeuler riscv上编译pytorch + 依赖 `dnf install python3 {hypothesis,psutil,pyyaml,requests,sympy,filelock,networkx,jinja2,fsspec,packaging,numpy,venv}` + 下载gitee预编译riscv whl `git clone recursive https://github.com/pytorch/pytorch`约4个GB，建议用梯子 `cd pytorch && python3 setup.py develop` + 测试 <div style \"text align:center;\"><img src \"QQ20241210 150124.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ## 安装transformers `git clone https://github.com/huggingface/transformers.git` >下面这句逆天操作据说是ninja和cmake互相依赖导致无限递归的问题，乐 `dnf install cmake python3 devel` 安装rust compiler(逆天) `export RUSTUP_DIST_SERVER https://mirrors.ustc.edu.cn/rust static` `export RUSTUP_UPDATE_ROOT https://mirrors.ustc.edu.cn/rust static/rustup` `curl proto ' https' tlsv1.2 sSf https://sh.rustup.rs sh` `pip install 'transformers[torch]'` ## 运行python inference + 问题1：需要安装flash attn，然而该库依赖cuda支持 修改12B模型中的modeling_telechat.py的357，增加config中的flash attn判断以取消初始化FlashSelfAttention类产生错误，如下图： <div style \"text align:center;\"><img src \"QQ20241210 223624.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 记得修改config中的参数 <div style \"text align:center;\"><img src \"QQ20241211 091833.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 修改虚拟环境中.venv/lib/python3.11/site packages/transformers/generation/utils.py用以显示进度 <div style \"text align:center;\"><img src \"QQ20241211 135143.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 推理脚本如下： ```python import os import torch from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig tokenizer AutoTokenizer.from_pretrained('../models/12B', trust_remote_code True) model AutoModelForCausalLM.from_pretrained('../models/12B', trust_remote_code True, torch_dtype torch.float16) device \"cpu\" model.to(device) generate_config GenerationConfig.from_pretrained('../models/12B') question \"你好！你是谁？\" answer, history model.chat(tokenizer tokenizer, question question, history [], generation_config generate_config, stream False) print(answer) ``` `python3 test.py` ## huggingface转为onnx `pip install optimum[exporters]` [下载12B模型](#从hf mirror下载模型) [修改推理模型](#运行python inference) 运行以下脚本转换为onnx格式 ```python import os import torch from transformers import AutoTokenizer, AutoModelForCausalLM import onnx import onnxruntime as ort import numpy as np os.environ['TRANSFORMERS_OFFLINE'] '1' class ModelWrapper(torch.nn.Module): def __init__(self, model): super().__init__() self.model model def forward(self, input_ids, attention_mask): # 显式传入 use_cache False，确保不使用 past_key_values outputs self.model(input_ids input_ids, attention_mask attention_mask, use_cache False) # 只返回 logits，以避免复杂的输出结构导致的问题 return outputs.logits def export_model_to_onnx(model_name, output_path, opset_version 13): tokenizer AutoTokenizer.from_pretrained(model_name, trust_remote_code True) model AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code True) model.eval() model.config.use_cache False text \"This is a sample input for ONNX export.\" inputs tokenizer(text, return_tensors \"pt\") # 用包装器替换原始模型 wrapped_model ModelWrapper(model) wrapped_model.eval() input_names [\"input_ids\", \"attention_mask\"] output_names [\"logits\"] dynamic_axes { \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"}, \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"}, \"logits\": {0: \"batch_size\", 1: \"sequence_length\"} } # 导出为 ONNX torch.onnx.export( wrapped_model, args (inputs[\"input_ids\"], inputs[\"attention_mask\"]), f output_path, input_names input_names, output_names output_names, dynamic_axes dynamic_axes, opset_version opset_version, export_params True, do_constant_folding True, ) print(f\"模型已成功导出到 {output_path}\") def validate_onnx_model(model_path, model_name, text): tokenizer AutoTokenizer.from_pretrained(model_name, trust_remote_code True) model AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code True) model.eval() model.c ``` ## onnx使用hhb转为二进制可执行文件 ## 利用npu加速执行 目前（2024 12 11）openeuler暂不支持npu。 2025 3 12, 当前使用openeuler24.03 sp1，已支持npu 手动挂载驱动，如下： ```bash insmod /lib/modules/6.6.0 72.0.0.76.oe2403sp1.riscv64/kernel/drivers/soc/xuantie/nna/img_mem/img_mem.ko.xz modprobe vha onchipmem_phys_start 0xffe0000000 onchipmem_size 0x100000 freq_khz 792000 insmod /lib/modules/6.6.0 72.0.0.76.oe2403sp1.riscv64/kernel/drivers/soc/xuantie/nna/vha/vha_info.ko.xz chmod a+rw /dev/vha0 lsmod ``` 此时/dev文件夹出现vha0"},"/docs/projects/sd1.5-fine-tuning/sd.html":{"title":"","content":"<style> pre { overflow y: auto; max height: 300px; } </style> # stable diffusion fine tuning ## ubuntu安装cuda toolkit `sudo apt install nvidia cuda toolkit` `nvcc V`验证安装并查看cuda版本 ## 安装pytorch 老生常谈了，这里不再给教程，需要注意的几个点是， + 下载的pytorch版本需要和cuda版本对应 + torch版本和GPU计算架构对应 ## 下载stable diffusion预训练模型 ### 从hf mirror下载模型 [点这里看更详细教程](../riscv telechat openeuler/telechat移植/telechat.html) 别问我为啥不在hf官网，梯子流量够用随便搞 + download hfd,a tool to download base on aria2 `wget https://hf mirror.com/hfd/hfd.sh` `chmod a+x hfd.sh` + 设置hugginface下载环境变量 `export HF_ENDPOINT https://hf mirror.com` + 下载模型 `./hfd.sh Tele AI/TeleChat 12B tool aria2c x 4` + 下载数据集（用于记录，不用执行） `./hfd.sh wikitext dataset tool aria2c x 4` ## 预处理数据 你的自定义数据集放在一个文件夹内，我们使用元数据的方式训练，放图片的同一个文件夹下创建一个`metadata.jsonl`文件，文件格式如下： ```json { \"file_name\": \"2.jpg\", \"text\": \"Off white flat garden layout,irregularity,water,mellow\" } { \"file_name\": \"3.jpg\", \"text\": \"Off white flat garden layout,irregularity,line,order\" } { \"file_name\": \"4.jpg\", \"text\": \"Off white flat garden layout,irregularity,bridge,water,line,mellow\" } ... ``` ## 微调 微调脚本，其中的MODEL_NAME赋值为你下载的模型文件路径， OUTPUT_DIR为微调中间文件 DATASET_NAME为放数据集的路径 如果你有多个gpu设备的话，可以将shell脚本中的python换为accelerate分布式训练（如果能到使用accelerate这一步，训练过程中产生的问题相信你也没有问题了） ```bash export MODEL_NAME \"../stable diffusion v1 5\" export OUTPUT_DIR \"./out_lora2\" export DATASET_NAME \"./data/layout\" python train.py \\ pretrained_model_name_or_path $MODEL_NAME \\ dataset_name $DATASET_NAME \\ dataloader_num_workers 0 \\ resolution 512 center_crop random_flip \\ train_batch_size 10 \\ gradient_accumulation_steps 4 \\ max_train_steps 15000 \\ learning_rate 1e 04 \\ max_grad_norm 1 \\ lr_scheduler \"cosine\" lr_warmup_steps 0 \\ output_dir ${OUTPUT_DIR} \\ checkpointing_steps 500 \\ seed 3407 \\ report_to wandb \\ ``` train.py文件是lora的训练脚本。 ```python #!/usr/bin/env python # coding utf 8 # Copyright 2024 The HuggingFace Inc. team. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE 2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"Fine tuning script for Stable Diffusion for text2image with support for LoRA.\"\"\" import argparse import logging import math import os import random import shutil from contextlib import nullcontext from pathlib import Path import datasets import numpy as np import torch import torch.nn.functional as F import torch.utils.checkpoint import transformers from accelerate import Accelerator from accelerate.logging import get_logger from accelerate.utils import ProjectConfiguration, set_seed from datasets import load_dataset from huggingface_hub import create_repo, upload_folder from packaging import version from peft import LoraConfig from peft.utils import get_peft_model_state_dict from torchvision import transforms from tqdm.auto import tqdm from transformers import CLIPTextModel, CLIPTokenizer import diffusers from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel from diffusers.optimization import get_scheduler from diffusers.training_utils import cast_training_params, compute_snr from diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card from diffusers.utils.import_utils import is_xformers_available from diffusers.utils.torch_utils import is_compiled_module if is_wandb_available(): import wandb # Will error if the minimal version of diffusers is not installed. Remove at your own risks. # check_min_version(\"0.31.0.dev0\") logger get_logger(__name__, log_level \"INFO\") def save_model_card( repo_id: str, images: list None, base_model: str None, dataset_name: str None, repo_folder: str None, ): img_str \"\" if images is not None: for i, image in enumerate(images): image.save(os.path.join(repo_folder, f\"image_{i}.png\")) img_str + f\"<div style \"text align:center;\"><img src \"./image_{i}.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div>\\n\" model_description f\"\"\" # LoRA text2image fine tuning {repo_id} These are LoRA adaption weights for {base_model}. The weights were fine tuned on the {dataset_name} dataset. You can find some example images in the following. \\n {img_str} \"\"\" model_card load_or_create_model_card( repo_id_or_path repo_id, from_training True, license \"creativeml openrail m\", base_model base_model, model_description model_description, inference True, ) tags [ \"stable diffusion\", \"stable diffusion diffusers\", \"text to image\", \"diffusers\", \"diffusers training\", \"lora\", ] model_card populate_model_card(model_card, tags tags) model_card.save(os.path.join(repo_folder, \"README.md\")) def log_validation( pipeline, args, accelerator, epoch, is_final_validation False, ): logger.info( f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\" f\" {args.validation_prompt}.\" ) pipeline pipeline.to(accelerator.device) pipeline.set_progress_bar_config(disable True) generator torch.Generator(device accelerator.device) if args.seed is not None: generator generator.manual_seed(args.seed) images [] if torch.backends.mps.is_available(): autocast_ctx nullcontext() else: autocast_ctx torch.autocast(accelerator.device.type) with autocast_ctx: for _ in range(args.num_validation_images): images.append(pipeline(args.validation_prompt, num_inference_steps 30, generator generator).images[0]) for tracker in accelerator.trackers: phase_name \"test\" if is_final_validation else \"validation\" if tracker.name \"tensorboard\": np_images np.stack([np.asarray(img) for img in images]) tracker.writer.add_images(phase_name, np_images, epoch, dataformats \"NHWC\") if tracker.name \"wandb\": tracker.log( { phase_name: [ wandb.Image(image, caption f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images) ] } ) return images def parse_args(): parser argparse.ArgumentParser(description \"Simple example of a training script.\") parser.add_argument( \" pretrained_model_name_or_path\", type str, default None, required True, help \"Path to pretrained model or model identifier from huggingface.co/models.\", ) parser.add_argument( \" revision\", type str, default None, required False, help \"Revision of pretrained model identifier from huggingface.co/models.\", ) parser.add_argument( \" variant\", type str, default None, help \"Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\", ) parser.add_argument( \" dataset_name\", type str, default None, help ( \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\" \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\" \" or to a folder containing files that 🤗 Datasets can understand.\" ), ) parser.add_argument( \" dataset_config_name\", type str, default None, help \"The config of the Dataset, leave as None if there's only one config.\", ) parser.add_argument( \" train_data_dir\", type str, default None, help ( \"A folder containing the training data. Folder contents must follow the structure described in\" \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\" \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\" ), ) parser.add_argument( \" image_column\", type str, default \"image\", help \"The column of the dataset containing an image.\" ) parser.add_argument( \" caption_column\", type str, default \"text\", help \"The column of the dataset containing a caption or a list of captions.\", ) parser.add_argument( \" validation_prompt\", type str, default None, help \"A prompt that is sampled during training for inference.\" ) parser.add_argument( \" num_validation_images\", type int, default 4, help \"Number of images that should be generated during validation with `validation_prompt`.\", ) parser.add_argument( \" validation_epochs\", type int, default 1, help ( \"Run fine tuning validation every X epochs. The validation process consists of running the prompt\" \" `args.validation_prompt` multiple times: `args.num_validation_images`.\" ), ) parser.add_argument( \" max_train_samples\", type int, default None, help ( \"For debugging purposes or quicker training, truncate the number of training examples to this \" \"value if set.\" ), ) parser.add_argument( \" output_dir\", type str, default \"sd model finetuned lora\", help \"The output directory where the model predictions and checkpoints will be written.\", ) parser.add_argument( \" cache_dir\", type str, default None, help \"The directory where the downloaded models and datasets will be stored.\", ) parser.add_argument(\" seed\", type int, default None, help \"A seed for reproducible training.\") parser.add_argument( \" resolution\", type int, default 512, help ( \"The resolution for input images, all the images in the train/validation dataset will be resized to this\" \" resolution\" ), ) parser.add_argument( \" center_crop\", default False, action \"store_true\", help ( \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\" \" cropped. The images will be resized to the resolution first before cropping.\" ), ) parser.add_argument( \" random_flip\", action \"store_true\", help \"whether to randomly flip images horizontally\", ) parser.add_argument( \" train_batch_size\", type int, default 16, help \"Batch size (per device) for the training dataloader.\" ) parser.add_argument(\" num_train_epochs\", type int, default 100) parser.add_argument( \" max_train_steps\", type int, default None, help \"Total number of training steps to perform. If provided, overrides num_train_epochs.\", ) parser.add_argument( \" gradient_accumulation_steps\", type int, default 1, help \"Number of updates steps to accumulate before performing a backward/update pass.\", ) parser.add_argument( \" gradient_checkpointing\", action \"store_true\", help \"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\", ) parser.add_argument( \" learning_rate\", type float, default 1e 4, help \"Initial learning rate (after the potential warmup period) to use.\", ) parser.add_argument( \" scale_lr\", action \"store_true\", default False, help \"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\", ) parser.add_argument( \" lr_scheduler\", type str, default \"constant\", help ( 'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",' ' \"constant\", \"constant_with_warmup\"]' ), ) parser.add_argument( \" lr_warmup_steps\", type int, default 500, help \"Number of steps for the warmup in the lr scheduler.\" ) parser.add_argument( \" snr_gamma\", type float, default None, help \"SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. \" \"More details here: https://arxiv.org/abs/2303.09556.\", ) parser.add_argument( \" use_8bit_adam\", action \"store_true\", help \"Whether or not to use 8 bit Adam from bitsandbytes.\" ) parser.add_argument( \" allow_tf32\", action \"store_true\", help ( \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\" \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat 32 tf32 on ampere devices\" ), ) parser.add_argument( \" dataloader_num_workers\", type int, default 0, help ( \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\" ), ) parser.add_argument(\" adam_beta1\", type float, default 0.9, help \"The beta1 parameter for the Adam optimizer.\") parser.add_argument(\" adam_beta2\", type float, default 0.999, help \"The beta2 parameter for the Adam optimizer.\") parser.add_argument(\" adam_weight_decay\", type float, default 1e 2, help \"Weight decay to use.\") parser.add_argument(\" adam_epsilon\", type float, default 1e 08, help \"Epsilon value for the Adam optimizer\") parser.add_argument(\" max_grad_norm\", default 1.0, type float, help \"Max gradient norm.\") parser.add_argument(\" push_to_hub\", action \"store_true\", help \"Whether or not to push the model to the Hub.\") parser.add_argument(\" hub_token\", type str, default None, help \"The token to use to push to the Model Hub.\") parser.add_argument( \" prediction_type\", type str, default None, help \"The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or leave `None`. If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.\", ) parser.add_argument( \" hub_model_id\", type str, default None, help \"The name of the repository to keep in sync with the local `output_dir`.\", ) parser.add_argument( \" logging_dir\", type str, default \"logs\", help ( \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\" \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\" ), ) parser.add_argument( \" mixed_precision\", type str, default None, choices [\"no\", \"fp16\", \"bf16\"], help ( \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch > \" \" 1.10.and an Nvidia Ampere GPU. Default to the value of accelerate config of the current system or the\" \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\" ), ) parser.add_argument( \" report_to\", type str, default \"tensorboard\", help ( 'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`' ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.' ), ) parser.add_argument(\" local_rank\", type int, default 1, help \"For distributed training: local_rank\") parser.add_argument( \" checkpointing_steps\", type int, default 500, help ( \"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\" \" training using ` resume_from_checkpoint`.\" ), ) parser.add_argument( \" checkpoints_total_limit\", type int, default None, help (\"Max number of checkpoints to store.\"), ) parser.add_argument( \" resume_from_checkpoint\", type str, default None, help ( \"Whether training should be resumed from a previous checkpoint. Use a path saved by\" ' ` checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.' ), ) parser.add_argument( \" enable_xformers_memory_efficient_attention\", action \"store_true\", help \"Whether or not to use xformers.\" ) parser.add_argument(\" noise_offset\", type float, default 0, help \"The scale of noise offset.\") parser.add_argument( \" rank\", type int, default 4, help (\"The dimension of the LoRA update matrices.\"), ) args parser.parse_args() env_local_rank int(os.environ.get(\"LOCAL_RANK\", 1)) if env_local_rank ! 1 and env_local_rank ! args.local_rank: args.local_rank env_local_rank # Sanity checks if args.dataset_name is None and args.train_data_dir is None: raise ValueError(\"Need either a dataset name or a training folder.\") return args DATASET_NAME_MAPPING { \"lambdalabs/naruto blip captions\": (\"image\", \"text\"), } def main(): args parse_args() if args.report_to \"wandb\" and args.hub_token is not None: raise ValueError( \"You cannot use both report_to wandb and hub_token due to a security risk of exposing your token.\" \" Please use `huggingface cli login` to authenticate with the Hub.\" ) logging_dir Path(args.output_dir, args.logging_dir) accelerator_project_config ProjectConfiguration(project_dir args.output_dir, logging_dir logging_dir) accelerator Accelerator( gradient_accumulation_steps args.gradient_accumulation_steps, mixed_precision args.mixed_precision, log_with args.report_to, project_config accelerator_project_config, ) # Disable AMP for MPS. if torch.backends.mps.is_available(): accelerator.native_amp False # Make one log on every process with the configuration for debugging. logging.basicConfig( format \"%(asctime)s %(levelname)s %(name)s %(message)s\", datefmt \"%m/%d/%Y %H:%M:%S\", level logging.INFO, ) logger.info(accelerator.state, main_process_only False) if accelerator.is_local_main_process: datasets.utils.logging.set_verbosity_warning() transformers.utils.logging.set_verbosity_warning() diffusers.utils.logging.set_verbosity_info() else: datasets.utils.logging.set_verbosity_error() transformers.utils.logging.set_verbosity_error() diffusers.utils.logging.set_verbosity_error() # If passed along, set the training seed now. if args.seed is not None: set_seed(args.seed) # Handle the repository creation if accelerator.is_main_process: if args.output_dir is not None: os.makedirs(args.output_dir, exist_ok True) if args.push_to_hub: repo_id create_repo( repo_id args.hub_model_id or Path(args.output_dir).name, exist_ok True, token args.hub_token ).repo_id # Load scheduler, tokenizer and models. noise_scheduler DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder \"scheduler\") tokenizer CLIPTokenizer.from_pretrained( args.pretrained_model_name_or_path, subfolder \"tokenizer\", revision args.revision ) text_encoder CLIPTextModel.from_pretrained( args.pretrained_model_name_or_path, subfolder \"text_encoder\", revision args.revision ) vae AutoencoderKL.from_pretrained( args.pretrained_model_name_or_path, subfolder \"vae\", revision args.revision, variant args.variant ) unet UNet2DConditionModel.from_pretrained( args.pretrained_model_name_or_path, subfolder \"unet\", revision args.revision, variant args.variant ) # freeze parameters of models to save more memory unet.requires_grad_(False) vae.requires_grad_(False) text_encoder.requires_grad_(False) # For mixed precision training we cast all non trainable weights (vae, non lora text_encoder and non lora unet) to half precision # as these weights are only used for inference, keeping weights in full precision is not required. weight_dtype torch.float32 if accelerator.mixed_precision \"fp16\": weight_dtype torch.float16 elif accelerator.mixed_precision \"bf16\": weight_dtype torch.bfloat16 # Freeze the unet parameters before adding adapters for param in unet.parameters(): param.requires_grad_(False) unet_lora_config LoraConfig( r args.rank, lora_alpha args.rank, init_lora_weights \"gaussian\", target_modules [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"], ) # Move unet, vae and text_encoder to device and cast to weight_dtype unet.to(accelerator.device, dtype weight_dtype) vae.to(accelerator.device, dtype weight_dtype) text_encoder.to(accelerator.device, dtype weight_dtype) # Add adapter and make sure the trainable params are in float32. unet.add_adapter(unet_lora_config) if args.mixed_precision \"fp16\": # only upcast trainable parameters (LoRA) into fp32 cast_training_params(unet, dtype torch.float32) if args.enable_xformers_memory_efficient_attention: if is_xformers_available(): import xformers xformers_version version.parse(xformers.__version__) if xformers_version version.parse(\"0.0.16\"): logger.warning( \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\" ) unet.enable_xformers_memory_efficient_attention() else: raise ValueError(\"xformers is not available. Make sure it is installed correctly\") lora_layers filter(lambda p: p.requires_grad, unet.parameters()) if args.gradient_checkpointing: unet.enable_gradient_checkpointing() # Enable TF32 for faster training on Ampere GPUs, # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat 32 tf32 on ampere devices if args.allow_tf32: torch.backends.cuda.matmul.allow_tf32 True if args.scale_lr: args.learning_rate ( args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes ) # Initialize the optimizer if args.use_8bit_adam: try: import bitsandbytes as bnb except ImportError: raise ImportError( \"Please install bitsandbytes to use 8 bit Adam. You can do so by running `pip install bitsandbytes`\" ) optimizer_cls bnb.optim.AdamW8bit else: optimizer_cls torch.optim.AdamW optimizer optimizer_cls( lora_layers, lr args.learning_rate, betas (args.adam_beta1, args.adam_beta2), weight_decay args.adam_weight_decay, eps args.adam_epsilon, ) # Get the datasets: you can either provide your own training and evaluation files (see below) # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub). # In distributed training, the load_dataset function guarantees that only one local process can concurrently # download the dataset. if args.dataset_name is not None: # Downloading and loading a dataset from the hub. dataset load_dataset( args.dataset_name, args.dataset_config_name, cache_dir args.cache_dir, data_dir args.train_data_dir, ) else: data_files {} if args.train_data_dir is not None: data_files[\"train\"] os.path.join(args.train_data_dir, \"**\") dataset load_dataset( \"imagefolder\", data_files data_files, cache_dir args.cache_dir, ) # See more about loading custom images at # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder # Preprocessing the datasets. # We need to tokenize inputs and targets. column_names dataset[\"train\"].column_names # 6. Get the column names for input/target. dataset_columns DATASET_NAME_MAPPING.get(args.dataset_name, None) if args.image_column is None: image_column dataset_columns[0] if dataset_columns is not None else column_names[0] else: image_column args.image_column if image_column not in column_names: raise ValueError( f\" image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\" ) if args.caption_column is None: caption_column dataset_columns[1] if dataset_columns is not None else column_names[1] else: caption_column args.caption_column if caption_column not in column_names: raise ValueError( f\" caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\" ) # Preprocessing the datasets. # We need to tokenize input captions and transform the images. def tokenize_captions(examples, is_train True): captions [] for caption in examples[caption_column]: if isinstance(caption, str): captions.append(caption) elif isinstance(caption, (list, np.ndarray)): # take a random caption if there are multiple captions.append(random.choice(caption) if is_train else caption[0]) else: raise ValueError( f\"Caption column `{caption_column}` should contain either strings or lists of strings.\" ) inputs tokenizer( captions, max_length tokenizer.model_max_length, padding \"max_length\", truncation True, return_tensors \"pt\" ) return inputs.input_ids # Preprocessing the datasets. train_transforms transforms.Compose( [ transforms.Resize(args.resolution, interpolation transforms.InterpolationMode.BILINEAR), transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution), transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x), transforms.ToTensor(), transforms.Normalize([0.5], [0.5]), ] ) def unwrap_model(model): model accelerator.unwrap_model(model) model model._orig_mod if is_compiled_module(model) else model return model def preprocess_train(examples): images [image.convert(\"RGB\") for image in examples[image_column]] examples[\"pixel_values\"] [train_transforms(image) for image in images] examples[\"input_ids\"] tokenize_captions(examples) return examples with accelerator.main_process_first(): if args.max_train_samples is not None: dataset[\"train\"] dataset[\"train\"].shuffle(seed args.seed).select(range(args.max_train_samples)) # Set the training transforms train_dataset dataset[\"train\"].with_transform(preprocess_train) def collate_fn(examples): pixel_values torch.stack([example[\"pixel_values\"] for example in examples]) pixel_values pixel_values.to(memory_format torch.contiguous_format).float() input_ids torch.stack([example[\"input_ids\"] for example in examples]) return {\"pixel_values\": pixel_values, \"input_ids\": input_ids} # DataLoaders creation: train_dataloader torch.utils.data.DataLoader( train_dataset, shuffle True, collate_fn collate_fn, batch_size args.train_batch_size, num_workers args.dataloader_num_workers, ) # Scheduler and math around the number of training steps. # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation. num_warmup_steps_for_scheduler args.lr_warmup_steps * accelerator.num_processes if args.max_train_steps is None: len_train_dataloader_after_sharding math.ceil(len(train_dataloader) / accelerator.num_processes) num_update_steps_per_epoch math.ceil(len_train_dataloader_after_sharding / args.gradient_accumulation_steps) num_training_steps_for_scheduler ( args.num_train_epochs * num_update_steps_per_epoch * accelerator.num_processes ) else: num_training_steps_for_scheduler args.max_train_steps * accelerator.num_processes lr_scheduler get_scheduler( args.lr_scheduler, optimizer optimizer, num_warmup_steps num_warmup_steps_for_scheduler, num_training_steps num_training_steps_for_scheduler, ) # Prepare everything with our `accelerator`. unet, optimizer, train_dataloader, lr_scheduler accelerator.prepare( unet, optimizer, train_dataloader, lr_scheduler ) # We need to recalculate our total training steps as the size of the training dataloader may have changed. num_update_steps_per_epoch math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) if args.max_train_steps is None: args.max_train_steps args.num_train_epochs * num_update_steps_per_epoch if num_training_steps_for_scheduler ! args.max_train_steps * accelerator.num_processes: logger.warning( f\"The length of the 'train_dataloader' after 'accelerator.prepare' ({len(train_dataloader)}) does not match \" f\"the expected length ({len_train_dataloader_after_sharding}) when the learning rate scheduler was created. \" f\"This inconsistency may result in the learning rate scheduler not functioning properly.\" ) # Afterwards we recalculate our number of training epochs args.num_train_epochs math.ceil(args.max_train_steps / num_update_steps_per_epoch) # We need to initialize the trackers we use, and also store our configuration. # The trackers initializes automatically on the main process. if accelerator.is_main_process: accelerator.init_trackers(\"text2image fine tune\", config vars(args)) # Train! total_batch_size args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps logger.info(\"***** Running training *****\") logger.info(f\" Num examples {len(train_dataset)}\") logger.info(f\" Num Epochs {args.num_train_epochs}\") logger.info(f\" Instantaneous batch size per device {args.train_batch_size}\") logger.info(f\" Total train batch size (w. parallel, distributed & accumulation) {total_batch_size}\") logger.info(f\" Gradient Accumulation steps {args.gradient_accumulation_steps}\") logger.info(f\" Total optimization steps {args.max_train_steps}\") global_step 0 first_epoch 0 # Potentially load in the weights and states from a previous save if args.resume_from_checkpoint: if args.resume_from_checkpoint ! \"latest\": path os.path.basename(args.resume_from_checkpoint) else: # Get the most recent checkpoint dirs os.listdir(args.output_dir) dirs [d for d in dirs if d.startswith(\"checkpoint\")] dirs sorted(dirs, key lambda x: int(x.split(\" \")[1])) path dirs[ 1] if len(dirs) > 0 else None if path is None: accelerator.print( f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\" ) args.resume_from_checkpoint None initial_global_step 0 else: accelerator.print(f\"Resuming from checkpoint {path}\") accelerator.load_state(os.path.join(args.output_dir, path)) global_step int(path.split(\" \")[1]) initial_global_step global_step first_epoch global_step // num_update_steps_per_epoch else: initial_global_step 0 progress_bar tqdm( range(0, args.max_train_steps), initial initial_global_step, desc \"Steps\", # Only show the progress bar once on each machine. disable not accelerator.is_local_main_process, ) for epoch in range(first_epoch, args.num_train_epochs): unet.train() train_loss 0.0 for step, batch in enumerate(train_dataloader): with accelerator.accumulate(unet): # Convert images to latent space latents vae.encode(batch[\"pixel_values\"].to(dtype weight_dtype)).latent_dist.sample() latents latents * vae.config.scaling_factor # Sample noise that we'll add to the latents noise torch.randn_like(latents) if args.noise_offset: # https://www.crosslabs.org//blog/diffusion with offset noise noise + args.noise_offset * torch.randn( (latents.shape[0], latents.shape[1], 1, 1), device latents.device ) bsz latents.shape[0] # Sample a random timestep for each image timesteps torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device latents.device) timesteps timesteps.long() # Add noise to the latents according to the noise magnitude at each timestep # (this is the forward diffusion process) noisy_latents noise_scheduler.add_noise(latents, noise, timesteps) # Get the text embedding for conditioning encoder_hidden_states text_encoder(batch[\"input_ids\"], return_dict False)[0] # Get the target for loss depending on the prediction type if args.prediction_type is not None: # set prediction_type of scheduler if defined noise_scheduler.register_to_config(prediction_type args.prediction_type) if noise_scheduler.config.prediction_type \"epsilon\": target noise elif noise_scheduler.config.prediction_type \"v_prediction\": target noise_scheduler.get_velocity(latents, noise, timesteps) else: raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\") # Predict the noise residual and compute loss model_pred unet(noisy_latents, timesteps, encoder_hidden_states, return_dict False)[0] if args.snr_gamma is None: loss F.mse_loss(model_pred.float(), target.float(), reduction \"mean\") else: # Compute loss weights as per Section 3.4 of https://arxiv.org/abs/2303.09556. # Since we predict the noise instead of x_0, the original formulation is slightly changed. # This is discussed in Section 4.2 of the same paper. snr compute_snr(noise_scheduler, timesteps) mse_loss_weights torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim 1).min( dim 1 )[0] if noise_scheduler.config.prediction_type \"epsilon\": mse_loss_weights mse_loss_weights / snr elif noise_scheduler.config.prediction_type \"v_prediction\": mse_loss_weights mse_loss_weights / (snr + 1) loss F.mse_loss(model_pred.float(), target.float(), reduction \"none\") loss loss.mean(dim list(range(1, len(loss.shape)))) * mse_loss_weights loss loss.mean() # Gather the losses across all processes for logging (if we use distributed training). avg_loss accelerator.gather(loss.repeat(args.train_batch_size)).mean() train_loss + avg_loss.item() / args.gradient_accumulation_steps # Backpropagate accelerator.backward(loss) if accelerator.sync_gradients: params_to_clip lora_layers accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm) optimizer.step() lr_scheduler.step() optimizer.zero_grad() # Checks if the accelerator has performed an optimization step behind the scenes if accelerator.sync_gradients: progress_bar.update(1) global_step + 1 accelerator.log({\"train_loss\": train_loss}, step global_step) train_loss 0.0 if global_step % args.checkpointing_steps 0: if accelerator.is_main_process: # _before_ saving state, check if this save would set us over the `checkpoints_total_limit` if args.checkpoints_total_limit is not None: checkpoints os.listdir(args.output_dir) checkpoints [d for d in checkpoints if d.startswith(\"checkpoint\")] checkpoints sorted(checkpoints, key lambda x: int(x.split(\" \")[1])) # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit 1` checkpoints if len(checkpoints) > args.checkpoints_total_limit: num_to_remove len(checkpoints) args.checkpoints_total_limit + 1 removing_checkpoints checkpoints[0:num_to_remove] logger.info( f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\" ) logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\") for removing_checkpoint in removing_checkpoints: removing_checkpoint os.path.join(args.output_dir, removing_checkpoint) shutil.rmtree(removing_checkpoint) save_path os.path.join(args.output_dir, f\"checkpoint {global_step}\") accelerator.save_state(save_path) unwrapped_unet unwrap_model(unet) unet_lora_state_dict convert_state_dict_to_diffusers( get_peft_model_state_dict(unwrapped_unet) ) StableDiffusionPipeline.save_lora_weights( save_directory save_path, unet_lora_layers unet_lora_state_dict, safe_serialization True, ) logger.info(f\"Saved state to {save_path}\") logs {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]} progress_bar.set_postfix(**logs) if global_step > args.max_train_steps: break if accelerator.is_main_process: if args.validation_prompt is not None and epoch % args.validation_epochs 0: # create pipeline pipeline DiffusionPipeline.from_pretrained( args.pretrained_model_name_or_path, unet unwrap_model(unet), revision args.revision, variant args.variant, torch_dtype weight_dtype, ) images log_validation(pipeline, args, accelerator, epoch) del pipeline torch.cuda.empty_cache() # Save the lora layers accelerator.wait_for_everyone() if accelerator.is_main_process: unet unet.to(torch.float32) unwrapped_unet unwrap_model(unet) unet_lora_state_dict convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet)) StableDiffusionPipeline.save_lora_weights( save_directory args.output_dir, unet_lora_layers unet_lora_state_dict, safe_serialization True, ) # Final inference # Load previous pipeline if args.validation_prompt is not None: pipeline DiffusionPipeline.from_pretrained( args.pretrained_model_name_or_path, revision args.revision, variant args.variant, torch_dtype weight_dtype, ) # load attention processors pipeline.load_lora_weights(args.output_dir) # run inference images log_validation(pipeline, args, accelerator, epoch, is_final_validation True) if args.push_to_hub: save_model_card( repo_id, images images, base_model args.pretrained_model_name_or_path, dataset_name args.dataset_name, repo_folder args.output_dir, ) upload_folder( repo_id repo_id, folder_path args.output_dir, commit_message \"End of training\", ignore_patterns [\"step_*\", \"epoch_*\"], ) accelerator.end_training() if __name__ \"__main__\": main() ```"},"/docs/projects/tinytodo/index.html":{"title":"tinytodo","content":"# tinytodo 找不到一个好用的待办软件，一怒之下怒了一下，自己搞一个！ ## 需求 序号功能描述 fun_1显示登陆窗口 fun_2输入邮箱登录 fun_3显示待办窗口，置于底层，并固定在右上角 fun_4点击＋按钮添加待办 fun_5输入代办内容，点击添加，添加待办 fun_6点击统计按钮，弹出统计窗口 fun_7 fun_8 fun_9 ## 流程 登陆 ```mermaid flowchart LR 1[\"click exe\"] 2[\"show login window\"] 3[\"get email\"] 4[\"request url to get todolist\"] 5[\"generate the todolist window\"] 6[\"show todolist window\"] 1 > 2 2 > 3 3 > 4 4 > 5 5 > 6 ``` 添加待办 ```mermaid flowchart LR 1[\"click +\"] 2[\"show add window\"] 3[\"get input\"] 4[\"add todolist\"] 5[\"sync todolist to url\"] 6[\"regenerate todolist window\"] 1 > 2 2 > 3 3 > 4 4 > 5 5 > 6 ``` ## c/s protocol client:SYNC?email <email> server: success:\"READY?DBSIZE <DBSIZE>\" client:\"READY RESV\" server:FILE<DBSIZE> server:\"FILEOK\" fail:\"FAIL\" client:UPLOAD?email <email> server: success:response<\"READY\"> client:\"READY?DBSIZE <DBSIZE>\" server:\"READY RESV\" client:FILE<DBSIZE> server\"FILEOK\" fail:response<\"FAIL\"> client:<file> server: success:response<\"FILEOK\"> fail:response<\"FAIL\"> ## server process ### sync + 1.检查是否存在邮箱记录 + 2.存在返回 + 3.不存在则返回FAIL ### upload + 1.检查是否存在邮箱记录 + 2.存在邮箱则查询对应的数据库名称，返回READY，并接受文件保存为查询到的名称 + 3.不存在则新建邮箱记录 + 4.新建邮箱记录首先获取最大值id + 5.提交数据库 ## exec file versiondownloadsource_code <! tinytodo v1.0[download here](./tinytodov1.0.zip)[download source](./sourcev1.0.zip) tinytodo v1.1[download here](./tinytodov1.1.zip) tinytodo v1.2[download here](./tinytodov1.2.zip) tinytodo v1.3[download here](./tinytodov1.3.7z)[download source](./sourcev1.3.7z) tinytodo v1.3[download here](./tinytodov1.4.7z)[download source](https://gitee.com/helloyutao/tinytodo.git) >"},"/docs/index.html":{"title":"Article title","content":" title: Article title keywords: keyword1, keyword2 desc: description for this article date: 2022 09 01 ## Add article * Create markdown file with file name end with `.md` in the directory of this file, e.g. `first.md` * Add link in `sidebar.yaml` ```markdown items: label: Brief file: README.md label: First file: first.md ``` ## More example More visit: [teedoc.neucrack.com](https://teedoc.neucrack.com/) or [teedoc.github.io](https://teedoc.github.io/) And more example see: [github.com/teedoc/teedoc.github.io](https://github.com/teedoc/teedoc.github.io) and [https://github.com/teedoc/template](https://github.com/teedoc/template) , and [sipeed wiki](https://github.com/sipeed/sipeed_wiki) ## 添加文章 * 在本文件所在目录创建 markdown 以 `.md` 结尾的文件，比如 `first.md` * 在 `sidebar.yaml` 中添加侧边栏链接 ```markdown items: label: Brief file: README.md label: First file: first.md ``` ## 更多例子 更多请访问: [teedoc.neucrack.com](https://teedoc.neucrack.com/) 或者 [teedoc.github.io](https://teedoc.github.io/) 更多例子访问: [github.com/teedoc/teedoc.github.io](https://github.com/teedoc/teedoc.github.io) 或者 [https://github.com/teedoc/template](https://github.com/teedoc/template) , 或 [sipeed wiki](https://github.com/sipeed/sipeed_wiki)"},"/docs/references/algorithms/ann/ann.html":{"title":"","content":"<script> MathJax { tex: { inlineMath: [['$', '$'], ['\\\\(', '\\\\)']] } }; </script> <script id \"MathJax script\" async src \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex chtml.js\"> </script> # ANN（Approximate Nearest Neighbor）近似最近邻算法 ANN是KNN的弱化版本 近似最近邻检索的核心思想：搜索可能是近邻的数据项而不再只局限于返回最可能的项目，在牺牲可接受范围内的精度的情况下提高检索效率。 ## 基于树的方法 KD树是其下的经典算法。一般而言，在空间维度比较低时，KD树的查找性能还是比较高效的；但当空间维度较高时，该方法会退化为暴力枚举，性能较差，这时一般会采用下面的哈希方法或者矢量量化方法。 k d树是一种空间划分树，说白了，就是把整个空间划分为特定的几个部分，然后在特定空间的部分内进行相关搜索操作。想一个三维(多维有点为难你的想象力了)空间，kd树按照一定的划分规则把这个三维空间划分了多个空间，经典的分治思想，类似归并排序（和八叉树<font color red>?</font>，老实说，这几种树，貌似都是平衡树的一种，将数据划分以减少计算量） 举一个多个数据点构造kd树的例子，划分结果如下图： <div style \"text align:center;\"><img src \"./3bd69691db3eacef35f65a6748fc52b1.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> **注意（7，2）为根节点，此时出现一个新点需要计算最近邻时，从根节点出发，位于根节点右边则进入（9，6）节点，避免了原始算法O（n）的计算复杂度，关于kd树的其他知识则是增删改，不再赘述。** ## 局部敏感哈希（LSH） 核心思想：在高维空间相邻的数据经过哈希函数的映射投影转化到低维空间后，他们落入同一个吊桶的概率很大而不相邻的数据映射到同一个吊桶的概率则很小。在检索时将欧式空间的距离计算转化到汉明（Hamming）空间，并将全局检索转化为对映射到同一个吊桶中的数据进行检索，从而提高了检索速度。这种方法的主要难点在于如何寻找适合的哈希函数。 **简而言之，假设我有n个点，我将这个n个点分散到m个桶内，分的方法使用hash函数，然后查询新点时，再通过同样的hash func计算属于哪个桶，将桶内数据取出进行线性匹配（挨个计算距离）。以上理论成立的前提是，通过该hash func，邻近的点真的能进入同一个桶，那么hash func的挑选就成了该算法的核心。** > 汉明空间即计算汉明距离，是比较两个长度相同的向量，若某个维度相等，则距离加一，汉明距离可以比较两个二进制串，a 11101010，b 11011010。a和b两个二进制串不同的位数为2，则汉明距离为2。相似度越高，距离越小。 <div style \"text align:center;\"><img src \"./v2 84f6c2b0c8caa9ac7b106484820204eb_1440w.jpg\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 对于传统的hash，上面的每个“桶”内只能装一个值避免冲突达到O（1）的复杂度，LSH则是将相邻的放在同一个桶。 ## 矢量量化 其代表是乘积量化（PQ）。它的主要思想是将特征向量进行正交分解，在分解后的低维正交子空间上进行量化，由于低维空间可以采用较小的码本进行编码，因此可以降低数据存储空间 。 **PQ**的简单概要：选取一个大向量，将其拆分为子向量，将每个子向量分配给其最接近的质心值，并将质心值替换为其唯一ID，生成一个较小的ID向量。 在该过程结束时，我们将高维向量（需要大量内存）简化为需要很少内存的ID向量。 假设向量长度 为为12。我们首先将此向量拆分为 个子向量，如下所示： <div style \"text align:center;\"><img src \"./QQ20241203 133938.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> PQ方法采用基于查找表的非对称距离计算(Asymmetric Distance Computation，ADC)快速求取特征向量之间的距离，在压缩比相同的情况下，与采用汉明距离的二值编码方法，采用ADC的PQ方法的检索精度更高。 **fine，fine，上面太复杂了，不过严谨的论述不能少，下面是不严谨的，先讲矢量量化，就是kmeans的结果有k个簇，例如256个簇，那么我可以用8bit表示完这256个簇，这个8bit所对应的值就是cluster_id，我们需要找簇时通过8bit来找，而不是原数据u，达到压缩数据的作用。** **PQ干嘛呢，就是在划分前先将d维向量划分成m份，在m份中单独做聚类，这样能并行化聚类的过程，m份中有k个簇，这k个簇两两之间计算距离，可以得到一个$m*k*k$的码本，这个后面查询的时候用** SDC <div style \"text align:center;\"><img src \"./v2 2e3233c422794f13cf275dc40e962f17_1440w.jpg\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> x是query向量，首先计算q（x），就是x的聚类中心，然后计算q（y），y是向量库里的向量（cluster id），所以问题被转化为计算聚类中心之间的距离，而计算聚类中心的距离可以直接查码本，也就是上面提到的。 ADC <div style \"text align:center;\"><img src \"./v2 470a71a9c06666c59102e325dcdd70b5_1440w.jpg\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ADC是非对称的，也就是x直接与q（y）计算距离，只有一个量化误差。但是显而易见，在线计算，没法查表，性能有所下降。 用以上两种算法去求query的近似数据。 上述方法在m份聚类组中都要进行，最后得到一个欧式距离的总和，就是x与y的距离。 ## 基于图索引的方法（文献总结） 和树很像，但不是一个东西，比如说a向量和b向量可以指向同一个相近向量，这一点在树结构中是不存在的。"},"/docs/references/algorithms/moe/index.html":{"title":"Mixture of Experts","content":"# Mixture of Experts 混合专家模型 Google Switch Transformer论文中的MoE结构： ![alt text](image.png) 把[transformer](../transformer/index.html)中的feed forward换成了多专家结构，如上图 每个专家可能是ffn或者是更复杂的结构。 这样扩大了整个网络中的参数量，同时各专家处理不同的子任务，最后将输出进行拼接。 决定输出输入到哪个专家的是门控网络，（可以粗浅的理解，也是个ffn）。 ## type sparse moe 和 dense moe， 前者选出top k个专家激活，后者激活所有专家，将所有token输入并乘以权重输出 ## problems 那么为什么这样的结构可以训练得到更好的效果？ 注意到不论是门控网络还是expert，其参数一开始都是随机的，那么一开始门控决定输入到哪个专家完全是随机的，但是其中涉及到一个“巧合”，就是数据输入到某个专家时，假设其参数更适合当前的token，那么他就能更快的拟合，得到更好的loss，那么门控网络的参数也将会被更新为更倾向于选择该expert的值，那么这样不断的拟合，不同的专家的参数适合不同的任务类型，当不同的任务被输入时，训练好的门控网络将会选择更适合当前任务的expert。 需要注意的是，为了避免某些“误会”，（不太好形容，所以用了很多比喻），就是当前expert明明更适合另一个子任务，但是由于输入数据相似，门控错误的认为这个expert更适合，所以在门控中加了些随机量，让门控给予其他专家尝试不同子任务的机会，这一点同蒙特卡洛搜索中相似，为了避免在某个子树中过度探索，会在计算奖励值时加入一些随机惩罚，让其他子树有机会被探索，避免错过最佳路径。"},"/docs/references/llm/deepspeed_llm/index.html":{"title":"","content":"## 专家并行加速 ![alt text](image.png) 这里应该是单纯的增加使用的卡数，对于moe来讲，可以将多个专家的参数放到一张卡上，由于token与参数的计算都是矩阵乘，可以将多个token的向量进行拼接，和多个专家同时拼成一张大矩阵乘来计算 但是不可避免地会增加单张卡地计算量，这里将16张卡上的专家拆到64张卡上，并行度提升是一方面，另一方面单张卡的计算量减少，性能自然提升，因此此处可能和所提到的内存空间更大没有太大的关系。 ## 双流并行 ![alt text](image 1.png) 这里双流并行可能是对于一个算子可以存在多个stream，只要stream的算子之间没有数据依赖，那就可以让两个stream互相争抢资源，以达到上面所说的计算与通信相互覆盖的效果。 ## autoPD ![alt text](image 2.png) 关于这个的资料只找到[pd分离特性](https://www.hiascend.com/doc_center/source/zh/mindie/10RC3/mindiellm/llmdev/mindie_llm0291.html) ![alt text](image 5.png) ## 多链路并发与软硬件协同通信算法 ![alt text](image 3.png) 这个上面是跨机通信的优化，原始的跨机通信需要按照roce网络通信的方式进行三次确认确保数据包送达，这里他们团队自己设计了通信原语，将数据与标志位一起传输，类似单机内的流水同步原语，使用waitflag和setflag的方式进行同步。（就是把多机也看做了多个部件，以流水并行的角度看待整个系统） ![alt text](image 4.png) 类似[fasstube](../../ann/faastube/faastube.html)这一篇，利用其他可达链路进行聚合通信，在这里也就是直接跨机通信，以及机内通信 >跨机通信这两条链路可以同时使用。 ## MLAPO [MLAPO](../MLAPO/index.html)"},"/docs/references/llm/flashAttention/index.html":{"title":"flash attention","content":"# flash attention ## method 使用算子融合的思想，将计算局限在SRAM中，例如QK计算中间结果不进行HBM的写回，而是通过分块的方式，继续同V矩阵的部分value进行计算，然后将这一部分的计算结果写回HBM，再进行下一块计算，这样就避免了SRAM频繁从HBM读取数据的问题。"},"/docs/references/llm/MLA/index.html":{"title":"MLA架构","content":"# MLA架构 ## MHA架构 标准多头注意力 ## MQA架构 ![alt text](image.png) 复用了每个token的KV，会影响表现 ## GQA架构 每两组token共享一组KV，本质是效果和性能的权衡 ## MLA架构 ![alt text](image 1.png) 对token先降维压缩，计算注意力时，通过升维矩阵进行解压 ![alt text](image 2.png) ![alt text](image 3.png) 这里的融合本质理解，首先通过矩阵乘法满足结合律可知，可以先计算$W^{Q}W^{UK^T}$,由于两个W权重矩阵都是未知量，因此类比于$(y ax_1 \\dot x_2) > (y ax_3)$,所以这里实际可以计算为一个未知矩阵。 但是以上的计算没有考虑RoPE ![alt text](image 4.png) 加上了旋转矩阵后以上就无法进行合并，因为每一个token的相对位置是变化的。 ![alt text](image 5.png) MLA架构为Q添加了一个$W^{QR}$矩阵，将token与该矩阵乘后的结果，在经过RoPE得到一个旋转向量，将该向量拼接到Q矩阵当中，从而添加了位置信息。 同样K也存在响应的R矩阵。 这样，将旋转位置编码与注意力计算解耦后，就可以实现矩阵融合的同时，利用到位置信息。"},"/docs/references/llm/MLAPO/index.html":{"title":"MLAPO","content":"# MLAPO ## 算子融合 ![alt text](image.png) 显然，融合算子可以避免和主机内存的频繁交换，提速是必然的， MLAPO中还将前向的13个算子融合成一个大算子，包括以下内容： ![alt text](image 1.png) 通过cube+vector并行的方式，并行计算一部分小算子，基本可以将向量运算进行覆盖。"},"/docs/references/llm/pageAttention/index.html":{"title":"page attention","content":"# page attention ## problem 生成了预定token数的KV cache，但是实际使用时仅能使用一部分的缓存。就是每一次的文本生成都预分配了一定的内存，但是不一定所有的内存都被使用了。 显存的碎片不足以预分配给下一个文本的生成。 ![alt text](image.png) 对于实际物理地址的索引是通过页表索引的，其主要作用是虚拟地址的分配是连续的，而物理地址可以是不连续的。当一个文本生成需要进行kv缓存时，向系统申请内存，系统会管理物理地址，拿到指定大小的内存，并构建映射。 以上操作的优势在于，物理地址的合并不影响虚拟地址的使用，以达到节省空间的功能。"},"/docs/references/llm/RoPE/index.html":{"title":"RoPE","content":"# RoPE ![alt text](image.png) ![alt text](image 2.png) ![alt text](image 3.png) ![alt text](image 1.png) 最后这个高维的旋转矩阵本质是高维空间的向量旋转，与二维空间的按角度旋转在数学性质上等价（线代学的太烂了，没去推）"},"/docs/tutorial/linux/dev_tree/index.html":{"title":"linux设备树","content":"# linux设备树 ## 问题 device与driver分离 前者作为硬件信息的描述，后者作为驱动代码，都会被编译为ko文件进入内核，但是硬件种类增多将会产生大量关于硬件描述的信息，但是不是所有的都用得上，出现大量垃圾代码。 ## 解决 用设备树的方式，实际上也就是将device的代码分离出来，不再进入内核，通过bootloader传给内核。类似反射的工作方式。 ## 基础"},"/docs/tutorial/linux/frp/frp.html":{"title":"","content":"> # frp穿透 >借助sakura frp建立内网穿透 ## 内网穿透浅析 现在假设我们在电脑上建立http server进程，该进程绑定到2345端口。 那么我们如果想让外部设备访问该服务，要么将电脑置于公网上，也就是获取一个公网ip。 要么借助中间人，让外部设备通过中间人访问，也就是内网穿透技术，我更愿意将之称为代理（proxy）的一种。 原理如下： <div style \"text align:center;\"><img src \"v2 5b95727dc00913e1675b981f5adcb72b_1440w.jpg\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 显而易见，当外部设备访问外网服务器的ip:port，例如101.70.105.150:4598时，该服务器上运行的frps程序将接受你所有的请求，并将该请求转发给你的局域网机器（公网机器能给局域网转发是因为局域网机器主动与公网机器建立了长连接），局域网机器获取请求返回响应，公网机器将响应返回给请求者。 ## sakura frp ### 建立通道 进入[sakura](https://www.natfrp.com/user/)官网,注册并认证。 点击`服务` >`隧道列表` >`创建隧道` <div style \"text align:center;\"><img src \"QQ20241213 002213.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 到这一步所看见的列表就是能选择的公网机器列表。 <div style \"text align:center;\"><img src \"QQ20241213 002331.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 一般选tcp或者udp协议，前者更常用（建网站），后者主要用于游戏服务器的搭建。 <div style \"text align:center;\"><img src \"QQ20241213 002506.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 最重要的就是本地端口的选择，也就是填你在局域网机器建立的服务绑定的端口，以这里的例子，也就是2345。然后点击完成。 点击`服务` >`软件下载` >`frpc` >选择架构 >`下载` 将下载到的二进制文件放到我们建立服务的局域网服务器。 <div style \"text align:center;\"><img src \"QQ20241213 003001.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 在以上图中复制参数，并在局域网服务器上运行 `./frpc f 你复制的参数` 当打印出公网ip，那么穿透成功。 ## 域名上网 ### 申请域名 ### 域名备案 ### ssl证书 1,华为云买一个免费的，选最后一个类型，供应商选di 2，添加记录集，类型TXT验证 3，将证书下载到frp工作目录，frp的证书放在哪里，你就把新证书放在哪里，证书的前缀由域名决定。 ![alt text](image.png) 4，frp的工作目录由执行路径决定，或者由service的workingDirectory决定： ![alt text](image 1.png) 5，完成更换后直接重启frp服务即可"},"/docs/tutorial/linux/service/service.html":{"title":"linux service建立","content":"# linux service建立 ## 创建service文件 <div style \"text align:center;\"><img src \"QQ20241213 003441.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 首先在`/etc/systemd/system/`目录下创建一个example.service文件，文件中填入上面的内容。 需要更改的部分： + 如果是不依赖网络的服务，那么可以将Requires那一行去掉 + ExecStart改为你需要执行的命令 + User代表你以什么用户运行该进程，如果需要高权限，可以改为root + WorkingDirectory代表你的运行该程序时将会进入哪个目录。 + StandardOutput代表进程的标准输出重定向到哪个文件 + StandardError代表标准错误输出重定向到哪个文件 ## 启动service 重新加载所有的service，也包括我们新加入的service文件。 `sudo systemctl daemon reload` 启动服务 `sudo systemctl start example.service` 查看服务状态 `sudo systemctl status example.service` 如果出现activating，那么启动成功，此时该进程将被加入守护进程与开机自启。 ## 关闭服务 `sudo systemctl stop example.service` ## 删除服务 `sudo rm /etc/systemd/system/example.service` `sudo systemctl stop example.service`"},"/docs/tutorial/linux/user_manage/index.html":{"title":"linux 用户管理","content":"# linux 用户管理 ## 用户权限 777权限不再赘述 ## 用户组 当一个用户a被添加到用户组g后，如果一个文件的权限为747，即 rxwr rxw，这个文件的用户组为g，那么a用户对于该文件的操作权限只有读权限，而没有执行权限和写权限。 一个文件的所有者的权限第一个数字，其他用户权限是第三个数字。 ## sudo 当我们在sudoer文件中添加一个用户组时，该用户组内用户即可使用sudo命令 例如wheel用户组，使用`sudo visudo`命令，添加如下: `%wheel ALl (ALL) NOPASSWD: ALL` 上面配置的含义是，ALL (ALL)第一个ALL对于所有主机的所有连接，第二个ALL表示用户 username 在执行后续命令时，可以切换到任何其他用户的身份 NOPASSWD代表执行sudo的命令时无需密码 冒号后面是可以执行的命令，ALL代表任何命令，如果想要限制仅能执行部分命令，如下： `%wheel ALl (ALL) NOPASSWD: apt install *,apt update`"},"/docs/tutorial/llm/vllm/index.html":{"title":"","content":""},"/docs/tutorial/python/pyside6/index.html":{"title":"pyside6 开发","content":"# pyside6 开发 ## 安装 `pip install pyside6` ## 打开pyside designer `pyside6 designer` ## ui转python代码 `pyside6 uic untitled.ui > widget.py` ## 实例化ui ```python class MainWindow(QMainWindow): def __init__(self): super(MainWindow, self).__init__() # 创建 UI 实例并设置到当前窗口 self.ui Ui_MainWindow()#这个Ui_MainWindow就是我们创建的ui self.ui.setupUi(self) # 添加自定义逻辑 self.ui.pushButton.clicked.connect(self.on_button_clicked) def on_button_clicked(self): print(\"按钮被点击了！\") if __name__ \"__main__\": app QApplication(sys.argv) mainWindow MainWindow() mainWindow.show() app.exec() ``` ## 创建托盘(创建菜单) ```python # 创建系统托盘图标 self.tray_icon QSystemTrayIcon(self) self.tray_icon.setIcon(QIcon(\"icon.png\")) # 设置托盘图标，替换为你的图标路径 # 创建托盘菜单 self.tray_menu QMenu() # 添加显示窗口选项 show_action QAction(\"Show Window\", self) show_action.triggered.connect(self.show_window) self.tray_menu.addAction(show_action) # 添加退出选项 exit_action QAction(\"Exit\", self) exit_action.triggered.connect(self.close_application) self.tray_menu.addAction(exit_action) # 将菜单绑定到托盘图标 self.tray_icon.setContextMenu(self.tray_menu) # 显示托盘图标 self.tray_icon.show() ``` ## 窗口属性 + 窗口置底 ```python self.setWindowFlags(Qt.WindowStaysOnBottomHint) ``` + 窗口无边框化 ```python self.setWindowFlags(Qt.FramelessWindowHint) ``` + 隐藏任务栏图标 ```python self.setWindowFlags(Qt.Tool) ``` ## 动态插入子部件 ```python new_todo Todo(self) new_todo.setText(text) self.ui.lineEdit.clear() self.ui.scrollVLayout.insertWidget(1,new_todo) ``` ## 背景窗口圆角 ```python def paintEvent(self, event): # 创建一个 QPainter 对象并指定绘制设备为当前窗口 painter QPainter(self) # 设置绘制选项为反锯齿，使绘制的图形边缘更加平滑 painter.setRenderHint(QPainter.RenderHint.Antialiasing) # 设置画刷颜色为白色 painter.setBrush(QBrush(QColor(255, 255, 255))) # 设置画笔颜色为透明，即不绘制边框线 painter.setPen(Qt.GlobalColor.transparent) # 获取当前窗口的矩形区域 rect self.rect() # 绘制圆角矩形，圆角半径为 15 painter.drawRoundedRect(rect, 15, 15) \"\"\" 通过遮罩设置窗口为圆角矩形 \"\"\" path QPainterPath() rect QRect(0, 0, self.width(), self.height()) radius 15 # 圆角半径 path.addRoundedRect(rect, radius, radius) # 使用 QRegion 进行裁剪 region QRegion(path.toFillPolygon().toPolygon()) self.setMask(region) ``` 不设置遮罩将会出现四个黑角，反正通用框架基于系统的窗口管理器必定出现许多问题 ## QCharts 太杂了，不太好说 ## QSS ```css QPushButton { background color: #4CAF50; /* 设置按钮背景颜色 */ color: white; /* 设置按钮文本颜色 */ border: none; padding: 5px 10px; } QPushButton:hover { background color: #45a049; /* 设置按钮悬停时的背景颜色 */ } ``` ## 弹出输入框 ```python # 创建一个 QInputDialog input_dialog QInputDialog(self) input_dialog.setWindowTitle(\"修改文本\") input_dialog.setLabelText(\"请输入新的文本：\") input_dialog.setTextValue(self.text()) # 设置默认值为当前文本 # 显示对话框并获取用户输入 if input_dialog.exec() QInputDialog.Accepted: new_text input_dialog.textValue() if new_text: self.setText(new_text) ``` ## 本文设置删除线 ```python def setStrike(self,ifstrikeout:bool): font self.font() font.setStrikeOut(ifstrikeout) # 移除删除线 self.setFont(font) ``` ## 将窗口嵌入桌面 ```python def set_desktop_window(self): # 获取桌面窗口的句柄 hwnd ctypes.windll.user32.FindWindowW(\"Progman\", \"Program Manager\") if hwnd: # print(\"setdesktopparent\") # 获取当前窗口的句柄 win_hwnd self.window.winId() # 将当前窗口设置为桌面窗口的子窗口 ctypes.windll.user32.SetParent(win_hwnd, hwnd) ``` 需要注意的是，嵌入桌面后，如果使用setStyleSheet(\"background: transparent;\")，背景将会变为纯黑（windows，预估是windows窗口管理器搞的鬼） 如果setAttribute(Qt.WA_TranslucentBackground),窗口上的空间将不再发生变化（难蚌） 解决！！！ ```python 查找到program manager窗口的句柄 progman_hwnd ctypes.windll.user32.FindWindowW(\"Progman\", \"Program Manager\") if not progman_hwnd: print(\"Failed to find Progman window\") return # 2. 发送 0x52C 消息，触发桌面窗口重新排列 ctypes.windll.user32.SendMessageW(progman_hwnd, 0x52C, 0, 0) # 3. 查找 WorkerW 窗口 self.workerw_hwnd ctypes.c_int(0) def enum_windows_callback(hwnd, lParam): # 查找 WorkerW 窗口 if ctypes.windll.user32.FindWindowExW(hwnd, None, \"SHELLDLL_DefView\", None): # self.workerw_hwnd ctypes.windll.user32.FindWindowExW(None, hwnd, \"WorkerW\", None) self.workerw_hwnd hwnd return True # 将 Python 回调函数转换为 C 回调函数 # 定义回调函数类型 WNDENUMPROC ctypes.WINFUNCTYPE(ctypes.c_bool, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int)) enum_windows_proc WNDENUMPROC(enum_windows_callback) # 枚举所有窗口 ctypes.windll.user32.EnumWindows(enum_windows_proc, 0) if not self.workerw_hwnd: print(\"Failed to find WorkerW window\") return # 4. 将当前窗口设置为 WorkerW 的子窗口 win_hwnd self.window.winId() ctypes.windll.user32.SetParent(win_hwnd, self.workerw_hwnd) # 5. 确保窗口可见 ctypes.windll.user32.ShowWindow(win_hwnd, 1) # SW_SHOWNORMAL ``` 最根本的问题在于，有部分设备的program manager与worker是分离的，其中一个worker是管理桌面图标的，另一个worker是透明背景，而program manager是桌面总窗口。 如下的spy视图 ![alt text](image.png) 我们之前是将我们的窗口加入program manager，这会造成worker的层级在我们窗口之上，我们的就被挡住了，现在我们先向program manager发送0x52c将program manager拆出workerw，此时将我们的窗口作为workerw的子窗口，注意，需要时包含shelldlldelview的workerw，否则也会被遮挡。如果想做桌面壁纸，那就将窗口作为另一个worker的子窗口。添柴！"},"/docs/tutorial/index.html":{"title":"some tutorials about the configure","content":"# some tutorials about the configure some tutorials about the configure > **click the sidebar to open markdown file** >! [read from the first file](./blkrv/fltk/fltk.html)"},"/docs/tutorial/somethingInteresting/officeAi/index.html":{"title":"officeAi 插件","content":"# officeAi 插件 ## src [download here](./OfficeAI.exe)"},"/docs/tutorial/ascend/problems/index.html":{"title":"910B1相关问题","content":"# 910B1相关问题 ## 环境 910A环境变量： ![alt text](image 2.png) 910B1环境变量： ![alt text](image 3.png) ## 分支代码 代码位置 + 910A /mnt/nvme0/pengyt/projects/matrix/optimize/ascendc + 910B1 /mnt/nvme0/pengyt/project/ascendc 执行命令： `./batch_run.sh batch_config` ### master分支 实现简单的分块矩阵乘 + 按行列组为aicore分配计算任务 + 组内再切分以外积的方式进行计算 + 计算结果在CO1累加，最后经CO2搬出到GM 如下图： ![alt text](image.png) 910A执行结果： ![alt text](image 1.png) 910B1执行结果： ![alt text](image 7.png) 分析： 注释掉代码主逻辑，进行一次分配与释放， ![alt text](image 11.png) 出现如下错误： ![alt text](image 12.png) 注释掉所有逻辑： ![alt text](image 14.png) 发现调用kernel就在报错： ![alt text](image 15.png) ### smaller_tile分支 提高第二次切分的细粒度，能够充分利用L0缓存，而不必受限于第一次切分的矩阵形状 + 按行列组为aicore分配计算任务 + 组内再切分以外积/内积的方式进行计算 + 计算结果在CO1累加，最后经CO2搬出到GM 如下图： ![alt text](image 4.png) 910A执行结果： ![alt text](image 5.png) 910B1执行结果： ![alt text](image 6.png) ### cvAgg_dataTransInhost分支 + 按行列组为aicore分配计算任务 + cube内按照master方式进行计算 + vec的任务分配由eq_core_num参数决定，计算公式为 $$task \\frac{group\\_sum}{block\\_num + eq\\_core\\_num} \\times eq\\_core\\_num$$ 如下图： ![alt text](image 8.png) 910A执行结果： ![alt text](image 9.png) 910B1执行结果： ![alt text](image 10.png)"},"/docs/tutorial/blkrv/quickstart/quickstart.html":{"title":"快速开始","content":"# 快速开始 ## docker（推荐） 非root用户以下所有指令都应在指令前添加sudo赋权。 + 拉取镜像 `docker pull docker.1ms.run/cnameless/blkrv` + 运行镜像 `docker run itd p 59066:22 p 59067:8000 name blkrv docker.1ms.run/cnameless/blkrv:1.0 /bin/bash` 此时宿主机的59066端口映射到容器的22端口，用于ssh连接，59067映射到8000端口用于访问可视化服务 + 安装并启动ssh服务 `docker exec blkrv apt install y openssh server && docker exec blkrv service ssh start` + 连接镜像 [windterm（推荐）](https://github.com/kingToolbox/WindTerm/releases) 如果在windows上使用虚拟机，那么此时需要使虚拟机作为代理服务器连接docker，代理服务器配置如下： ![alt text](image.png) 主机ip为虚拟机地址，跳转服务器的ip为docker的ip地址。 虚拟机直接进入docker `docker exec it blkrv /bin/bash` vscode 配置ssh连接配置如下： ```config Host j2docker HostName [docker镜像ip] User root Port [docker镜像ssh端口] ProxyJump user@[虚拟机ip]:[虚拟机ssh端口] ``` docker镜像ip获取： `docker exec blkrv apt install y net tools && docker exec blkrv ifconfig` ## 手动安装（推荐使用docker的方式，节省时间） [verilator编译教程](../tools/verilator/verilator.html) [riscv编译链编译教程](../tools/riscv elf unknown gcc/gcc.html) [fltk安装教程（可选）](../tools/fltk/fltk.html) ## 编译＋执行 + 递归克隆项目 `git clone recursive https://gitee.com/helloyutao/blkrv.git` + 编译运行项目 cli `make` 带图形化界面（依赖flkt库） `make ENABLE_GPU 1` 需要注意该编译方式标准输入将会被重定向到图形化窗口，输入将不被命令行接收。"},"/docs/tutorial/blkrv/index.html":{"title":"BLKRv","content":" title: BLKRv <html> <head> <meta charset \"utf 8\"> <style> .expandable list { width: 300px; margin: 20px; font family: Arial, sans serif; } details { border: 1px solid #ddd; border radius: 5px; margin bottom: 10px; padding: 10px; background color: #f9f9f9; } summary { font weight: bold; cursor: pointer; padding: 8px; list style: none; position: relative; /* 为图标定位提供参考 */ } summary:: webkit details marker { display: none; } summary:after { content: \"▼\"; position: absolute; right: 10px; font size: 12px; transition: transform 0.3s ease; /* 添加旋转动画 */ } details:hover { background color: #f5f8ff; /* 浅蓝色背景 */ border color: #c2d6ff; /* 边框高亮 */ box shadow: 0 4px 12px rgba(0,0,0,0.1); /* 立体阴影 */ transition: all 0.3s ease; /* 平滑过渡 */ } summary:hover { color: #1a73e8; /* 文字变蓝 */ background color: #e6eeff; /* 标题背景渐变 */ } a.link { display: block; /* 设置为块级元素[1,2](@ref) */ padding: 12px 15px; margin: 8px 0; background color: #fff; border radius: 4px; color: #1a73e8; text decoration: none; transition: all 0.3s ease; border: 1px solid #eaeaea; box shadow: 0 2px 4px rgba(0,0,0,0.05); } a.link:hover { background color: #f5f8ff; /* 悬停背景色[5](@ref) */ transform: translateY( 2px); box shadow: 0 4px 8px rgba(0,0,0,0.1); border color: #c2d6ff; } a.link:active { background color: #e6eeff; /* 点击效果[4](@ref) */ transform: translateY(0); } </style> </head> <body> <div class \"expandable list\"> <details> <summary>blkrv</summary> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\quickstart\\quickstart.html\">quickstart</a> <details> <summary>进阶</summary> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\进阶\\寄存器\\README.html\">寄存器</a> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\进阶\\外设\\README.html\">外设</a> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\进阶\\MMU\\README.html\">MMU</a> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\进阶\\RIB BUS\\README.html\">RIB BUS</a> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\进阶\\wrapper\\README.html\">Wrapper</a> </details> <details> <summary>tools</summary> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\tools\\fltk\\fltk.html\">fltk</a> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\tools\\mingw\\mingw.html\">mingw</a> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\tools\\riscv elf unknown gcc\\gcc.html\">riscv交叉编译链</a> <a class \"link\" href \"\\docs\\tutorial\\blkrv\\tools\\verilator\\verilator.html\">second Tier memory</a> </details> </details> </details> </div> </body> </html>"},"/docs/tutorial/blkrv/tools/fltk/fltk.html":{"title":"fltk","content":"# fltk ## ubuntu + 官网下载源码 https://www.fltk.org/software.php + 解压文件 `tar xvf fltk .tar.gz` + 编译 `./configure enable debug enable shared enable threads enable xdbe enable xft` + 安装 `sudo make install` ## windows ### 安装mingw64 [mingw教程](../mingw/mingw.html) ### 安装cmake 略~ ### 安装fltk + 1，下载源码 + 2，启动cmake gui，也可以直接CLI configure，配置选择源码路径以及make生成路径 <div style \"text align:center;\"><img src \"./QQ20241204 180724.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 3，点configure配置mingw编译器，只用gcc和g++，配置安装路径 + 4，点generate生成makefile + 5，进入make生成路径文件夹，执行mingw32 make install ### 使用 示例代码test.cpp ```c++ #include <FL/Fl.H> #include <FL/Fl_Window.H> #include <FL/Fl_Box.H> int main(int argc, char **argv) { Fl_Window *window new Fl_Window(300,180); Fl_Box *box new Fl_Box(20,40,260,100,\"Hello, World!\"); box >box(FL_UP_BOX); box >labelsize(36); box >labelfont(FL_BOLD+FL_ITALIC); box >labeltype(FL_SHADOW_LABEL); window >end(); window >show(argc, argv); return Fl::run(); } ``` makefile文件 ```makefile all:test.cpp \tg++ test.cpp o fltk static \\ \t LC:\\myfile\\softwares\\fltk\\installed\\lib \\ \t lfltk \\ \t lole32 luuid lgdiplus lcomdlg32 lwinspool lgdi32 lws2_32 lComctl32 \\ \t IC:\\myfile\\softwares\\fltk\\installed\\include ``` 后边儿一大堆库是mingw带的系统库，写一般程序用不到，gui库是这样的，乐。 **注意到那个Comctl32，没有的话会报错，报的错问gpt会让你添加user32，没卵用，那个库是在微软官网找到的。** 执行`mingw32 make`编译得到fltk可执行文件，执行效果如下： <div style \"text align:center;\"><img src \"./QQ20241204 180826.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ### 在blkrv中使用 blkrv仅支持在linux上运行，fltk用于模拟显示器，因此，如果需要显示图形化内容，请使用`make clean && make ENABLE_GPU 1`以开启图形化显示。"},"/docs/tutorial/blkrv/tools/mingw/mingw.html":{"title":"mingw编译","content":"# mingw编译 ## 下载mingw 下载mingw预编译版本（联系作者获取预编译文件） 将mingw的bin目录加到path环境目录，很多小朋友的os换win11，这里就不给配置环境变量的教程了。 加到环境变量后，win+r输入cmd再按enter，控制台输入gcc v，有输出说明配置成功。"},"/docs/tutorial/blkrv/tools/riscv-elf-unknown-gcc/gcc.html":{"title":"riscv交叉编译链","content":"# riscv交叉编译链 ## 依赖安装 顺序执行以下： [download gmp](https://gmplib.org/) ```bash tar xvjf gmp 6.2.1.tar.bz2 cd gmp 6.2.1/ ./configure make make install ``` [download mpfr](https://ftp.gnu.org/gnu/mpfr/) ```bash tar xvjf mpfr 4.1.0.tar.bz2 cd mpfr 4.1.0/ ./configure make make install ``` [download mpc](https://www.multiprecision.org/mpc/download.html) ```bash tar xvzf mpc 1.2.1.tar.gz cd mpc 1.2.1/ ./configure make make install ``` [download gettext](https://mirrors.aliyun.com/gnu/gettext/) ```bash tar zxvf gettext.tar.gz cd gettext ./configure make make install ``` ## 安装 以下两种方式 + 获取预编译版本 1,通过网盘分享的文件：riscv32.tar.gz 链接: https://pan.baidu.com/s/1pf0ikqstdLKcauD90w3KeQ?pwd b3vt 提取码: b3vt 2,`tar zxvf riscv32.tar.gz` + 手动编译 1,`git clone recursive https://github.com/riscv/riscv gnu toolchain` fine,你可以选择不递归下载，后续编译会自动下载相关内容。 2,`cd riscv gnu toolchain` 3,`./configure prefix /opt/riscv32 with arch rv32i with abi ilp32` 上面prefix代表编译完后的文件存放路径， with arch rv32i代表支持riscv 32位 整数指令集，abi是调用规范，使用ilp32，不支持浮点。 4,`make` ### 添加环境变量 1,`vim ~/.bashrc` + 采用手动编译的，末尾增加`export PATH /opt/riscv32/bin:$PATH$`，也就是上面的prefix。 + 采用预编译包的，末尾添加`export PATH /path/to/riscv/bin:$PATH$`。/path/to/riscv/也就是riscv文件夹路径 2,按ESC键，输入:wq退出vim编辑器，冒号别少了。 3,`source ~/.bashrc` 4,运行`riscv32 unknown elf gcc version`查看是否有输出"},"/docs/tutorial/blkrv/tools/verilator/verilator.html":{"title":"verilator安装","content":"# verilator安装 ## linux 1,`git clone https://github.com/verilator/verilator` 2,`sudo apt install autoconf` 3,`autoconf` 4,`cd verilator && ./configure` 5,`make j n`n为主机的核数 6,`sudo make install` 7,`verilator`测试是否有输出"},"/docs/tutorial/blkrv/进阶/MMU/index.html":{"title":"MMU","content":"# MMU ## 工作流程 + 启动 当satp寄存器值低12位不为零 + 工作 1，检查tlb快表 2，如果命中则直接返回物理地址。 3，如果未命中则经过如下流程进行虚拟地址转换 ![alt text](image.png) + 未命中 此处功能可扩展 当前对于未命中的处理为： 1，如果tlb一级快表已满，则删除快表第一项，新表项插入末尾 2，如果未满，则插入末尾"},"/docs/tutorial/blkrv/进阶/RIB-BUS/index.html":{"title":"BUS","content":"# BUS 在blkrv中，rib总线仅具备地址分发与数据传输的作用。即区分设备以及作为数据的中间代理。 ## 地址分发 在rib中规定了各设备的地址空间 如果需要进行扩展，即设备的注册，可以通过csr寄存器结合内存的方式，进行热插拔等扩展。 rib中的分发函数如下： ```c++ #define DISPATCH(n,dev) do { \\ this >s##n##_req 1; \\ this >s##n##_addr addr dev##_start_addr; \\ this >s##n##_write_data top >write_data; \\ this >s##n##_we top >we; \\ this >s##n##_mem_op_type top >mem_op_type; \\ } while(0) ``` n为设备端口号，dev为设备名 当判断地址属于某一个设备时，将会调用上面的函数进行地址分发，并将相应的数据传输给相应的设备。 ## 数据传输 数据的正向传输被包含在地址分发当中，数据从设备的响应如下： ```c++ #define SET_INT(n) top >int_port##n this >int_port##n #define SET_READ_DATA(n) if(REQ(n)){top >read_data READ_DATA(n);} ``` 在devices处理完数据后，需要调用setflag将响应的数据或者信号（如中断信号）响应给cpu。"},"/docs/tutorial/blkrv/进阶/wrapper/index.html":{"title":"wrapper","content":"# wrapper 该模块用于启动cpu，也是启动整个系统的运行 wrapper的逻辑代码位于blkrv/rtl/csrc/main.cpp ## verilator + 初始化上下文 ```c++ VerilatedContext* contextp new VerilatedContext; contextp >commandArgs(argc, argv); Vtop* top new Vtop{contextp}; VerilatedVcdC* tfp new VerilatedVcdC; contextp >traceEverOn(true); ``` + 打开trace跟踪 ```c++ top >trace(tfp, 0); tfp >open(\"wave.vcd\"); ``` + 模拟 ```c++ top >eval() ``` ## devices（外设） + 初始化设备 ```c++ extern devices my_devices;//已在devices.cpp文件中声明为全局变量 ``` + 初始化监测器 ```c++ monitor* my_monitor; if(argc> 2)//传入性能数据存储文件路径 my_monitor new monitor(1<<28,argv[1]); else my_monitor new monitor(1<<28,\"../data.txt\"); ``` + 片上系统工作流 ```c++ for(;;i++){//死循环 top >clk 0;//时钟在0，1间来回切换 top >eval();//模拟 my_rib.dispatch(top,my_mmu.convert(top,&my_devices));//mmu转换虚拟地址后，将物理地址传入rib总线，决定传入指定的设备 my_monitor >process(&my_rib,&my_mmu,main_time);//监测器采集相关数据。 if(my_devices.process(&my_rib,i)){//外设接受信号 break;//如果外设返回1，则代表出现关机信号。 } my_rib.set_flag(top);//将外设的信号传回，由rib传给cpu top >clk 1; top >eval(); } ``` ## 全部代码 + ```c++ #include<verilated.h> #include<verilated_vcd_c.h> #include \"Vtop.h\" #include \"devices.h\" #include \"mmu.h\" #include \"monitor.h\" extern vluint64_t main_time; extern devices my_devices; extern rib my_rib; extern mmu my_mmu; int main(int argc, char** argv, char** env) { std::cout<<\"\\033[3;1;31mstarting sim...\\033[0m\"<<std::endl; VerilatedContext* contextp new VerilatedContext; contextp >commandArgs(argc, argv); Vtop* top new Vtop{contextp}; VerilatedVcdC* tfp new VerilatedVcdC; contextp >traceEverOn(true); top >trace(tfp, 0); tfp >open(\"wave.vcd\"); std::cout<<\"start initializing devices...\"<<std::endl; std::cout<<\"$init monitor\"<<std::endl; monitor* my_monitor; if(argc> 2) my_monitor new monitor(1<<28,argv[1]); else my_monitor new monitor(1<<28,\"../data.txt\"); clock_t start,end; start clock(); top >clk 0; int i 0; for(;;i++){ top >clk 0; top >eval(); my_rib.dispatch(top,my_mmu.convert(top,&my_devices)); my_monitor >process(&my_rib,&my_mmu,main_time); if(my_devices.process(&my_rib,i)){ break; } my_rib.set_flag(top); top >clk 1; top >eval(); } end clock(); printf(\"ticktimes:%d,timecost:%f s\\ndevices shuting down...\\n\",i,((double)(end start))/CLOCKS_PER_SEC); delete top; tfp >close(); delete contextp; return 0; } ```"},"/docs/tutorial/blkrv/进阶/外设/index.html":{"title":"外设","content":"# 外设 ## 虚拟抽象 ![alt text](image.png) ## 协议 所有的外设均是经过简化的设备，对于现实设备不具备参考价值，我们自定义了大量的控制寄存器与数据寄存器等。 + bios 说明 pc寄存器初始为0，程序将从bios的0地址开始执行。 地址空间 0x00000000 0x00100000 + ram 说明 可扩展内存控制器，需要注意某些耦合设备实现，例如ram等。 地址空间 0x00100000 0x10100000 + keyboard 地址空间 0x10100000 0x10200000 driver 键盘按下时产生中断，0x10100000地址处将会存储三个字符缓存。 + screen（字符输出设备） 地址空间 0x10200000 0x20200000 driver 0x10200003写入1时触发输出，screen设备将会将0x10200004后的字符输出到宿主机终端。 + gpu 说明 gpu设备并不具备通用并行计算能力，但是可以扩展，绘图功能被限制，也可以扩展 该设备贴图时为了性能需求，将会直接从flash读取文件内容，这与文件系统强绑定。 地址空间 0x20200000 0x30200000 driver 0x20200000为控制寄存器地址 写入信号对应功能数据寄存器功能与地址偏移 1绘制文字4:x;8:y;12:r;16:g;20:b;24:font_size 2绘制三角形4:x0;8:y0;12:x1;16:y1;20:x2;24:y2;28:r;32:g;36:b 3刷新屏幕 4绘制jpg4:x;8:y;12:image_file_size;16:inode_id 5绘制png4:x;8:y;12:image_file_size;16:inode_id + nic 说明 该设备可以理解为对socket的简化封装，也可以自己实现完整的计算机网络体系结构内容得到更高的网络带宽。 nic设备采用dma，直接操作ram。 地址空间 0x30200000 0x40200000 drivers 0x30200000为控制寄存器地址 写入信号对应功能数据寄存器功能与地址偏移 1accept0 16:ip;20:port;24:protocol;28:return sockfd 2send4:sockfd;8:send_data_ram_addr;12:data_len 3recv4:sockfd;8:send_data_ram_addr;12:data_len 4connect0 16:ip;20:port;24:protocol;28:return sockfd 5close4:sockfd + flash 地址空间 0x40200000 0x60200000 + pmc 地址空间 0x60200000 0x60300000 driver 0x60200000为控制寄存器，向该地址写入任何信号都会触发关机。 + monitor 地址空间 0x60300000 0x60400000 drivers 0x60300000为控制寄存器地址 当向控制寄存器地址写入1时触发监测器的启动，此时开始进行性能数据的采集。 写入1后写入0，将会触发监测器的关闭，并将数据存储至宿主机。"},"/docs/tutorial/blkrv/进阶/寄存器/index.html":{"title":"CPU寄存器详解","content":"# CPU寄存器详解 ## 通用寄存器 寄存器别名全称说明 X0\tzero\t零寄存器\t可做源寄存器(rs)或目标寄存器(rd) X1\tra\t链接寄存器\t保存函数返回地址 X2\tsp\t栈指针寄存器\t指向栈的地址 X3\tgp\t全局寄存器\t用于链接器松弛优化（blkos中未启用） X4\ttp\t线程寄存器\t常用于在OS中保存指向进程控制块(task_struct)数据结构的指针（blkos中未启用） X5 ~ X7，X28 ~ X31\tt0 ~ t6\t临时寄存器\t X8\ts0/fp\t帧指针寄存器\t用于函数调用，被调用函数需保存数据 X9\ts1\t\t用于函数调用 ，被调用函数需要保存的数据 X10 ~ X17\ta0 ~ a7\t\t用于函数调用，传递参数和返回值 X18 ~ X27\ts2 ~ s11\t\t用于函数调用 ，被调用函数需要保存的数据 ## CSR寄存器 寄存器别名全称说明 c305mtvecMachine Trap Vector Base Address用于保存中断查询程序地址，需要在系统初始化时通过csr指令赋值 c341mepcMachine Exception Program Counter用于保存发生中断时，程序执行的pc，也就是中断地址 c342mcauseMachine Cause保存中断发生的原因，此处保存的是发生中断的端口 c300mstatusMachine Statusblkrv中该寄存器仅作为中断使能状态寄存器，低8位是否为1代表是否使能 c180satpSupervisor Address Translation and Protection Register保存页表基址 c181satp_s_cp发生系统调用时，保存satp副本 c182satp_i_cp发生中断调用时，保存satp副本 ## 流水寄存器 寄存器全称说明 pcProgram counter用于保存当前执行指令的地址 imm用于保存解析指令时产生的立即数 addr_vvirtual address用于保存LOAD/STORE指令等指向的地址 data2regs用于保存需要存储到通用寄存器的数据 data2mem用于保存需要存储到内存的数据 instinstruction用于保存读取的指令"},"/docs/tutorial/c++/cmake/index.html":{"title":"cmake","content":"# cmake ## 相关指令功能 ### set 功能：设置变量 用法： 1,一般变量 `set(normal \"normalValue\")` 2,缓存变量 ```cmake set(MY_VARIABLE \"Hello, World!\" CACHE STRING \"A greeting message\" FORCE) message(\"${MY_VARIABLE}\") # 输出Hello, World! set(MY_VARIABLE \"Good Morning!\" CACHE STRING \"A greeting message\") message(\"${MY_VARIABLE}\") # 输出Hello, World!因为不加FORCE默认不覆盖 set(MY_VARIABLE \"Good Morning!\" CACHE STRING \"A greeting message\" FORCE) message(\"${MY_VARIABLE}\") # 输出Good Morning! 加了FORCE强制覆盖 ``` 存储在CMakeCache当中，可以跨越多次编译，持久化存储。 3,环境变量 `set(ENV{<variable>} [<value>])` `unset(ENV{<variable>})` ```cmake # 设置一个名为 MY_VARIABLE 的环境变量，其值为 Hello World： set(ENV{MY_VARIABLE} \"Hello World\") ``` 可以用于在不同cmakelist之间传递变量，但是不可持久化储存 ### file 1,READ 读取文件（filename）的内容保存到变量（variable）中； file(READ config.cmake contents) 2,STRINGS 解析filename中的ASCII字符串列表，并将其存储在中。忽略文件中的二进制数据。忽略回车(\\r, CR)字符； ### execute_process 通过execute_process方法可以执行多个子进程。 命令COMMAND会并行执行，每个子进程的标准输出映射到下一个进程的标准输入上，所有进程共用standard error管道。 `execute_process(COMMAND rm rf ${ASCEND_AUTOGEN_PATH}/${CUSTOM_COMPILE_OPTIONS} COMMAND touch ${ASCEND_AUTOGEN_PATH}/${CUSTOM_COMPILE_OPTIONS})` ### function ```cmake function(<name> [<arg1> ...]) <commands> endfunction([<name>]) ``` ```cmake function(get_system_info SYSTEM_INFO) if (UNIX) execute_process(COMMAND grep i ^id /etc/os release OUTPUT_VARIABLE TEMP) elseif (WIN32) message(STATUS \"System is Windows. Only for pre build.\") else () message(FATAL_ERROR \"${CMAKE_SYSTEM_NAME} not support.\") endif () endfunction() ``` ### string 操作字符串 [具体教程](https://zhuanlan.zhihu.com/p/661283261) ### $<...> 1,检查变量 SOC_VERSION 的值是否在列表 CUSTOM_ASCEND310P_LIST 中 2,将上一步的结果强制转换为布尔类型（0 或 1） 3,如果条件为 True，则返回值 CUSTOM_ASCEND310P；否则不返回任何内容。 ```cmake ascendc_compile_definitions(ascendc_kernels_${RUN_MODE} PRIVATE $<$<BOOL:$<IN_LIST:${SOC_VERSION},${CUSTOM_ASCEND310P_LIST}>>:CUSTOM_ASCEND310P> ) ``` ### get_filename_component get_filename_component 函数用于获取文件名或目录的各个部分，如文件名、扩展名、目录等。 ```cmake get_filename_component(filename ${filepath} NAME) get_filename_component(directory ${filepath} DIRECTORY) message(STATUS \"File Name: ${filename}\") message(STATUS \"Directory: ${directory}\") ``` ### ExternalProject ExternalProject是CMake提供的另一个模块，用于在构建时下载、配置、构建和安装外部项目（即不是作为当前CMake项目一部分的项目）。 ExternalProject模块通过以下主要步骤实现其功能： 1. 下载源代码：在构建过程中下载外部项目的源代码。可以指定URL、Git仓库、SVN仓库等多种来源。 2. 配置项目：对下载的外部项目进行配置，包括设置构建选项和参数。 3. 构建项目：构建外部项目，生成需要的二进制文件或库。 4. 安装项目：将构建的项目安装到指定位置，以便于主项目使用。 基本步骤： 1. 引入ExternalProject模块： 在CMakeLists.txt文件中使用include(ExternalProject)命令来启用ExternalProject模块的功能。 2. 声明外部项目： 使用ExternalProject_Add函数声明一个外部项目，指定其源代码来源、配置选项、构建命令、安装位置等。 3. 配置依赖关系（可选）： 如果外部项目依赖于其他项目或特定的构建步骤，可以使用add_dependencies命令来配置这些依赖关系。 4. 使用安装的依赖： 一旦外部项目被构建和安装，其产物（如库、可执行文件等）就可以在主项目中被使用。 ### add_library 编译生成库文件 ### target_compile_options target_compile_options 指令用于为指定的目标添加编译器选项。通过这些选项，可以影响编译器如何处理目标的源代码。"},"/docs/tutorial/c++/fltk/index.html":{"title":"","content":"# fltk"},"/docs/tutorial/c++/makefile/index.html":{"title":"Makefile 教程","content":"# Makefile 教程 仅仅是零碎知识点 ## 变量命名 ```makefile var: variable $(var) ``` ## 获取指定文件夹所有文件"}}