{"/docs/references/ann/fusionANNS/fusionANNs.html":{"title":"FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for Billion-scale Approximate Nearest Neighbor Search","content":"# FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for Billion scale Approximate Nearest Neighbor Search ## RAG <div style \"text align:center;\"><img src \"./QQ20241203 152351.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 上图为RAG框架，首先将领域知识作为高维向量嵌入到向量数据库中。当聊天机器人接收到查询时，它使用ANNS引擎从向量数据库中检索最相关的知识，从而允许LLM将该知识用作额外的上下文以进行更准确的推理。*事实上，我认为该框架更符合AGI的发展方向，如今人工智能的发展过于依赖神经网络的万能拟合，而忽略了严谨知识，我们应该理解的是，先验知识被训练作为NN中的参数具有一定的不可控性，比较明显的证据就是大模型的幻觉。神经网络应该作为逻辑推理模块而非一个知识库（<font color red>?</font>）*。 > 提到的IVF（inverted file），是直接对所有的向量进行聚类，然后将查询向量同聚类中心计算距离，取最近的前topk个簇，然后遍历计算簇下面的向量。 文章认为IVF占用了大量的内存资源，同时PQ是一种有损压缩，准确率不高。而如果直接使用gpu加速基于IVF的SPANN，性能甚至比原来下降10%，根本原因是数据的大量迁移。 ## ANN 关于ANN的知识点详见[ANN](../../algorithms/ann/ann.html) ANN中有三种常见的方法，kd树，在论文中称为分层索引；乘积量化，结合矢量量化使用；局部敏感hash；论文中还提到了使用GPU加速技术，但将这些方法集和起来使用发现比微软的SPANN性能要差。 <div style \"text align:center;\"><img src \"./QQ20241204 200225.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> > *直观地说，可以将分层索引、乘积量化和GPU加速技术结合起来，以实现最佳的ANNS解决方案。然而，我们发现，这些技术的组合导致甚至比SPANN更差的性能，它只利用层次索引（第2.3节）。总的来说，在GPU加速的ANNS系统中，将分层索引与产品量化合作仍然存在一些挑战。* <div style \"text align:center;\"><img src \"./QQ20241204 194332.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 对于spann，其qps不随线程增加而增大，在4个线程就达到了最大值，且不再增大，也就说硬件资源的增加不影响算法的效率，从软硬协同的角度这显然阻碍了效率的提升。 文章探究了其原因，测量了每个ANNS查询平均所需的I/O数量以及跨SSD、主内存和GPU HBM传输的数据量。如图c所示，虽然PQ技术显著地将发布列表的I/O大小从12 ‐ 48 KB减少到了页面粒度（4 KB），但由于重新排序过程，它将I/O数量增加了70%。因此，I/O性能瓶颈从固态硬盘的带宽转移到每秒输入/输出操作数（IOPS）。此外，CPU和GPU之间会传输大量的发布列表，从而抵消了GPU加速带来的好处，如图d所示。 也就是数据传输的频率提高了，每个数据传输请求之间的转换是存在开销的。 <div style \"text align:center;\"><img src \"./QQ20241204 204808.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ## 存在的三个挑战 + 1，为了提高查询的准确性和效率，大多数ANNS系统采用复制策略来构建高质量的IVF索引，其中边界向量被复制到相邻的倒排列表中。这可以显着扩展索引的大小，比原始向量大8倍。即使这些索引使用PQ压缩，GPU的HBM仍然无法容纳所有压缩的索引，导致GPU和CPU之间的大量数据交换。 + 2，由于PQ会造成不小的准确性损失，因此通常将其与向量重排序过程相关联以提高查询准确性。然而，由于在不同的压缩向量之间准确度损失显著地变化，因此确定在给定的准确度约束下对于每个查询需要重新排序的向量的最小数目是具有挑战性的。 + 3，由于原始向量（128 〜 384字节）远小于现代NVMe固态硬盘的最小读取粒度（4 KB），因此每次请求原始向量都会导致读取放大，从而导致重新排序期间的I/O效率较低。(不是很能理解这个，按道理，即使是向量，在主存中也应是顺序存储，这与读取粒度的关系是什么<font color red>?</font>也就是说，虽然原始向量只有128字节，但是一次可以读取多个向量<font color red>?</font>受到影响的应该仅仅是最后向量的读取。) ## solution 先看fusionANNS的架构图(非常清晰的图，很值得学习)： <div style \"text align:center;\"><img src \"./QQ20241204 205436.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 离线部分略过，讲讲在线部分，当有一个查询向量时， + 1，gpu生成距离表用于PQ计算（<font color red>?</font>） + 2，cpu查询前m个最近的倒排列表，就是前m个距离最近的质心，因为涉及排序，所以放在cpu（<font color red>?</font>存疑），排序也可以放在gpu来着。 + 3，查询元数据，因为上面拿到的是质心，需要获取质心所属簇的向量用来计算距离。 + 4，cpu传输这些向量到gpu + 5，PU接收到向量ID时，它首先使用并行hash模块删除重复数据。（更像是相近的归为一组，仅保留一个数据） + 6，GPU从HBM中读取相应的压缩向量，并计算它与查询向量之间的PQ距离。就是求查询向量与PQ得到的码本中的距离之和。 + 7.取计算得到的前n个id。 + 8，cpu使用启发式重新排序机制（<font color red>?</font>） + 9，re ranking，返回前k个邻居，在这个阶段采用冗余感知i/o消除重复i/o。 看看详细设计 <div style \"text align:center;\"><img src \"./QQ20241204 230506.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 上图中可以看到，首先将原始的vector数据通过分层平衡聚类算法聚类成n个list，每个list中就是每个簇的向量数据，将这些簇构造为图，并将图和簇内向量复制到主存。（图索引的构建方法：该图是通过不断向空图添加新的向量来构建的。将向量添加为新顶点时，将创建新边以将此新添加的顶点与其top k(通常为)最近的相邻顶点连接起来。然后，其相邻顶点应更新其最近的邻居以限制最大边数。） 后文提到了一个提高聚类质量的方法，我们focus on efficiency，所以不做赘述。 到这里，丢弃掉中间发布列表，仅保留上图所示的主机内存中的内容，到这里该系统在通用服务器上能够支持十亿规模的ANN。 同时注意到图下方的PQ矢量被存储在HBM当中。将所有压缩向量固定在HBM，避免了cpu与gpu大量的数据交换。（到这里，存在的一个疑惑是，原本的系统是如何处理这些数据的<font color red>?</font>为何之前的系统没有这样做<font color red>?</font>） 由于内存中的索引仍然保留了边界向量复制机制的优点，FusionANNS可以有效地获得候选向量的所有ID，然后将这些向量ID（不包括向量的内容）发送到GPU进行距离计算。通过这种方式，FusionANNS还消除了CPU和GPU之间有限的PCIe带宽造成的性能瓶颈。 后文提到： > SSD上的原始矢量。与基于IVF的SPANN将所有发布列表存储在SSD上不同，FusionANNS只需要将原始向量存储在SSD上以进行重新排名。由于原始向量的体积几乎是倒排列表的8倍，FusionANNS可以显著降低存储消耗。对于每个查询，由于只有重新排序过程会产生少量I/O请求，因此FusionANNS还可以缓解并发查询的SSD I/O瓶颈。 我不是很能理解，我非常好奇原本的系统难道不是这么做的吗<font color red>?</font>或者是将所有的原始向量都加载到主存<font color red>?</font><font color red>?</font> ### 重排机制 该机制是用以提高PQ带来的准确率降低（<font color red>?</font>），将重排化为多个小批处理（batch）按顺序执行，这样可以达到一旦重排不再有利于准确率就立即停止重排，提升效率。 > 由于所有候选向量都以其距离升序排序，因此较早执行的小批量通常具有更高的可能性来识别属于最终前k个最近邻居的更多向量。一旦一个小批处理完成，我们利用一个轻量级的反馈控制模型来检查后续的小批处理是否有利于提高查询精度。 轻量级反馈控制模型依据以下公式：$$\\Delta \\frac{S_{n} (S_{n} \\bigcap S_{n 1})}{k}$$ $S_{n}$和$S_{n 1}$分别是第n批和第n 1批向量id集合。k表示需要得到的前k个向量，也就是维护的max heap的向量数。这个公式的意义也就是向量的变化率，如果变化率连续$\\beta$次小于阈值$\\varepsilon$，那就终止重排。（显然，公式计算时间可以忽略。 <div style \"text align:center;\"><img src \"./QQ20241204 233713.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 看看上述过程的伪码，仅做个参考，抽象层级太高了。总结一下就是，按批处理数据，在每个批次中更新大端堆，当一个批次更新完后，看看是否停止重排。 ### 冗余感知i/o接口 到这里，前面提到的SSD细粒度问题得到解决，那就是之所以会产生细粒度不匹配问题，也就是单个向量过小（> 128k），而SSD访存单位为4k的这个问题，是因为在上述过程中，每次访存只访问一个向量，见算法1. 因此对于上面的过程会面临大频率的i/o请求，带来效率降低。 > 虽然需要重新排序的向量是通过PQ距离获得的，但它们都与查询向量高度相似，这使得它们通常在空间上彼此接近。这种相似性提供了通过仔细组织SSD上的数据布局来减轻读取放大的机会。 > 具体来说，当离线创建内存中索引时,对于导航图中的每一个质心，我们使用桶来存储最接近质心的多个原始向量。我们注意到，在桶之间没有重复的向量。对于每个桶，如果它不与SSD页面对齐，我们使用最大 最小算法(<font color red>?</font>)基于未对齐部分的大小组合联合收割机桶，以最小化SSD页面上的可用空间。最后，我们将所有桶分组为单个文件并将其存储在SSD上，并使用内存中的表来维护向量和SSD页面之间的映射。 似乎以上方法仅仅是将每个簇分文件存储以达到并行访问的目的<font color red>?</font>但似乎没什么意义，因为每次查询仅访问单个向量，此处没办法采用并行策略。 以下是i/o重复数据删除机制： <div style \"text align:center;\"><img src \"./QQ20241204 235737.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 原文已经讲得十分清楚，这里贴一下原文： > 其中小批量0的任务是重新排序向量：V2、V4和V6。mini batch 1的任务是对向量V5、V8和V9重新排序。当执行mini batch 0时，它首先查询映射表以获得与所请求的向量相对应的SSD页面ID。由于V2和V6都存储在同一个SSD页面P0中，因此我们可以合并这两个I/O请求，只读取一个SSD页面以获得V2和V6。由于P0和P2不存在于DRAM缓冲区中，我们通过两个I/O请求直接将它们读取到DRAM缓冲区。在小批量1中，尽管V5、V8和V9被存储在不同的SSD页中，但是DRAM缓冲器已经包含P2，P2包括V5。因此，mini batch 1只需要通过两个I/O请求读取P1和P3。 其中提到的合并v2和v6作为一个i/o请求，但并未提到其实现方法，dram缓存和这篇论文貌似没啥关系<font color red>?</font>这是SOC中已经非常成熟的cache设计，不再赘述。"},"/docs/references/ann/faastube/faastube.html":{"title":"FaaSTube: Optimizing GPU-oriented Data Transfer for Serverless Computing","content":"# FaaSTube: Optimizing GPU oriented Data Transfer for Serverless Computing 推理应用程序通常将多个模型和操作缝合到一个工作流中。这个是本文的前提，因为存在这样的工作流，因此在任务进行的过程中存在大量的数据transfer，这会造成大量的时间开销。 <div style \"text align:center;\"><img src \"./workflows.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 无服务器推理工作流涉及各种类型的数据传递。除了典型的cFunc（cpu函数）到cFunc数据传递之外，还有主机到gFunc（gpu函数）（其中主机表示主机内存中的cFunc或I/O数据）和gFunc到gFunc数据传递。不幸的是，当前的无服务器系统依赖于面向主机的数据传递方法 <div style \"text align:center;\"><img src \"./QQ20241202 162700.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 如上图，对于gpu到gpu的数据传输，首先将数据复制到host，然后再复制到gpu，忽略了nvlink的存在；而对于host到gpu，则采用单个PCIe链路，而忽略了gpu间存在的PCIe链路。如下图，将需要传输的data拆分，分别传输（并行）。 <div style \"text align:center;\"><img src \"./QQ20241202 163326.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> faastube作为数据传输的透明管道，用户不必操心传输的问题，就是开了一个抽象层，用户直接用其抽象层提供的api进行数据传输。 总结存在的第一个问题就是，现有的工作都是单点传输，即传输数据的时候使用一条PCIe总线通路，而没有考虑复杂系统中多条PCIe通路的利用，以及提出相应算法来利用gpu间PCIe的拓扑关系最大化传输效率。 第二个问题是，现在长时间运行的如ML任务，现有的内存管理系统会占用大量的内存，且临时分配内存会带来数据传输延迟，因此提出一个内存池方案，用以缓解内存压力。 <div style \"text align:center;\"><img src \"./QQ20241202 165639.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ## 解决问题1 各种拓扑链接如上图，私以为，除开右上角的链接方式，其余部分不太能够利用到gpu间的传输链路。 >! 论文中提到，G1 G4可以通过G4 G6 G7加倍带宽（<font color red>?</font>），这个数据通路不是很符合逻辑（看不懂）。以及后面提到的，G3 G7通过G3 G2 G1 G7以及G3 G4 G6 G7提高带宽，这个可以理解。 FaaSTube提供统一的数据ID，以简化管理主机端存储（例如，共享主机内存，Redis服务器）和GPU端存储（例如，CUDA IPC句柄） 也就是说该系统将数据进行封装，类似于“页表”机制<font color red>?</font>为每个页维护一条信息，可以传递这个页的id以获取地址信息。在文中，也就是一个tube，如下图。与页表机制相同，采用多级页表映射机制，这是由于内存空间很大，如果要保证内存划分的细粒度，那就必须要建立多级映射以维护页表。 <div style \"text align:center;\"><img src \"./QQ20241202 192113.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 论文中还提到，数据的复制采用流水线的方式进行，但我并没有看到任何关于流水线的说明。 关于数据传输的带宽控制，论文中提到将全局带宽抽象成整体来分配，如下图，每一个函数的data transfer会被划分为块来进行传输。 <div style \"text align:center;\"><img src \"./QQ20241202 193111.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> PCIe bandwidth schedular中计算$$Rate_{least} data_{size}/(L_{slo} L_{infer})$$(最小传输需求)$$L_{infer}$$代表推理计算延迟。SLO代表服务等级目标（serverless的一些专业知识，不是很了解），然后注意到还有一个循环固定缓冲区，类似于双缓冲机制，提前分配两块固定大小的缓冲区，两块缓冲同时使用，一块用来缓存需要传输的数据，另一块用来传输数据，也类似流水线的方式，提高传输效率。 FaaSTube利用并行NVLink路径来增强非统一拓扑中的点对点数据传递。FaaSTube引入了一种竞争感知路径选择算法，可优化NVLink使用，同时最大限度地减少来自其他功能的带宽竞争（避免路径重复）。给定无服务器工作流，FaaSTube首先应用MAPA 中的放置策略将函数分配给GPU，并最大化函数之间的NVLink连接。在确定功能布局后，FaaSTube首先在无服务器工作流中包含的GPU之间分配直接连接路径。如果这些直接NVLinks已经被其他函数占用，FaaSTube将强制其他函数释放路径并重新规划其他路径。 <div style \"text align:center;\"><img src \"./QQ20241202 194506.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 上面那个挺难理解的，直接看他抽象到最上面的算法，1 7行就是搜索空闲路径，然后将空闲path放入paths列表。如果找不到空闲路径，且输入输出带宽没有耗尽，那就搜索忙碌的路径，将占用这条忙碌路径的函数和当前需要路径的函数比较，如果不平衡，且占用当前路径的函数能够切换到另一条路径，那就将该函数挪到另一条路径，当前函数占用该path，这个过程看上去似乎比较复杂，因此作者强调了该过程在实验中仅花费10微秒。 ## 解决问题2 1）在每个GPU上提供了一个自动伸缩的内存池，可以根据功能的实际需求弹性伸缩，2）基于请求队列智能地在主机和GPU内存之间迁移数据。 当函数工作负载和中间数据大小动态变化时，这种方法会导致内存占用高达我们实验中实际需求的4倍。虽然PyTorch允许手动回收内存池，但它会回收所有内存块，从而在未来的分配中引入开销。最近的工作，GMlake ，通过使用CUDA虚拟内存和统一的2MB内存块来减少内存池中的碎片，但它仍然缺乏弹性内存回收。此外，GMlake中每个块上昂贵的IPC操作在数据传递中引入了大量开销（在我们的实验中高达45 ms）。 原本keep alive策略根据每个函数的请求间隔定义函数停留时间，也就是该函数所分配的内存需要维持的时间，但是对于本文所解决的工作流的场景，每个函数的中间数据大小也在波动，因此该波动仍会引起以上提到的问题，因此引入了三个变量用来计算保留的内存大小，如下图： <div style \"text align:center;\"> <img src \"./QQ20241202 200556.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/> </div> f分别是请求间隔（$R_{window}$），中间数据大小($R_{size}$)，以及数据积累度($R_{con}$)，内存保留计算为 $Data_{size} R_{size} \\cdot R_{con}$ ,内存池的最大大小为 $MemPool_{size} \\sum_{func}Data_{size} \\cdot 1_{ {R_{window} \\bigcap t \\neq 0} }$。 <div style \"text align:center;\"> <img src \"./QQ20241202 201622.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/> </div> 如上图，对于函数a，其请求数据a1由于先入队，所以先出（LRU策略），但是b1需要用到a1，但此时a1已经被复制到主机，b1还需要重新加载数据而带来延迟，因此faastube优先将a2这种不再使用的数据复制到主机（如何知道是否使用<font color red>?</font>），并且清除不再需要的中间数据，在有足够内存时，将先前迁移出去的数据再迁移回来，以避开transfer的高峰。"},"/docs/references/moe/parallel_method/index.html":{"title":"moe parallelism","content":"# moe parallelism ## GShard [论文链接](https://arxiv.org/pdf/2006.16668) 论文中提到超过千台设备上的A naive graph representation将会成为计算的瓶颈。 ![alt text](image.png) 对模型不同层进行分区，通过设备间通讯的方式同步参数将会涉及到底层通信机制的改变，对于开发者是一个严重的负担。 GShard是一个XLA的编译器扩展，简单来说就是按照其api规范添加注释，这个编译器会自动将模型进行分区，然后并行化。 ![alt text](image 1.png) ### 多FFN跨设备 ![alt text](image 2.png) 如图，feed forward被替换为多个expert，当采用多设备计算时，除开ffn部分，其余部分都是被复制到多台设备上。 3.2节再次提到，将注意力层复制到多个设备上，多个专家进行分割，分割到多台设备，因此，对于一台设备上的数据流，则类似原版的transformer。 每个router副本与每个expert进行all to all通信，容易理解。（糖丸了，all to all通信，这通信开销得多大啊） ### 自动分片 因为是利用了2048个TPU，那么如何利用tensor计算能力就很重要了，所以论文里面将tensor运算进行抽象，让用户将整个计算集群看作单个设备，只用加注释进行数据分片，至于哪些分片到哪个计算unit是系统负责的。我们主要探索分布式下的性能，所以不做过多探索。 ## switch transformer [论文链接](https://arxiv.org/pdf/2101.03961) 传统的moe将一个token路由到了多个专家，并将多个专家的输出乘以偏好进行输出，（也就是偏好决定了专家输出的重要性）。 ![alt text](image 3.png) 本文rethinking这个操作，使k 1，（乐） ### 辅助损失 ![alt text](image 4.png) 以上用于路由平衡，确保专家分配token的平衡，保证token dropped率低的情况下，减少expert容量。 ### 不同并行方式对比 ![alt text](image 5.png) 上面一排是权重参数在不同core上的分布，下面一排是数据在不同核上的分布。 直接看最后一个红框，按理说是最复杂的，对于上面的4×4方格，也就是16个core，不同颜色代表不同的权重参数，这里比较难理解，我看网上有的作者认为，对于第一个蓝色的块，里面是有四个expert的，他们作为一个整体在被处理，但是这样逻辑说不通。我更倾向于认为，一个块代表的一个expert，对于下面的数据分配，蓝色的块都是同一批数据，也就是比如[x1,x2,x3,x4]这么一组数据被复制到四个核上。 那么单个核的计算流程就变成了，一个批次的数据被input时，该核拥有一个专家的一部分参数，对应参数与数据中对应的部分计算后后，四个核的数据进行reduce，那么通信就被局限在了四个核中，从而减少了一定的通信量。 ## 其他 显然，直接对transformer进行分层然后分配设备进行pipeline计算一定有相关课题，不做过多赘述。 ## 梳理 那么从头看moe在多设备上的计算，从inference到back propagation。以每个设备分配一个专家为例。 + 1，tokens被输入到模型，并进入device 0 + 2，经过注意力模块计算，sync with other devices + 3，token经过router，得到输入每个expert的偏好值 + 4，根据偏好值得到top k个需要被激活的expert。 + 5，将token投入这k个expert，注意到expert在其他设备上，因此这里面临第一重通信，tokens的传输。 + 6，expert计算得到output后，所有的expert的output进行规约，那么这里面临第二重通信，output的传输。（还有个输入的残差结构的通信在上一重已经进行了） + 7，得到总的output后通过label得到loss，loss反向更新expert参数，expert反向的梯度将会更新router的参数，注意：router都是复制的副本，因此这里涉及第三重通信，gradient的传输/新parameters的传输 + 8，梯度传到注意力模块，再次进行参数更新，产生gradient的传输/新parameters的传输 从以上过程看，一旦设备数过多，那么就会产生大量的设备间通信。 需要注意的是，以上只是一个基础的过程，实际执行中不同设备同一时刻的输入tokens可能是不同batch的数据，此时多设备间进行梯度合并更新参数。不同的策略将会极大的影响moe的效率。 那么问题转移到了，怎么平衡计算与通信，如果看计算的极端，就是让每个设备都维护一份gradient，从而单独计算从而更新参数；那么通信的极端就是只让一个主机进行计算，其余从机继续向下一个状态转移，主机计算完成后，通知从机更新后的参数。 ## dualPipe 直接讲主要方法， 关于双路pipeline，如下： ![alt text](image 8.png) 上面是常规的流水（8卡），将整个网络中的layer平等的分为8份，每一份放在一张卡上，例如GPU0持有layer0 9，GPU1持有layer10 19，以此类推。 那么对于上面一部分，训练流程如下： + 1，batch0输入GPU0，layer进行inference，将output传输到GPU1，以此类推，直到layer7得到输出。 + 2，计算loss + 3，从layer7开始反向传播，将gradients传输到GPU6，以此类推，传输的同时进行参数更新。 + 4，循环以上步骤 通过调整forward与backward的顺序，能够减少流水线中的气泡，例如1F1B，ZB PP等。 因此引出下面一部分，我们需要结合论文中提到的Overlapping strategy来看： ![alt text](image 6.png) 上面的图是dualpipe中的pp方式，可以注意到的是，GPU0，GPU7同时持有layer0，那么我们在一开始可以同时输入两个batch分别到GPU0与GPU1 需要注意的是，这种操作单独来看并没有什么太大的作用，因为每个GPU同一时刻的计算能力仅能够负载一个layer的forward/backward。好就好在dualpipe中设计了下面流水块。 我们需要知道在单个GPU内，通信与计算是独立的，意味着这两者也可以形成流水，所以dualpipe设计了下面的流水方式。 ![alt text](image 7.png) 上面是一个紧密的流水块，怎么来的？看下面的流水线，可以注意到，两头batch输入，同时采用FB交替的方式进行流水，在流水线的中间部分就会“产生”这样的块。 （节奏大师） ![alt text](image 9.png) ## 整体分布 那么综合以上的并行结构，粗略的总结一个整体的分布式架构（或者pp在节点间进行？）： ![alt text](image 10.png) expert位于多个节点上，由于每一个batch的数据不同，router产生的结果也不同，导致每一次激活的expert也不同，因此，不可避免的，采用all 2 all通信是最佳选择。 > 一个问题是，为什么不在节点间流水？因为每个expert并不是每一次都会被激活，直觉上router产生的节点间的通信带宽需求远远小于pp中的数据传递。 > 同时也可以注意到，之前的主要瓶颈在于节点间的通信，因此负载均衡loss func的提出才能有效的提升整体的效率。 那么所谓的拓扑架构也就不再存在，因为不存在一个数据跨越多个节点传输的问题。"},"/docs/references/index.html":{"title":"reference read note","content":"# reference read note reference read note > **click the sidebar to open markdown file** >! [read from the first file](./ann/faastube/faastube.html)"},"/docs/references/algorithms/transformer/index.html":{"title":"transformer","content":"# transformer ## 整体结构 整体 ![alt text](image 1.png) 细节 ![alt text](image 2.png) ## 流程 简述一下 + 1，输入一句话，如“how are you” + 2，将这句话进行词向量嵌入，每个单词转换成一个m为的向量，这句话中有3个词，所以最终输入的矩阵维度就是3*m。 + 3，将词向量与位置编码直接相加，然后输入encoder，一层层encoder向下传 + 4，将最后一层encoder的结果输入给每一层的decoder，再一层层计算decoder的值，并向下传递，最终输出一个词向量矩阵，然后查词典得到对应的自然语言 + 5，需要注意的，对于这个结构而言，我们是一次性将所有的单词进行输入，而非像传统时序网络那般，每一个编码/解码器负责一个向量输入，这样对于整个网络更能注意到整体的信息，而无需像lstm那般设置很多的门来避免信息丢失，这也更符合人理解自然语言的流程。 ## self attention 只需要知道是计算qkv相关矩阵，基本是矩阵运算，不用深入原理，说不清的。 通过多头注意力计算得到输出，然后输出进行拼接，然后输入linear，如下： ![alt text](image 3.png) ## 其他 ADD && NORM就是残差连接再归一化，都好理解，前者避免linear信息丢失，后者避免输出爆炸 需要注意的是decoder中的第一个多头用的masked： ![alt text](image 4.png) 第二个多头中的KV用的是encoder的输出矩阵计算得到的。 Feed forward层就是一个非线性层，现在这些概念都快绕昏了，mlp，linear，dense，ffn一大堆。"},"/docs/references/ann/a_realtime_adaptive_multi_stream_GPU_system/adaptive_multi_stream.html":{"title":"A Real-Time Adaptive Multi-Stream GPU System for Online Approximate Nearest Neighborhood Search","content":"# A Real Time Adaptive Multi Stream GPU System for Online Approximate Nearest Neighborhood Search > 解决在线场景中需要实时插入的问题。 ## main problem + 1，在线场景实时插入 + 现有系统对ANNS采用单流串行执行模式：如下图上部分： <div style \"text align:center;\"><img src \"QQ20241207 102525.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 当进行实时向量扩展时，批处理需要进行阻塞，如果复制的数据量大，等待期延长，导致搜索过程被阻碍。 + 同时，触发向量扩展时，需要GPU启动kernel来进行内存分配和复制，这个过程是耗时的。 ## solution ### CPU/GPU ANNS中的在线向量插入方法 <div style \"text align:center;\"><img src \"QQ20241207 105307.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 首先改变了数据结构改为链表（如上图），现有系统将所有的向量连续的排列在内存当中。header info的内容报头包含指示prev_header、next_header和block_info的地址的重要信息（向量容量、向量大小等）。ml中记录的是这个簇中属于这个block的vectorid。mv被以32维交错的放在内存块中（？）如下图，D是单个向量的维度。 <div style \"text align:center;\"><img src \"QQ20241207 154410.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 设计新的内存分配方式，就是加了一个内存池管理，内存池的大小是整个主存的大小。然后将内存按块分为最小单位， <div style \"text align:center;\"><img src \"QQ20241207 160804.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 直接看看它的算法吧，老实说，看不太懂，就对于上面的伪码而言，k是从哪里冒出来的？文中似乎并没有提到这一点，我们就假设代指全部。同时忽略需要同步的代码，仅看代码逻辑。 那么当需要插入一个新向量时： + 1，计算向量列表的末尾索引did + 2，通过did计算应该放在哪个block，blockid为mid，还需要该向量在block内的偏移moff才能准确定位（吐槽一下，$T_{m}$又是哪来的？） + 3，如果mid大于已分配的block list，那就分配新的block，注意到嵌套if，第一个if查看是否需要插入新的block，第二个if判断是否是第一个元素，才选择从内存池中新分配block，这里需要强调的是，为什么这么做呢，别忘记了整个函数是一个gpu核函数，此时其他线程也在操作。原子化的判断，不会使其余线程也同时加入新block造成冲突（精彩） + 4，后续就是更新$m_{mid}$，然后将新的vector写入block。 + 5，如果新加入的block list的长度超过预定义的$T^{'}_{m}$,那么执行重排操作。 重排操作（严重吐槽，这还是个递归函数，关于函数的解释就两句话？？，图也没有一个，不确定正式发表是否有改动，我要是reviewer包给他挂了）： <div style \"text align:center;\"><img src \"QQ20241207 163515.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 言归正传，其实主要思想就是，我们不是新加入了一个block，上图中是$m_{j}$,我们这个新block可能和旧列表中的向量是连续排列的，因此需要将$m_{j}$放到$m_{i}$后面，那么就交换$m_{j}$和$m_{i+1}$，交换需要中间空闲临时块。 那么产生的新问题是，交换过去的$m_{i+1}$也可能和旧列表中的块邻近，因此采用递归操作知道收敛。这里的递归终止实在是看不懂了，merge是在干嘛是真不知道。 后面才注意到还有个图c，甚至论文里面没有引用过，如下： <div style \"text align:center;\"><img src \"QQ20241207 184738.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ### 流缓存多流执行 <div style \"text align:center;\"><img src \"QQ20241207 184852.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 如上图，将多个执行核放到多个流上。 这样做有两个问题要考虑 > 1）系统必须避免联机内存分配和解除分配，以防止全局阻塞和降级为串行化机制。2)系统中的每个内核都不应消耗大量资源，因为过多的资源消耗会导致阻塞。 总结就是由于存在共享资源，所以会有锁的问题，一旦多个流同时访问同一个资源，那就会由并行转为串行执行。 对于问题1： > 设计了一个双层的基于流的资源池。最初，每个批量搜索请求被分配给一个专用流，拥有单独的小内存分配（足以使用ivfflat和ivfpq算法）。 之前提到内存池大小是整个内存，这里就是为每一个搜索流单独分配一个足够使用的内存空间。这样就不会和实时插入流产生冲突。 如果不够用了，再从中央内存池分配空间。 ## deployment detail 实际使用的系统，（虽然文章让人很不爽，但是能用起来的都是牛逼方法） 直接看原文描述： > 我们的系统已经在T4/A10 GPU机器上运行了六个多月，无缝集成到一个广泛使用的信息应用程序的搜索和推荐系统中，每天有超过1亿用户。包括两个基本部分，即离线和在线组件，我们的系统有效地管理准备好的数据和实时矢量插入任务。离线段专用于处理准备好的数据，而在线段编排实时向量插入操作 > 在线段由多个CPU线程管理，并与Kafka接口进行消息摄取，它实现了一个动态的队列策略，在将聚合的批次分派给GPU之前，每秒或在达到128次插入的倍数的阈值时聚合向量，上限为1024。在内核执行之前，我们仔细地从资源池中分配了32个独立的资源，每个资源都配备了50 MB的缓存内存，如前所述。在临时内存需求超过此阈值的情况下，额外的内存将从中央池中获得，每个分配设置为200 MB。为了确保公平的资源分配，特别是在高QPS条件下，我们设计了无锁队列机制。当所有32个资源用尽时，请求被拒绝。此外，我们为载体插入任务建立了专用流，以简化处理。 实际执行算法时有一些细节还是需要注意，不算是trick，只能说作者没在method里面提到过，但是很重要。 **高地址分配策略进行搜索，低地址插入。**（看不懂，原文就这一句，乐）"},"/docs/references/algorithms/ann/ann.html":{"title":"","content":"<script> MathJax { tex: { inlineMath: [['$', '$'], ['\\\\(', '\\\\)']] } }; </script> <script id \"MathJax script\" async src \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex chtml.js\"> </script> # ANN（Approximate Nearest Neighbor）近似最近邻算法 ANN是KNN的弱化版本 近似最近邻检索的核心思想：搜索可能是近邻的数据项而不再只局限于返回最可能的项目，在牺牲可接受范围内的精度的情况下提高检索效率。 ## 基于树的方法 KD树是其下的经典算法。一般而言，在空间维度比较低时，KD树的查找性能还是比较高效的；但当空间维度较高时，该方法会退化为暴力枚举，性能较差，这时一般会采用下面的哈希方法或者矢量量化方法。 k d树是一种空间划分树，说白了，就是把整个空间划分为特定的几个部分，然后在特定空间的部分内进行相关搜索操作。想一个三维(多维有点为难你的想象力了)空间，kd树按照一定的划分规则把这个三维空间划分了多个空间，经典的分治思想，类似归并排序（和八叉树<font color red>?</font>，老实说，这几种树，貌似都是平衡树的一种，将数据划分以减少计算量） 举一个多个数据点构造kd树的例子，划分结果如下图： <div style \"text align:center;\"><img src \"./3bd69691db3eacef35f65a6748fc52b1.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> **注意（7，2）为根节点，此时出现一个新点需要计算最近邻时，从根节点出发，位于根节点右边则进入（9，6）节点，避免了原始算法O（n）的计算复杂度，关于kd树的其他知识则是增删改，不再赘述。** ## 局部敏感哈希（LSH） 核心思想：在高维空间相邻的数据经过哈希函数的映射投影转化到低维空间后，他们落入同一个吊桶的概率很大而不相邻的数据映射到同一个吊桶的概率则很小。在检索时将欧式空间的距离计算转化到汉明（Hamming）空间，并将全局检索转化为对映射到同一个吊桶中的数据进行检索，从而提高了检索速度。这种方法的主要难点在于如何寻找适合的哈希函数。 **简而言之，假设我有n个点，我将这个n个点分散到m个桶内，分的方法使用hash函数，然后查询新点时，再通过同样的hash func计算属于哪个桶，将桶内数据取出进行线性匹配（挨个计算距离）。以上理论成立的前提是，通过该hash func，邻近的点真的能进入同一个桶，那么hash func的挑选就成了该算法的核心。** > 汉明空间即计算汉明距离，是比较两个长度相同的向量，若某个维度相等，则距离加一，汉明距离可以比较两个二进制串，a 11101010，b 11011010。a和b两个二进制串不同的位数为2，则汉明距离为2。相似度越高，距离越小。 <div style \"text align:center;\"><img src \"./v2 84f6c2b0c8caa9ac7b106484820204eb_1440w.jpg\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 对于传统的hash，上面的每个“桶”内只能装一个值避免冲突达到O（1）的复杂度，LSH则是将相邻的放在同一个桶。 ## 矢量量化 其代表是乘积量化（PQ）。它的主要思想是将特征向量进行正交分解，在分解后的低维正交子空间上进行量化，由于低维空间可以采用较小的码本进行编码，因此可以降低数据存储空间 。 **PQ**的简单概要：选取一个大向量，将其拆分为子向量，将每个子向量分配给其最接近的质心值，并将质心值替换为其唯一ID，生成一个较小的ID向量。 在该过程结束时，我们将高维向量（需要大量内存）简化为需要很少内存的ID向量。 假设向量长度 为为12。我们首先将此向量拆分为 个子向量，如下所示： <div style \"text align:center;\"><img src \"./QQ20241203 133938.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> PQ方法采用基于查找表的非对称距离计算(Asymmetric Distance Computation，ADC)快速求取特征向量之间的距离，在压缩比相同的情况下，与采用汉明距离的二值编码方法，采用ADC的PQ方法的检索精度更高。 **fine，fine，上面太复杂了，不过严谨的论述不能少，下面是不严谨的，先讲矢量量化，就是kmeans的结果有k个簇，例如256个簇，那么我可以用8bit表示完这256个簇，这个8bit所对应的值就是cluster_id，我们需要找簇时通过8bit来找，而不是原数据u，达到压缩数据的作用。** **PQ干嘛呢，就是在划分前先将d维向量划分成m份，在m份中单独做聚类，这样能并行化聚类的过程，m份中有k个簇，这k个簇两两之间计算距离，可以得到一个$m*k*k$的码本，这个后面查询的时候用** SDC <div style \"text align:center;\"><img src \"./v2 2e3233c422794f13cf275dc40e962f17_1440w.jpg\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> x是query向量，首先计算q（x），就是x的聚类中心，然后计算q（y），y是向量库里的向量（cluster id），所以问题被转化为计算聚类中心之间的距离，而计算聚类中心的距离可以直接查码本，也就是上面提到的。 ADC <div style \"text align:center;\"><img src \"./v2 470a71a9c06666c59102e325dcdd70b5_1440w.jpg\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ADC是非对称的，也就是x直接与q（y）计算距离，只有一个量化误差。但是显而易见，在线计算，没法查表，性能有所下降。 用以上两种算法去求query的近似数据。 上述方法在m份聚类组中都要进行，最后得到一个欧式距离的总和，就是x与y的距离。 ## 基于图索引的方法（文献总结） 和树很像，但不是一个东西，比如说a向量和b向量可以指向同一个相近向量，这一点在树结构中是不存在的。"},"/docs/references/algorithms/moe/index.html":{"title":"Mixture of Experts","content":"# Mixture of Experts 混合专家模型 Google Switch Transformer论文中的MoE结构： ![alt text](image.png) 把[transformer](../transformer/index.html)中的feed forward换成了多专家结构，如上图 每个专家可能是ffn或者是更复杂的结构。 这样扩大了整个网络中的参数量，同时各专家处理不同的子任务，最后将输出进行拼接。 决定输出输入到哪个专家的是门控网络，（可以粗浅的理解，也是个ffn）。 ## type sparse moe 和 dense moe， 前者选出top k个专家激活，后者激活所有专家，将所有token输入并乘以权重输出 ## problems 那么为什么这样的结构可以训练得到更好的效果？ 注意到不论是门控网络还是expert，其参数一开始都是随机的，那么一开始门控决定输入到哪个专家完全是随机的，但是其中涉及到一个“巧合”，就是数据输入到某个专家时，假设其参数更适合当前的token，那么他就能更快的拟合，得到更好的loss，那么门控网络的参数也将会被更新为更倾向于选择该expert的值，那么这样不断的拟合，不同的专家的参数适合不同的任务类型，当不同的任务被输入时，训练好的门控网络将会选择更适合当前任务的expert。 需要注意的是，为了避免某些“误会”，（不太好形容，所以用了很多比喻），就是当前expert明明更适合另一个子任务，但是由于输入数据相似，门控错误的认为这个expert更适合，所以在门控中加了些随机量，让门控给予其他专家尝试不同子任务的机会，这一点同蒙特卡洛搜索中相似，为了避免在某个子树中过度探索，会在计算奖励值时加入一些随机惩罚，让其他子树有机会被探索，避免错过最佳路径。"},"/docs/references/llm/pageAttention/index.html":{"title":"page attention","content":"# page attention ## problem 生成了预定token数的KV cache，但是实际使用时仅能使用一部分的缓存。就是每一次的文本生成都预分配了一定的内存，但是不一定所有的内存都被使用了。 显存的碎片不足以预分配给下一个文本的生成。 ![alt text](image.png) 对于实际物理地址的索引是通过页表索引的，其主要作用是虚拟地址的分配是连续的，而物理地址可以是不连续的。当一个文本生成需要进行kv缓存时，向系统申请内存，系统会管理物理地址，拿到指定大小的内存，并构建映射。 以上操作的优势在于，物理地址的合并不影响虚拟地址的使用，以达到节省空间的功能。"},"/docs/references/llm/RoPE/index.html":{"title":"RoPE","content":"# RoPE ![alt text](image.png) ![alt text](image 2.png) ![alt text](image 3.png) ![alt text](image 1.png) 最后这个高维的旋转矩阵本质是高维空间的向量旋转，与二维空间的按角度旋转在数学性质上等价（线代学的太烂了，没去推）"},"/docs/references/llm/SLO_PD_multiplex/index.html":{"title":"Optimizing SLO-oriented LLM Serving with PD-Multiplexing","content":"# Optimizing SLO oriented LLM Serving with PD Multiplexing ## limitation ### pd分配 in place compute 和out place compute方式，如图： ![alt text](image.png) in place计算通过将prefill与decode分离达到减低TBT的作用，out place计算则是在实例内进行以上工作，能达到低内存开销，但是TBT无法保证 > 论文说将两者结合会带来较大的KV cache传输开销,out place计算不也是跨实例传输嘛？ ### prefill分块控制 为了满足TBT的需求，以前的做法将prefill分为了多个chunk，但是chunk大小不好控制，如果太大了TBT受到影响，如果太小了GPU利用率降低 且由于多个请求以及多个阶段间的kv cache共享，分块会造成对kv cache那一块内存的重复访问 ## solution"},"/docs/references/moe/deepseekMoe/index.html":{"title":"deepseek Moe","content":"# deepseek Moe [论文原文链接](https://arxiv.org/pdf/2401.06066) ## 传统moe存在的问题 + 知识混合 就是虽然分了不同的expert，但是输入时，有些专家分配了多个token，蕴含不同的知识 + 知识冗余 某些输入tokens包含的相同（通用）的知识，将会被每个专家学习到，那么专家的参数就是冗余的。 ## 解决 ### 细粒度化expert + 增加expert的数量 + 降低expert ffn的参数量 相应的，将激活的expert的数量增大到m倍，以维持计算成本近似 moe部分计算公式如下： ![alt text](image.png) $g_{i,t}$代表选择expert的权重 ### 设置共享专家处理共享知识 新增了一定的shared expert，他们用来处理共享知识，结构如下图： ![alt text](image 1.png) 计算公式如下： ![alt text](image 2.png) 这些expert是固定激活的，不参与路由的选择，因此相应的，为了维持计算成本，将会将其余的expert的激活数减少，有多少个shared expert，就会减少相应数量的常规expert。 ## 负载均衡问题 这个在[moe problems](../../algorithms/moe/readme.html#problems)提到了，也就是路由网络过分选择某几个专家，造成路由崩溃，同时如果专家分布在多个设备上，会因为负载不均衡产生计算瓶颈。 解决方法： 专家级平衡损失，用来解决某些expert分配的任务量过多，而有些闲置的问题导致的负载不均衡 ![alt text](image 3.png) 没看懂，信息太少了 还引入了一个设备级平衡损失，用来解决设备负载不均衡，貌似是分组计算的： ![alt text](image 4.png) ### 有个视频讲了相关的损失 [视频链接](https://www.bilibili.com/video/BV15XFQebEBM/?spm_id_from 333.337.search card.all.click&vd_source e32e76663edb97323b10f324b6d846ec) 先看switch transformer中的做法，总的说，就是经过softmax每个expert会有个选择概率，然后将每个expert接收的token数占总token数的占比作为实际值， 将softmax输出值平均作为理论值，取差值为loss，用loss更新参数。如下： ![alt text](image 5.png) 然后发现，deepseek中提到的参数是在switch tranformer中的，所以deepseek直接略过了。。 其中的f就是实际值，P就是理论值。 视频提到了这个loss func的问题，就是target是minimize这个loss，但是这个loss func没有能力去最小化loss，计算公式的问题。也就是最终其实没有达到load balance的目的。 而deepseek中采用loss free，在router的softmax中增加了一个bias用来修正，也就是如果某个专家过多被选择，这个bias会被减小从而减小该专家被选择的可能性（如果训练数据中该专家确实需要被多次选择，该做法是否会错过最佳路径？如果采用随机偏置是否有可能得到更好的效果）。类似于正则惩罚？和RL中的随机偏置也有点像。"},"/docs/references/llm/deepspeed_llm/index.html":{"title":"","content":"## 专家并行加速 ![alt text](image.png) 这里应该是单纯的增加使用的卡数，对于moe来讲，可以将多个专家的参数放到一张卡上，由于token与参数的计算都是矩阵乘，可以将多个token的向量进行拼接，和多个专家同时拼成一张大矩阵乘来计算 但是不可避免地会增加单张卡地计算量，这里将16张卡上的专家拆到64张卡上，并行度提升是一方面，另一方面单张卡的计算量减少，性能自然提升，因此此处可能和所提到的内存空间更大没有太大的关系。 ## 双流并行 ![alt text](image 1.png) 这里双流并行可能是对于一个算子可以存在多个stream，只要stream的算子之间没有数据依赖，那就可以让两个stream互相争抢资源，以达到上面所说的计算与通信相互覆盖的效果。 ## autoPD ![alt text](image 2.png) 关于这个的资料只找到[pd分离特性](https://www.hiascend.com/doc_center/source/zh/mindie/10RC3/mindiellm/llmdev/mindie_llm0291.html) ![alt text](image 5.png) ## 多链路并发与软硬件协同通信算法 ![alt text](image 3.png) 这个上面是跨机通信的优化，原始的跨机通信需要按照roce网络通信的方式进行三次确认确保数据包送达，这里他们团队自己设计了通信原语，将数据与标志位一起传输，类似单机内的流水同步原语，使用waitflag和setflag的方式进行同步。（就是把多机也看做了多个部件，以流水并行的角度看待整个系统） ![alt text](image 4.png) 类似[fasstube](../../ann/faastube/faastube.html)这一篇，利用其他可达链路进行聚合通信，在这里也就是直接跨机通信，以及机内通信 >跨机通信这两条链路可以同时使用。 ## MLAPO [MLAPO](../MLAPO/index.html)"},"/docs/references/llm/flashAttention/index.html":{"title":"flash attention","content":"# flash attention ## method 使用算子融合的思想，将计算局限在SRAM中，例如QK计算中间结果不进行HBM的写回，而是通过分块的方式，继续同V矩阵的部分value进行计算，然后将这一部分的计算结果写回HBM，再进行下一块计算，这样就避免了SRAM频繁从HBM读取数据的问题。"},"/docs/references/llm/MLA/index.html":{"title":"MLA架构","content":"# MLA架构 ## MHA架构 标准多头注意力 ## MQA架构 ![alt text](image.png) 复用了每个token的KV，会影响表现 ## GQA架构 每两组token共享一组KV，本质是效果和性能的权衡 ## MLA架构 ![alt text](image 1.png) 对token先降维压缩，计算注意力时，通过升维矩阵进行解压 ![alt text](image 2.png) ![alt text](image 3.png) 这里的融合本质理解，首先通过矩阵乘法满足结合律可知，可以先计算$W^{Q}W^{UK^T}$,由于两个W权重矩阵都是未知量，因此类比于$(y ax_1 \\dot x_2) > (y ax_3)$,所以这里实际可以计算为一个未知矩阵。 但是以上的计算没有考虑RoPE ![alt text](image 4.png) 加上了旋转矩阵后以上就无法进行合并，因为每一个token的相对位置是变化的。 ![alt text](image 5.png) MLA架构为Q添加了一个$W^{QR}$矩阵，将token与该矩阵乘后的结果，在经过RoPE得到一个旋转向量，将该向量拼接到Q矩阵当中，从而添加了位置信息。 同样K也存在响应的R矩阵。 这样，将旋转位置编码与注意力计算解耦后，就可以实现矩阵融合的同时，利用到位置信息。"},"/docs/references/llm/MLAPO/index.html":{"title":"MLAPO","content":"# MLAPO ## 算子融合 ![alt text](image.png) 显然，融合算子可以避免和主机内存的频繁交换，提速是必然的， MLAPO中还将前向的13个算子融合成一个大算子，包括以下内容： ![alt text](image 1.png) 通过cube+vector并行的方式，并行计算一部分小算子，基本可以将向量运算进行覆盖。"},"/docs/references/ann/second-Tier-memory/second-Tier-memory.html":{"title":"Characterizing the Dilemma of Performance and Index Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory","content":"# Characterizing the Dilemma of Performance and Index Size in Billion Scale Vector Search and Breaking It with Second Tier Memory 本文较特殊，前半部分花费了大量篇幅论证了在当前的存储下，图索引以及聚类索引所面临的困境，这一大部分内容对阅读造成了困难，因此后续分析将进行简略处理。 提到的二层存储是使用RDMA和CXL连接的远程DRAM/NVM（非易失性存储）。 ## main problem > 对于一个64核的商品化的服务器，使用SIMD在100 384维的公共向量数据集上搜索，效率可以达到3624 5212 Mvectors/sec。 > > 但是对于5.3GB/s带宽的固态硬盘，即使带宽利用率拉满，也仅仅能支持14 53Mvectors/s的向量查询效率，这个差距也就是存储墙的问题。 因此这里武断地说，如果不改变查询向量的系统结构，类似于fusionANNS那样，那唯一的解决方案就是降低i/o请求频率，如何解决呢？cache！乐，实际上就是类似于内存到三级缓存那样处理，如果一个数据之前被取过，那我不必再去发起新的i/o请求，这一点在fusionANNS中有提到过类似的解决方案。 论文又提到在fusionANNS上提到的问题，SSD访存颗粒度不匹配问题。 > 我们认为，第二层存储器 易失性或非易失性（NVM）存储器通过快速互连（如RDMA和CXL （§4））连接到主机，为系统地解决这个问题提供了机会。具体而言，这些设备的行为类似于存储，但支持更精细的访问粒度（256 B vs. 4 KB），这与向量索引的工作负载模式相匹配。此外，它们对于具有甚至更小访问粒度的随机读取（例如，100 B） 其实就是如果直接访问SSD，那单次请求的颗粒度大小为4KB（或许与文件系统的BLOCK相关），但是使用RDMA或者CXL这种类总线的间接访问方式可以降低其粒度以减少i/o请求的开销（<font color red>存疑，虽然粒度减小，但是请求次数没变，二层内存是否能够带来更高的访存效率？</font>）。 主要问题： > 我们将这种困境归因于高性能SSD访问的I/O需求与具有小索引放大的矢量索引发出的I/O工作负载之间的不匹配。 > 图索引需要细粒度的存储读取来实现实际的索引大小。对于具有最小索引大小的实用图索引，我们必须用每个节点的少量边来构造它。从算法的角度来看，这意味着图遍历必须使用具有小有效负载的随机I/ o来读取这些边。但是，这与使用足够大的I/O有效负载(4 KB)以有效利用SSD等传统存储设备的要求存在根本冲突。 文章用了大量篇幅论证上面的观点，论证过程不再赘述。 ## solution 再看看图索引的实现： <div style \"text align:center;\"><img src \"./QQ20241205 152623.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 其中节点是向量，边连接距离近的向量。例如，如果a → B，这意味着向量B是a的前k向量。提到的辅助搜索的压缩图是什么（<font color red>?</font>，查：质心代表簇中的向量数据，每个节点的邻居数有参数配置的k相关，不是查询使用的那个top k。 下面是在图上寻找前k个最近向量的流程，思路很简单，不再赘述。 <div style \"text align:center;\"><img src \"./QQ20241205 152916.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 图索引的缺点： >由于其指针追逐访问模式，导致长延迟和低带宽利用率。此外，图索引对向量插入不友好，因为构建图需要重建图。 聚类索引（也可以叫簇索引，只是名字不同，其本质是一种量化方法）： 基于簇索引的方法中需要注意的是不平衡划分问题以边界问题，边界问题在fusionANNS中提到，在此处更加明了，解决方案是将边界向量复制到一组关闭的集群，其中复制的数量在索引构建之前静态配置。下图可以说明该问题： <div style \"text align:center;\"><img src \"./QQ20241205 163411.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 如果top c质心仅包含cluster0，那cluster1中的邻近向量就被丢掉了，导致准确率降低，解决方法就是将cluster1中的边界向量复制到cluster0，缺点显而易见，存储代价增加。另一种解决方案就是将两个cluster都读一遍，也可以获取到所有的邻近向量，但是缺点也很明显，搜索代价增加。 看看一层存储与二层存储的结构图： <div style \"text align:center;\"><img src \"./QQ20241205 160018.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> 优点是（1）老一代内存更便宜，（2）间接允许通过池化未使用的内存来提高内存利用率。 然而，它们仍然比DRAM慢得多，并且表现得更像存储，也就是说单次i/o请求的延迟比直接dram要慢得多，特别是RDMA（究其原因是经过的流程太多了，cpu向DMA发送请求，DMA接受请求处理完transfer后触发中断，等待中断执行），那解决方案显然是将单次i/o请求内容增大（术语叫请求载荷）。 常用的十亿规模的数据集（仅做个记录） <div style \"text align:center;\"><img src \"./QQ20241205 160649.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> **文章的主要方法就是借助二层内存（二级内存？），在该内存上构建图索引和聚类索引，借助二层内存更细粒度的访存，我们可以加速向量查询，以上说法不是很精准，因为借助RDMA以及CXL等中间传输介质，不能将二层内存直接看做内存** ### 基于second Tier Memory改进的图索引 + 软件流水线 很简单，就是使用异步i/o，（虽然不算惊艳，但是是非常稳健的方法） > 我们提出了一种软件管道机制，用计算来异步处理I/O，从而最大限度地利用计算能力。 RDMA中使用协程（就是一个用户级的多线程，可以理解为在多个函数之间来回调度，但是不通过os kernel，这么说可能有点抽象，你可简单理解成用户自己实现的多线程），NVM和CXL中使用预加载（mm_prefecth），能理解的，类似于操作总线，我们通过store/load指令访存，但是该操作无法实现异步，由于是cpu指令，需要强制同步等待，所以论文也提出了重新组织指令顺序（术语：乱序执行）。 + 压缩图布局 呃呃，因为是图，邻接稀疏矩阵，所以转为csr存储，就酱。。 ### 基于second Tier Memory改进的聚类索引 <div style \"text align:center;\"><img src \"./QQ20241206 100136.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> + 解耦索引布局 如上图，a图为原始的布局，对于边界向量，是直接将向量复制到各个簇当中，显然浪费空间。b为解耦布局，将向量地址和向量数据分开存储，这样边界向量的占用就被减小了。（注意，此处不要将二层内存看作内存，实际存储设备仍然是SSD，二层内存是一个抽象，借助它的细粒度以及异步i/o我们可以做到加速）。 那么新问题是，如何在新布局上执行搜索，先查询向量地址，再用向量地址读取数据（小负载），（这里有一个大前提，还记得前面各设备延迟对比么，那就是SSD的访问延迟是75微秒，而二级内存的延迟低得多，平均在个位数以下）。 + 集群感知分组 基于以上方式产生的新的问题是，该方式将原本的单个数据访问解耦后，产生了起码两倍的i/o数量，怎么解决？ 又是非常精彩的cache优化（万物皆可cache，悲）。 仍然见上图， > 通过将属于同一集群的向量分组在一起，并将它们存储在相邻的存储中，我们可以使用一个大I/O来读取所有组中的向量（地址） 注意到，虽然我们可以一次读取一个聚类所有的数据地址，但是别忘了还要读取数据，如果这个聚类中存在边界向量是由其他聚类复制过来的，那么我们就需要一个单独的i/o请求来访问该向量（读到这里，你肯定疑惑，都有地址了，为啥要单独访问该向量？别忘了，我们用的二层内存是RDMA这一类“介质”，每次的请求是一个连续的地址块，精彩） 所以引出了后续的工作，怎么分组？假设有一个向量数据库，有一组向量V，和一组聚类C，使用$P_{i,j}$表示向量i是否被分配到一个组（应该是不与同聚类其他向量不在一个组的意思，否则后边儿的公式说不通），$h_{j}$表示聚类的接入频率（访问频率，后台监测得到），所以问题用公式表述如下： $$minimize \\sum_{j}^{C}h_{j} \\cdot (1 + \\sum_{i}^{V}P_{i,j})$$ 以上问题使用整数线性规划（ILP）解决，（简单认为是动态规划，毕竟背包问题就是用ILP解决），论文没讲具体实现，针对不同场景，该算法可以千差万别。 聚类约束： <div style \"text align:center;\"><img src \"./QQ20241206 105125.png\" style \"zoom:70%;border radius: 10px;border:2px solid #23D18B;padding:10px\"/></div> ok，$A_{i,j}$哪来的？似乎论文里面没有提到过。 组约束：就是每一个向量必须分配到最少一个组。 ok，看后文，似乎他们并没有给出实际的ILP解决方法，而是用贪心策略 > 观察问题的简单结构，我们可以使用简单的贪心算法来寻找最优解。具体来说，对于一个已经被复制到多个聚类的向量，将其分配给访问频率最高的聚类是最优选择。这是因为，非正式地说，将其分配给访问频率较低的集群会增加I/ o的数量。 ## interesting phenomenon 在常规的开发中，图索引有更小的存储占用，不需要太多边就能达到高精度，但是高频率的i/o访问带来性能瓶颈。 而聚类索引的访问模式更适合SSD的粗粒度访问，性能更高，但精度不足。 但在本文的实验中，恰好反过来了，图索引有更好的i/o效率。聚类索引有更小的索引占用。（？前面的好理解，但是聚类索引更小占用是因为将数据转为地址了？）"},"/docs/references/ascend/ascend_llm/index.html":{"title":"","content":""},"/docs/references/llm/APC/index.html":{"title":"自动前缀缓存","content":"# 自动前缀缓存 跨请求缓存KV矩阵 ## vLLM 对于每一个token采用hash编码，同时通过前缀构建前缀树，采用LRU逐出策略。"}}